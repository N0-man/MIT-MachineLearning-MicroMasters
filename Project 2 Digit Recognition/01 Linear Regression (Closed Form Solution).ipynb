{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form Solution of Linear Regression\n",
    "\n",
    "The Closed Form Solution of Linear Regression, particularly when referring to **linear regression with L2 regularization** (also known as Ridge Regression), is a mathematical approach to find the optimal parameters (weights) of the model without needing iterative optimization methods like Gradient Descent. This solution directly computes the weights that minimize the cost function of the linear regression model.\n",
    "\n",
    "### Mathematical Expression\n",
    "Given a dataset with $n$ samples, each having $d$ features. The Closed Form Solution for $\\theta$ for Ridge Regression is given by:\n",
    "\n",
    "$$\\theta = (X^TX + \\lambda I)^{-1}X^TY$$\n",
    "\n",
    "where:\n",
    "- $X$ is the feature matrix,\n",
    "- $Y$ is the vector of labels,\n",
    "- $\\lambda$ is the regularization parameter,\n",
    "- $X^T$ is the transpose of $X$,\n",
    "- $I$ is the identity matrix of size $(d+1) \\times (d+1)$ (to exclude the bias term from regularization, the first element of $I$, $I_{00}$, is set to 0),\n",
    "- $(X^TX + \\lambda I)^{-1}$ is the inverse of the matrix $(X^TX + \\lambda I)$.\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the problem, your classmate Alice immediately argues that we can apply a linear regression model, as the labels are numbers from 0-9, very similar to the example we learned from Unit 1. Though being a little doubtful, you decide to have a try and start simple by using the raw pixel values of each image as features.\n",
    "\n",
    "Alice wrote a skeleton code run_linear_regression_on_MNIST in main.py, but she needs your help to complete the code and make the model work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mPASS\u001b[m Closed form \n",
      "Linear Regression test_error with regularization constant as 1  = 0.7697\n",
      "Linear Regression test_error with regularization constant as 0.1 = 0.7698\n",
      "Linear Regression test_error with regularization constant as 0.01 = 0.7702\n"
     ]
    }
   ],
   "source": [
    "import mnist.utils as U\n",
    "import numpy as np\n",
    "import mnist.test_utils as T\n",
    "\n",
    "# Jupiter notebook cache the imported modules, so we need to reload the module to get the latest changes\n",
    "import importlib\n",
    "importlib.reload(T)\n",
    "\n",
    "# Create a link to the test_utils.py file\n",
    "# ln -s /Users/n03an/Documents/projects/AI/edx/MIT-MachineLearning-MicroMasters/test/utils.py test_utils.py \n",
    "\n",
    "def run_linear_regression_on_MNIST(lambda_factor=1, dataset='mnist/Datasets/mnist.pkl.gz'):\n",
    "    \"\"\"\n",
    "    Trains linear regression, classifies test data, computes test error on test set\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    # Load MNIST data\n",
    "    train_x, train_y, test_x, test_y = U.get_MNIST_data(dataset)\n",
    "    # Add bias dimension (adds a column of ones to the training features)\n",
    "    # This bias term allows the linear regression model to learn an intercept term.\n",
    "    train_x_bias = np.hstack([np.ones([train_x.shape[0], 1]), train_x])\n",
    "    # Add bias dimension to test data\n",
    "    test_x_bias = np.hstack([np.ones([test_x.shape[0], 1]), test_x])\n",
    "    # Compute parameter / weights using closed form solution using training data\n",
    "    theta = closed_form(train_x_bias, train_y, lambda_factor)\n",
    "    # Compute test error using test data\n",
    "    test_error = compute_test_error_linear(test_x_bias, test_y, theta)\n",
    "    return test_error\n",
    "\n",
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "\n",
    "    I = np.identity(X.shape[1])\n",
    "    #  np.dot(X.T, X) == X.T @ X\n",
    "    theta = np.linalg.inv((X.T @ X) + (lambda_factor * I)) @ X.T @ Y\n",
    "    return theta\n",
    "\n",
    "def compute_test_error_linear(test_x, Y, theta):\n",
    "    # Look at individual pridictions and compare to actual\n",
    "    # (nsamples, nfeatures) = test_x.shape\n",
    "    # for i in range(nsamples):\n",
    "    #     prediction = np.round(np.dot(test_x[i], theta))\n",
    "    #     print(f\"Prediction: {prediction}, Actual: {Y[i]}\")\n",
    "    test_y_predict = np.round(np.dot(test_x, theta))\n",
    "    test_y_predict[test_y_predict < 0] = 0\n",
    "    test_y_predict[test_y_predict > 9] = 9\n",
    "    return 1 - np.mean(test_y_predict == Y)\n",
    "\n",
    "# Test\n",
    "def check_closed_form():\n",
    "    ex_name = \"Closed form\"\n",
    "    X = np.arange(1, 16).reshape(3, 5)\n",
    "    Y = np.arange(1, 4)\n",
    "    lambda_factor = 0.5\n",
    "    exp_res = np.array([-0.03411225,  0.00320187,  0.04051599,  0.07783012,  0.11514424])\n",
    "    if T.check_array(\n",
    "            ex_name, closed_form,\n",
    "            exp_res, X, Y, lambda_factor):\n",
    "        return\n",
    "\n",
    "    T.log(T.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "check_closed_form()\n",
    "\n",
    "# Run linear regression on MNIST dataset with varying regularization constants\n",
    "print('Linear Regression test_error with regularization constant as 1  =', run_linear_regression_on_MNIST(lambda_factor=1))\n",
    "print('Linear Regression test_error with regularization constant as 0.1 =', run_linear_regression_on_MNIST(lambda_factor=0.1))\n",
    "print('Linear Regression test_error with regularization constant as 0.01 =', run_linear_regression_on_MNIST(lambda_factor=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def compute_test_error_linear(test_x, Y, theta):\n",
    "    test_y_predict = np.round(np.dot(test_x, theta))\n",
    "    test_y_predict[test_y_predict < 0] = 0\n",
    "    test_y_predict[test_y_predict > 9] = 9\n",
    "    return 1 - np.mean(test_y_predict == Y)\n",
    "```\n",
    "\n",
    "\n",
    "The function `compute_test_error_linear` in the provided code snippet calculates the test error for linear regression. Here's a step-by-step explanation of how it works:\n",
    "\n",
    "1) **Predict Test Labels**: It uses the linear regression model's weights (`theta`) to predict the labels for the test dataset. This is done by multiplying the test feature matrix (`test_x`) with the weight vector (`theta`). The result is a vector of predicted continuous values.\n",
    "\n",
    "2) **Round Predictions**: The continuous predictions are rounded to the nearest integer using np.round, as the MNIST labels are integers from 0 to 9. This step converts the continuous predictions into discrete class labels.\n",
    "\n",
    "3) **Clip Predictions**: The predictions are then clipped to the range [0, 9] using `test_y_predict[test_y_predict < 0] = 0` and `test_y_predict[test_y_predict > 9] = 9`. This ensures that after rounding, any prediction outside the valid range of labels is corrected to the nearest valid label.\n",
    "\n",
    "4) **Calculate Test Error**: Finally, the test error is calculated by comparing the predicted labels (`test_y_predict`) with the actual labels (`Y`). The comparison `test_y_predict == Y` produces a boolean array, where True indicates a correct prediction and False indicates an incorrect prediction. The mean of this boolean array gives the accuracy (the proportion of correct predictions). Subtracting this accuracy from 1 gives the error rate, i.e., the proportion of incorrect predictions.\n",
    "\n",
    "The function returns the final test error, which is the proportion of the test dataset that was incorrectly classified by the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Notice how the error rate is so high irrespective of regularization $\\lambda$. The function `compute_test_error_linear` calculates the test error for a linear regression model applied to a classification problem (MNIST digits classification). Linear regression is not the best choice for classification tasks, especially for multi-class problems like MNIST. Hence it does not matter what regularization constant we use, but the choice of the model itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

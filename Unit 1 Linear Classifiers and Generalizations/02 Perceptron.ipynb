{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is a simple algorithm for binary classification. It's a type of linear classifier that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "Perceptron is used on the dataset which is linearly seperable (line)\n",
    "\n",
    "Mathematically, the prediction of a Perceptron for an input vector `x` is given by:\n",
    "\n",
    "$$y = sign(w.x + b)$$\n",
    "\n",
    "Here:\n",
    "- `w` is the weight vector\n",
    "- `x` is the input vector\n",
    "- `b` is the bias term\n",
    "- `.` denotes the dot product\n",
    "- `sign` The signum function of a real number {\\displaystyle x}\n",
    "\n",
    "$$\n",
    "sign(x) = -1 \\space \\space if \\space\\space x < 0 \\\\\n",
    "sign(x) = 0 \\space \\space if \\space\\space x = 0 \\\\\n",
    "sign(x) = 1 \\space \\space if \\space\\space x > 0\n",
    "$$\n",
    "\n",
    "The Perceptron learning algorithm works as follows:\n",
    "\n",
    "1. Initialize the weight vector `w` and the bias `b` to zero.\n",
    "\n",
    "2. For each training example $(x, y)$ in our training set, perform the following steps:\n",
    "\n",
    "   a. Compute the prediction $y\\_hat = sign(w.x + b)$. Here, `sign` is a function that returns `-1` if the input is less than `0`, and `1` otherwise.\n",
    "\n",
    "   b. If the prediction `y_hat` is incorrect (i.e., `y_hat != y`), update the weight vector and bias as follows:\n",
    "      - $w = w + y * x$\n",
    "      - $b = b + y$\n",
    "\n",
    "3. Repeat step 2 until all training examples are correctly classified, or a maximum number of iterations has been reached.\n",
    "\n",
    "The algorithm iterates over the training examples, making these updates until all examples are correctly classified, or a maximum number of iterations has been reached. The resulting `w` and `b` define a linear decision boundary that can be used to classify new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0, Actual: 1, xi: [-4  2], w: [-4.  2.], b: 1\n",
      "Prediction: 1, Actual: 1, xi: [-2  1], w: [-4.  2.], b: 1\n",
      "Prediction: 1, Actual: -1, xi: [-1 -1], w: [-3.  3.], b: 0\n",
      "Prediction: 0, Actual: -1, xi: [2 2], w: [-5.  1.], b: -1\n",
      "Prediction: -1, Actual: -1, xi: [ 1 -2], w: [-5.  1.], b: -1\n",
      "Prediction: 1, Actual: 1, xi: [-4  2], w: [-5.  1.], b: -1\n",
      "Prediction: 1, Actual: 1, xi: [-2  1], w: [-5.  1.], b: -1\n",
      "Prediction: 1, Actual: -1, xi: [-1 -1], w: [-4.  2.], b: -2\n",
      "Prediction: -1, Actual: -1, xi: [2 2], w: [-4.  2.], b: -2\n",
      "Prediction: -1, Actual: -1, xi: [ 1 -2], w: [-4.  2.], b: -2\n",
      "Prediction: 1, Actual: 1, xi: [-4  2], w: [-4.  2.], b: -2\n",
      "Prediction: 1, Actual: 1, xi: [-2  1], w: [-4.  2.], b: -2\n",
      "Prediction: 0, Actual: -1, xi: [-1 -1], w: [-3.  3.], b: -3\n",
      "Prediction: -1, Actual: -1, xi: [2 2], w: [-3.  3.], b: -3\n",
      "Prediction: -1, Actual: -1, xi: [ 1 -2], w: [-3.  3.], b: -3\n",
      "Prediction: 1, Actual: 1, xi: [-4  2], w: [-3.  3.], b: -3\n",
      "Prediction: 1, Actual: 1, xi: [-2  1], w: [-3.  3.], b: -3\n",
      "Prediction: -1, Actual: -1, xi: [-1 -1], w: [-3.  3.], b: -3\n",
      "Prediction: -1, Actual: -1, xi: [2 2], w: [-3.  3.], b: -3\n",
      "Prediction: -1, Actual: -1, xi: [ 1 -2], w: [-3.  3.], b: -3\n",
      "Final w: [-3.  3.]\n",
      "Final b: -3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple binary classification dataset\n",
    "X = np.array([[-4, 2], [-2, 1], [-1, -1], [2, 2], [1, -2]]) # feature vector\n",
    "y = np.array([1, 1, -1, -1, -1]) # corresponding labels\n",
    "\n",
    "# Initialize w and b to zero\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "\n",
    "# Define the sign function\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Perceptron learning algorithm\n",
    "for _ in range(200):  # Limit iterations to prevent infinite loop\n",
    "    error_count = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        prediction = sign(np.dot(w, xi) + b)\n",
    "        if prediction != yi:\n",
    "            w += yi * xi\n",
    "            b += yi\n",
    "            error_count += 1\n",
    "        print(f\"Prediction: {prediction}, Actual: {yi}, xi: {xi}, w: {w}, b: {b}\")\n",
    "    if error_count == 0:\n",
    "        break\n",
    "\n",
    "print(\"Final w:\", w)\n",
    "print(\"Final b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this code, we first define a simple binary classification dataset with four examples. The feature vectors are in `X` and the corresponding labels are in `y`. We initialize `w` and `b` to zero and define the `sign` function.\n",
    "\n",
    "Then we run the Perceptron learning algorithm. For each training example, we compute the prediction using the current `w`, `b` and the `sign` function. If the prediction is incorrect, we update `w` by adding the product of the label and the feature vector, and we increment `b` by the label. We count the number of errors made in each iteration.\n",
    "\n",
    "The algorithm stops when it makes no errors on the training set, or when it has performed a maximum number of iterations (in this case, 100). Finally, we print the learned `w` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1, Actual: 1, xi: [-1 -1], w: [-1 -1], b: 0\n",
      "Prediction: -1, Actual: -1, xi: [1 0], w: [-1 -1], b: 0\n",
      "Prediction: -1, Actual: 1, xi: [-1 10], w: [-2  9], b: 1\n",
      "Prediction: -1, Actual: 1, xi: [-1 -1], w: [-3  8], b: 2\n",
      "Prediction: -1, Actual: -1, xi: [1 0], w: [-3  8], b: 2\n",
      "Prediction: 1, Actual: 1, xi: [-1 10], w: [-3  8], b: 2\n",
      "Prediction: -1, Actual: 1, xi: [-1 -1], w: [-4  7], b: 3\n",
      "Prediction: -1, Actual: -1, xi: [1 0], w: [-4  7], b: 3\n",
      "Prediction: 1, Actual: 1, xi: [-1 10], w: [-4  7], b: 3\n",
      "Prediction: 0, Actual: 1, xi: [-1 -1], w: [-5  6], b: 4\n",
      "Prediction: -1, Actual: -1, xi: [1 0], w: [-5  6], b: 4\n",
      "Prediction: 1, Actual: 1, xi: [-1 10], w: [-5  6], b: 4\n",
      "Prediction: 1, Actual: 1, xi: [-1 -1], w: [-5  6], b: 4\n",
      "Prediction: -1, Actual: -1, xi: [1 0], w: [-5  6], b: 4\n",
      "Prediction: 1, Actual: 1, xi: [-1 10], w: [-5  6], b: 4\n",
      "Final w: [-5  6]\n",
      "Final b: 4\n",
      "Number of mistakes: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple binary classification dataset\n",
    "X = np.array([[-1, -1], [1, 0], [-1, 10]]) # feature vector\n",
    "y = np.array([1, -1, 1]) # corresponding labels\n",
    "\n",
    "# Initialize w and b to zero\n",
    "# w = np.zeros(X.shape[1])\n",
    "w = np.array([-1, -1])\n",
    "b = 0\n",
    "mistakes = 0\n",
    "\n",
    "# Define the sign function\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Perceptron learning algorithm\n",
    "for _ in range(200):  # Limit iterations to prevent infinite loop\n",
    "    error_count = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        prediction = sign(np.dot(w, xi) + b)\n",
    "        if prediction != yi:\n",
    "            w += yi * xi\n",
    "            b += yi\n",
    "            error_count += 1\n",
    "            mistakes += 1\n",
    "        print(f\"Prediction: {prediction}, Actual: {yi}, xi: {xi}, w: {w}, b: {b}\")\n",
    "    if error_count == 0:\n",
    "        break\n",
    "\n",
    "print(\"Final w:\", w)\n",
    "print(\"Final b:\", b)\n",
    "print(\"Number of mistakes:\", mistakes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

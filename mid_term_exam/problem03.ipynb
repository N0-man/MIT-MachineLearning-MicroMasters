{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡💡⭐️ **Note** refer 05 Graident Descent.ipynb\n",
    "\n",
    "\n",
    "Stochastic gradient descent (SGD) is a simple but widely applicable optimization technique. For example, we can use it to train a Support Vector Machine. The objective function in this case is given by:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\left[\\frac{1}{n}\\sum _{i=1}^ n \\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})\\right]+\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\n",
    "$$\n",
    "\n",
    "where $\\text {Loss}_ h (z) = \\text {max}\\{  0, 1 - z \\}$ is the hinge loss function, $(x^{(i)}, y^{(i)})$ with for $i=1,\\ldots n$ are the training examples, with $y^{(i)} \\in \\{ 1,-1\\}$ being the label for the vector $x^{(i)}$.\n",
    "\n",
    "For simplicity, we ignore the offset parameter $\\theta _0$ in all problems on this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭕️ Solve below problem\n",
    "\n",
    "The stochastic gradient update rule involves the gradient $\\nabla _\\theta \\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})$ of $\\displaystyle \\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})$ with respect to $\\theta$.\n",
    "\n",
    "Hint: Recall that for a k-dimensional vector $\\theta =\\begin{bmatrix}  \\theta _1 & \\theta _2& \\cdots & \\theta _ k \\end{bmatrix}^ T$, the graident of $f(\\theta )$ w.r.t $\\theta$ is $\\nabla _\\theta f(\\theta )= \\begin{bmatrix} \\frac{\\partial f}{\\partial \\theta _1}& \\frac{\\partial f}{\\partial \\theta _2}& \\cdots & \\frac{\\partial f}{\\partial \\theta _ k} \\end{bmatrix}^ T$. Find $\\nabla _\\theta \\text {Loss}_ h (y\\theta \\cdot x)$ in terms of $x$\n",
    "\n",
    "For $y\\theta \\cdot x\\leq 1$\n",
    "\n",
    "$$\n",
    "\\displaystyle \\nabla _\\theta \\text {Loss}_ h(y\\theta \\cdot x)=\\quad ??\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⭐️🔰💡 Solution appraoch\n",
    "Given the hinge loss function:\n",
    "\n",
    "$$ \\text{Loss}_h(y \\theta \\cdot x) = \\max\\{0, 1 - y \\theta \\cdot x\\} $$\n",
    "\n",
    "For $ y \\theta \\cdot x \\leq 1 $, the hinge loss is active, and we have:\n",
    "\n",
    "$$ \\text{Loss}_h(y \\theta \\cdot x) = 1 - y \\theta \\cdot x $$\n",
    "\n",
    "We need to find the gradient of this loss with respect to $\\theta$:\n",
    "\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = \\nabla_\\theta (1 - y \\theta \\cdot x) $$\n",
    "\n",
    "Recall that $\\theta$ is a $k$-dimensional vector $\\theta = [\\theta_1, \\theta_2, \\ldots, \\theta_k]^T$. The gradient of a function $f(\\theta)$ with respect to $\\theta$ is:\n",
    "\n",
    "$$ \\nabla_\\theta f(\\theta) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial \\theta_1} & \\frac{\\partial f}{\\partial \\theta_2} & \\cdots & \\frac{\\partial f}{\\partial \\theta_k}\n",
    "\\end{bmatrix}^T $$\n",
    "\n",
    "For $f(\\theta) = 1 - y \\theta \\cdot x$, we have:\n",
    "\n",
    "$$ f(\\theta) = 1 - y \\sum_{j=1}^k \\theta_j x_j $$\n",
    "\n",
    "Now, taking the partial derivative of $f(\\theta)$ with respect to each $\\theta_i$:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\theta_i} = -y x_i $$\n",
    "\n",
    "Therefore, the gradient of the hinge loss with respect to $\\theta$ when $ y \\theta \\cdot x \\leq 1 $ is:\n",
    "\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = \\begin{bmatrix}\n",
    "-\\frac{\\partial (y \\theta \\cdot x)}{\\partial \\theta_1} & -\\frac{\\partial (y \\theta \\cdot x)}{\\partial \\theta_2} & \\cdots & -\\frac{\\partial (y \\theta \\cdot x)}{\\partial \\theta_k}\n",
    "\\end{bmatrix}^T $$\n",
    "\n",
    "Since $\\frac{\\partial (y \\theta \\cdot x)}{\\partial \\theta_i} = y x_i$, we have:\n",
    "\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = -y \\begin{bmatrix}\n",
    "x_1 & x_2 & \\cdots & x_k\n",
    "\\end{bmatrix}^T $$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = -y x $$\n",
    "\n",
    "Thus, for $ y \\theta \\cdot x \\leq 1 $:\n",
    "\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = -y x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭕️ For $y\\theta \\cdot x> 1$?\n",
    "\n",
    "For $y \\theta \\cdot x > 1$, the hinge loss $\\text{Loss}_h(y \\theta \\cdot x)$ is zero because the point is correctly classified with a margin greater than or equal to 1. \n",
    "\n",
    "The hinge loss function is defined as:\n",
    "$$ \\text{Loss}_h(z) = \\max\\{0, 1 - z\\} $$\n",
    "\n",
    "When $z = y \\theta \\cdot x$ and $y \\theta \\cdot x > 1$:\n",
    "$$ \\text{Loss}_h(y \\theta \\cdot x) = \\max\\{0, 1 - y \\theta \\cdot x\\} = 0 $$\n",
    "\n",
    "Since the loss is zero in this case, its gradient with respect to $\\theta$ is also zero:\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = \\nabla_\\theta 0 = 0 $$\n",
    "\n",
    "Therefore, for $y \\theta \\cdot x > 1$:\n",
    "$$ \\nabla_\\theta \\text{Loss}_h(y \\theta \\cdot x) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭕️ stochastic gradient update rule, where $\\eta >0$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct stochastic gradient update rule for the SVM objective function with hinge loss and regularization is:\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\left( \\nabla_\\theta \\text{Loss}_h(y^{(i)} \\theta \\cdot x^{(i)}) + \\lambda \\theta \\right) $$\n",
    "\n",
    "Let's evaluate each of the given options:\n",
    "\n",
    "1. $\\theta + \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] + \\eta \\lambda \\theta$\n",
    "    - Incorrect. This option adds the gradients, but the update rule should subtract them.\n",
    "\n",
    "2. $\\theta - \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] - \\eta \\lambda \\theta$\n",
    "    - Correct. This matches the correct update rule.\n",
    "\n",
    "3. $\\theta + \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] + \\eta \\nabla _\\theta \\left[\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\\right]$\n",
    "    - Incorrect. This option adds the gradients, but the update rule should subtract them.\n",
    "\n",
    "4. $\\theta - \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] - \\eta \\nabla _\\theta \\left[\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\\right]$\n",
    "    - Correct. This explicitly shows the correct update rule, including the regularization term gradient.\n",
    "\n",
    "5. $\\theta + \\eta \\frac{1}{n}\\sum _{i=1}^ n\\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] + \\eta \\nabla _\\theta \\left[\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\\right]$\n",
    "    - Incorrect. This option adds the gradients and includes an averaging term, which isn't appropriate for stochastic gradient descent.\n",
    "\n",
    "6. $\\theta - \\eta \\frac{1}{n}\\sum _{i=1}^ n\\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] - \\eta \\nabla _\\theta \\left[\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\\right]$\n",
    "    - Incorrect. This option includes an averaging term, which isn't appropriate for stochastic gradient descent. \n",
    "\n",
    "Thus, the correct options are:\n",
    "\n",
    "- **Option 2**: $\\theta - \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] - \\eta \\lambda \\theta$\n",
    "- **Option 4**: $\\theta - \\eta \\nabla _\\theta [\\text {Loss}_ h(y^{(i)}\\theta \\cdot x^{(i)})] - \\eta \\nabla _\\theta \\left[\\frac{\\lambda }{2}\\left\\|  \\theta  \\right\\| ^2\\right]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAC8CAYAAAC9gmnuAAAKs2lDQ1BJQ0MgUHJvZmlsZQAASImVlwdUk9kSgOf/00NCgIQISAm9CdIJICX0AArSQVRCEiCUEANBxa4sruBaEBHBsqCrIgquBZC1YsGKYsO+IIuCui4WbLj6fuAQdved9955k3PPfJnMnZl7z51zJgB0lkAmy0LVALKlefLIIF9OfEIih9QHRGADCawAJxDmyngREWGAyZj+u7y/A8iwvmkzHOvff/+voi4S5woBkAiMU0S5wmyMD2PrlVAmzwPA7cTsxnPzZMN8AWOWHCsQ44fDnDbKA8OcMsJ4/IhPdKQfxloAZJpAIE8DoJlgdk6+MA2LQ/PH2E4qkkgxxr6DV3Z2jghjLC9YYD4yjIfjc1P+EiftbzFTlDEFgjQlj55lRMj+klxZlmD+/3kd/1uysxRjOcywRUuXB0dimo3d2d3MnFAlS1OmhY+xRDTiP8LpiuCYMRbm+iWOsUjgH6rcmzUtbIxTJYF8ZZw8fvQYi3MDosZYnhOpzJUq9+ONsUA+nleRGaO0p4v5yvgF6dFxY5wviZ02xrmZUaHjPn5Ku1wRqaxfLA3yHc8bqDx7du5fzivhK/fmpUcHK88uGK9fLOWNx8yNV9YmEvsHjPvEKP1leb7KXLKsCKW/OCtIac/Nj1LuzcMe5PjeCOUdZghCIsYY/CEAwrAPB2LAEZzAHtwAu5k88bzhNwp+ObL5cklaeh6Hh3WZmMOXCm0ncRzsHJwAhnt29Em8jRzpRYR9YtyWg/UQ9z3WJ+vHbSllAE1FAFr3x20m2wAYhQCNrUKFPH/UNtxOQAAqMIAF2qAPxmABNuAALuABPljFIRAO0ZAAs0AI6ZANcpgLC2EZFEEJrIONUAnbYQfsgf1wEJrgGJyG83AZrsNteABd0AsvYADewxCCICSEjjARbcQAMUWsEQeEi3ghAUgYEokkIMlIGiJFFMhCZAVSgpQilUg1Uov8jBxFTiMXkQ7kHtKN9CNvkM8oDqWhLFQPNUMno1yUh4ai0ehMNA2dgxaghegatAKtQfehjehp9DJ6G+1CX6CDOMCp4Ng4Q5wNjovzw4XjEnGpODluMa4YV46rwdXjWnBtuJu4LtxL3Cc8Ec/Ec/A2eA98MD4GL8TPwS/Gr8ZX4vfgG/Fn8Tfx3fgB/FcCnaBLsCa4E/iEeEIaYS6hiFBO2EU4QjhHuE3oJbwnEolsojnRlRhMTCBmEBcQVxO3EhuIp4gdxB7iIIlE0iZZkzxJ4SQBKY9URNpM2kc6SbpB6iV9JKuQDcgO5EByIllKXk4uJ+8lnyDfID8jD1HUKKYUd0o4RUSZT1lL2UlpoVyj9FKGqOpUc6onNZqaQV1GraDWU89RH1LfqqioGKm4qUxXkagsValQOaByQaVb5RNNg2ZF86Ml0RS0NbTdtFO0e7S3dDrdjO5DT6Tn0dfQa+ln6I/pH1WZqraqfFWR6hLVKtVG1RuqrxgUhimDx5jFKGCUMw4xrjFeqlHUzNT81ARqi9Wq1I6qdaoNqjPV7dXD1bPVV6vvVb+o3qdB0jDTCNAQaRRq7NA4o9HDxDGNmX5MIXMFcyfzHLOXRWSZs/isDFYJaz+rnTWgqaHppBmrOU+zSvO4ZhcbxzZj89lZ7LXsg+w77M8T9CbwJognrJpQP+HGhA9aE7V8tMRaxVoNWre1PmtztAO0M7XXazdpP9LB61jpTNeZq7NN55zOy4msiR4ThROLJx6ceF8X1bXSjdRdoLtD94ruoJ6+XpCeTG+z3hm9l/psfR/9DP0y/RP6/QZMAy8DiUGZwUmD5xxNDo+TxangnOUMGOoaBhsqDKsN2w2HjMyNYoyWGzUYPTKmGnONU43LjFuNB0wMTKaaLDSpM7lvSjHlmqabbjJtM/1gZm4WZ7bSrMmsz1zLnG9eYF5n/tCCbuFtMceixuKWJdGSa5lpudXyuhVq5WyVblVldc0atXaxllhvte6YRJjkNkk6qWZSpw3NhmeTb1Nn023Ltg2zXW7bZPtqssnkxMnrJ7dN/mrnbJdlt9Pugb2GfYj9cvsW+zcOVg5ChyqHW450x0DHJY7Njq+drJ3ETtuc7joznac6r3Rudf7TxdVF7lLv0u9q4prsusW1k8viRnBXcy+4Edx83Za4HXP75O7inud+0P0PDxuPTI+9Hn1TzKeIp+yc0uNp5CnwrPbs8uJ4JXv96NXlbegt8K7xfuJj7CPy2eXzjGfJy+Dt473ytfOV+x7x/eDn7rfI75Q/zj/Iv9i/PUAjICagMuBxoFFgWmBd4ECQc9CCoFPBhODQ4PXBnXw9vpBfyx8IcQ1ZFHI2lBYaFVoZ+iTMKkwe1jIVnRoydcPUh9NMp0mnNYVDOD98Q/ijCPOIORG/TCdOj5heNf1ppH3kwsi2KGbU7Ki9Ue+jfaPXRj+IsYhRxLTGMmKTYmtjP8T5x5XGdcVPjl8UfzlBJ0GS0JxISoxN3JU4OCNgxsYZvUnOSUVJd2aaz5w38+IsnVlZs47PZswWzD6UTEiOS96b/EUQLqgRDKbwU7akDAj9hJuEL0Q+ojJRv9hTXCp+luqZWpral+aZtiGtP907vTz9pcRPUil5nRGcsT3jQ2Z45u7Mb1lxWQ3Z5Ozk7KNSDWmm9GyOfs68nA6ZtaxI1jXHfc7GOQPyUPmuXCR3Zm5zHgsbjq4oLBTfKbrzvfKr8j/OjZ17aJ76POm8K/Ot5q+a/6wgsOCnBfgFwgWtCw0XLlvYvYi3qHoxsjhlcesS4yWFS3qXBi3ds4y6LHPZ1eV2y0uXv1sRt6KlUK9waWHPd0Hf1RWpFsmLOld6rNz+Pf57yfftqxxXbV71tVhUfKnErqS85Mtq4epLP9j/UPHDtzWpa9rXuqzdto64Trruznrv9XtK1UsLSns2TN3QWMYpKy57t3H2xovlTuXbN1E3KTZ1VYRVNG822bxu85fK9MrbVb5VDVt0t6za8mGraOuNbT7b6rfrbS/Z/vlHyY93q4OqG2vMasp3EHfk73i6M3Zn20/cn2p36ewq2fXnbunurj2Re87WutbW7tXdu7YOrVPU9e9L2nd9v//+5nqb+uoGdkPJATigOPD85+Sf7xwMPdh6iHuo/rDp4S1HmEeKG5HG+Y0DTelNXc0JzR1HQ462tni0HPnF9pfdxwyPVR3XPL72BPVE4YlvJwtODp6SnXp5Ou10T+vs1gdn4s/cOjv9bPu50HMXzgeeP9PGazt5wfPCsYvuF49e4l5quuxyufGK85UjV52vHml3aW+85nqt+brb9ZaOKR0nbnjfOH3T/+b5W/xbl29Pu91xJ+bO3c6kzq67ort997Luvb6ff3/owdKHhIfFj9QelT/WfVzzq+WvDV0uXce7/buvPIl68qBH2PPit9zfvvQWPqU/LX9m8Ky2z6HvWH9g//XnM573vpC9GHpZ9Lv671teWbw6/IfPH1cG4gd6X8tff3uz+q32293vnN61DkYMPn6f/X7oQ/FH7Y97PnE/tX2O+/xsaO4X0peKPy3/bPka+vXht+xv32QCuWBkFMBhC01NBXizG4CeAMC8DkCdMTpTjwgy+j9ghOA/8ejcPSIuADs6AaIXAIRdBdhciY20WHxGEkAEA7N7AOroqFxj8+/IrD4savsAqh3sQiLD7vl4P4J/yOgc/5e6/6lhOKoT/FP/C70hCHdEGzoIAAAAVmVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAADkoYABwAAABIAAABEoAIABAAAAAEAAAO5oAMABAAAAAEAAAC8AAAAAEFTQ0lJAAAAU2NyZWVuc2hvdNIT4ZIAAAHWaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA2LjAuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjE4ODwvZXhpZjpQaXhlbFlEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlBpeGVsWERpbWVuc2lvbj45NTM8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpVc2VyQ29tbWVudD5TY3JlZW5zaG90PC9leGlmOlVzZXJDb21tZW50PgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4K/MhR9gAAQABJREFUeAHsnQeYNEXRx5sMSgYlSlBAESSIAhIUkIySlSBJRXKOKqAfoAhKkoySFZEoQTLyIpIkSM5RFBAQiYIiMF/9Wmrs65vdmb3d9729e//1PHuzO9Pd0/3v6uqq6uq+CQqjIBICQkAICAEhIASEgBAQAkJACAgBITAKEJhwFLRBTRACQkAICAEhIASEgBAQAkJACAgBIRARmLgKhzFjxlTd1j0hIASEgBAQAkJACAgBISAEhIAQEAJ9hcDyyy8/oD6VRi4pFllkkQEJ9UMICAEhIASEgBAQAkJACAgBISAE+geBu+66a7y328AgJ4Ur54jotxAQAkJACAgBISAEhIAQEAJCQAiMWARk5I7YrlPFhYAQEAJCQAgIASEgBISAEBACQiBHQEZujoh+CwEhIASEgBAQAkJACAgBISAEhMCIRUBG7ojtOlVcCAgBISAEhIAQEAJCQAgIASEgBHIEZOTmiOi3EBACQkAICAEhIASEgBAQAkJACIxYBGTkjtiuU8WFgBAQAkJACAgBISAEhIAQEAJCIEdARm6OiH4LASEgBISAEBACQkAICAEhIASEwIhFQEbuiO06VVwICAEhIASEgBAQAkJACAgBISAEcgRk5OaI6LcQEAJCQAgIASEgBISAEBACQkAIjFgEZOSO2K5TxYWAEBACQkAICAEhIASEgBAQAkIgR0BGbo6IfgsBISAEhIAQEAJCQAgIASEgBITAiEVARu6I7TpVXAgIASEgBISAEBACQkAICAEhIARyBGTk5ojotxAQAkJACAgBISAEhIAQEAJCQAiMWAQm7rbm77zzTnjyyScHFDPzzDOHqaaaasC9bn/cfvvt4fzzzw/bbrttmGOOObotTvmFgBAQAkJACAgBISAEhIAQEALjDQKvvfZaeP7558MHP/jBMOuss9a2+4UXXgivvvpqmGGGGcL0009fm74qQafvrCpjKPe6Xsn9xz/+ETbaaKMBnxVWWCGst9564e677x5KnSrzXHHFFeHqq68ON998c+Vz3RQCQkAICAEhIASEgBAQAkJACAiBagSuueaaaLMdfPDB1QmyuyeeeGJMf95552VPmv/s9J3NS26fsuuV3LT4ddddNxRFEe67777w6KOPhu233z6cc845jTwFaTlV33fdddew4oorhsUWW6zqse4JASEgBISAEBACQkAICAEhIASEgBAIPTVyd9999zDZZJOFd999N2y44YYxjPnWW28Na6+9doSa7zfddFN49tlnw7zzzhs9A1NOOWUgFPniiy8OhDlvt912MS1L44cddlj8vueeewZWcu+9995AePQyyywT7z/xxBPhuuuuCw8++GCYZZZZwvrrrx9DmVldPvLII8PEE08c9t133zDhhBOGs88+O9x///1hiSWWCGussUb45z//GQ455JAwwQQThH322SdMOumksUz9EQJCQAgIASEgBISAEBACQkAIjA8IYE9hhz399NNh2mmnDauuumpYfPHFBzX98ssvj3bXnHPOGdZaa60w22yzlWlefvnlGHF75513xlDolVZaKdpcZYJh+NJTI9frP9FEE4XpppsuGrmvv/56vH3BBReEgw46KBqc00wzTbj22msjoGeddVaYffbZA8BhcG6yySZh6qmnjsbwZZddFuabb764vxfQrrrqqjDPPPNEI/eBBx4I22yzTXjzzTdjjPiYMWMCS+ksqy+44ILhlltuCRi7G2+8ccxz5plnRuOaDsTIxWD28mXges/pKgSEgBAQAkJACAgBISAEhMD4gMA999wT7am33347zDTTTIE9uBi8BxxwQFh99dVLCLDbHnvssWirEbWL3XbaaafFvbrsud16660DxjI2HguJF154Ydhtt92iHVYWMo6/dL0nN63vlVdeGQ3HH//4xwGjFCK8GOue2G+8AzSavbVf//rXw3PPPRfOOOOMuIK78MILh/feey8ap+S78cYbuYRVVlklXvM/+++/fzRwjzjiiGj8HnPMMYEO4jfGsnsg2BdMh7F6DLHqi2FMp0Ks7IqEgBAQAkJACAgBISAEhIAQEALjEwLYYqy6Yrtdeumlge2h0CWXXDIABhYOWbDEuJ177rmjDef7dE844YRo4G666aaB/bcYySx2HnvsseGNN94YUM64/NHTlVys/pQIM/7kJz8Zrr/++mjAvvLKK+H000+PSTB8IUKIIYzZu+66K9xwww1x7y0HTGGsrrzyyvF5+gfAHn/88XiL9HwgwpIfeuih+K4ll1wyhjhj5HKCGPSZz3wmhkZj4PqhWKQTCQEhIASEgBAQAkJACAgBISAExicEsL/YQsrWUbaJ+gG/bqc5Fthj/t9t1llnnXD44YeHhx9+OD52O4woWz/Qiu2rlMHq7nBRT43cPfbYIxqchApDq622Wrxi/Ts988wz/jVgYPrx1Rwqdeihh0ZwCSVmTy6ru+y1zSkFPi3PV2/feuutcoUWw3mKKaaIBvBOO+0UNttss9iRvIMw5UUXXTQvXr+FgBAQAkJACAgBISAEhIAQEAKjGoGTTz45HH/88dEmmn/++eMW0qeeempQm33BkAecpwRhq0EvvvhivBI5O8kkk8Tvc801V+BDlO5wUU+NXCz7DTbYIGy++eYBa/6UU04Ju+yyS1hggQVi+/AUHH300XGFFkP1kUceifHfPGRZm9BhDqZiXy3ExucqYg8v+3ZZ0T3wwANjXg915j4dweejH/1o9CCwX5d9uqwqY1RfdNFFMWSZ92k/bhXCuicEhIAQEAJCQAgIASEgBITAaEaAM4sgroQhE47sK7Npu2+77ba4d5coWw4Shjg3CeJAYGytHXfcMfBvZCGiZjmImDTDtZrb0z25NIrG77DDDnyN/z6IWO+PfexjYaGFFor/VmjnnXeOxi+HRvEvhjBqnXz/LeBxeBWru1XEOzixGcOWck466aS4uZmVWgxrJw9FZiXZv3P1lWDtx3WkdBUCQkAICAEhIASEgBAQAkJgfEKARUbo3HPPjXtu2UdbRYQmb7XVVgE7jv94wxbR5ZdfPiZdc8014+8f/vCH4bjjjov/vWbLLbcMe++9d7TnqsobF/d6buRSacKGMSA5CIolcIA46qijIhjshQWA559/PgAAK79OgOUrq5ThwPvz9Iq3gNBjTm9mwzOGMUbxfvvtVyZzw5Yb/t2v6b0yg74IASEgBISAEBACQkAICAEhIATGAwT496+ciHzOOecEDvFdd911K1vNv4ZlkZGDgWecccaw1157lYf8LrvssvE05hlmmCGceuqp4fzzzw+f+MQnYnnszR0umsCOgS7yl7PkvMgii+S3e/Kb1VdWUgGiV8RKLR3E6q9ICAgBISAEhIAQEAJCQAgIASEwPiDA+UPd2G2Ygi+99FJcXKyzpdgqypZQDN4q4t8JsWA5+eSTVz0ea/fAwFeW/SU93ZPrhba7sqrbSwOXd00//fTtXqlnQkAICAEhIASEgBAQAkJACAgBIZAhgMHK6mwT8kOnWqXlbKR+obESrtwvjVM9hIAQEAJCQAgIASEgBISAEBACQmD8QkBG7vjV32qtEBACQkAICAEhIASEgBAQAkJgVCMgI3dUd68aJwSEgBAQAkJACAgBISAEhIAQGL8QkJE7fvW3WisEhIAQEAJCQAgIASEgBISAEBjVCMjIHdXdq8YJASEgBISAEBACQkAICAEhIATGLwRk5I5f/a3WCgEhIASEgBAQAkJACAgBISAERjUCMnJHdfeqcUJACAgBISAEhIAQEAJCQAgIgfELARm541d/q7VCQAgIASEgBISAEBACQkAICIFRjYCM3FHdvWqcEBACQkAICAEhIASEgBAQAkJg/EJARu741d9qrRAQAkJACAgBISAEhIAQEAJCYFQjICN3VHevGicEhIAQEAJCQAgIASEgBISAEBi/EJCRO371t1orBISAEBACQkAICAEhIASEgBAY1QhM3Kp1d911V6tHui8EhIAQEAJCQAgIASEgBISAEBACfYCA7LbBndDSyF1++eUHp9YdISAEhIAQEAJCQAgIASEgBISAEOgLBMaMGRPGd7sNDHJSuHKOiH4LASEgBISAEBACQkAICAEhIASEwIhFQEbuiO06VVwICAEhIASEgBAQAkJACAgBISAEcgRk5OaI6LcQEAJCQAgIASEgBISAEBACQkAIjFgEZOSO2K5TxYWAEBACQkAICAEhIASEgBAQAkIgR0BGbo6IfgsBISAEhIAQEAJCQAgIASEgBITAiEVARu6I7TpVXAgIASEgBISAEBACQkAICAEhIARyBGTk5ojotxAQAkJACAgBISAEhIAQEAJCQAiMWARk5I7YrlPFhYAQEAJCQAgIASEgBISAEBACQiBHQEZujoh+CwEhIASEgBAQAkJACAgBISAEhMCIRUBG7ojtOlVcCAgBISAEhIAQEAJCQAgIASEgBHIEZOTmiOi3EBACQkAICAEhIASEgBAQAkJACIxYBGTkjtiuU8WFgBAQAkJACAgBISAEhIAQEAJCIEdARm6OiH4LASEgBISAEBACQkAICAEhIASEwIhFoKdG7ptvvhkeeuihSjBeffXV8OSTT1Y+G46bf/nLX8Lf//73ylc/9thj4fXXX6981vTmn/70p7D//vuHn/70pzELZYJPv9Ebb7wR+4x+q/o4Dv1a/07wfPnllwP9PhR6+umnw6WXXjqUrOHmm28Od95555DyDmemv/71r+Gll15qVIWxPfb/9a9/hZ///Odhzz33DA888EB45ZVXAn0yEui1114LjJ9+opE2nv/973+Hhx9+eMgQjqT2Iof7Za6ow63bfhlyh/YwI3MC8mQ46e23326pOw1nvXr57m7m0F7WQ2UJASEw/iDQUyP3kksuCWussUb4wx/+MAjBE088Mfz4xz8edH8oN5h4zz333KFkLfNssMEGYZtttil/+xeUC9rwzDPP+K2Or2PGjAlbbbVV+MAHPhA+/elPB5Tc1VZbLWBk9Rv99re/DZtttln8gMmXv/zl8jf3waGf698Jnocffng46aSTOslSpsVZcfbZZ5e/O/myyy67hNtuu62TLC3TXn/99eHGG29s+byXD77xjW+EP/7xj42KHJtjvyiKsMkmm4QbbrghfPSjHw0f+9jHwnHHHReOOeaYRnUb7kSnn356dHj1oh7IJ2QpxsVQaSSO52uuuSbstNNOQ2rySGrvn//85zj/9IOR2wS3bvplSJ05FjIx791zzz1joeTqIqvGMDrT1ltvXZ1hlNztZg4dJRCoGUJACIxjBCbu5ftcIT7llFPCsssuO6Bonq255poD7g31xwUXXBCef/758JWvfGVIRaBIkJ8PK2yLLrpoWc7tt98epppqqvDxj3+8vNfpFxTw7bffPnz961+PWX/3u9+FWWedNcw222ydFjXW02+44YaBD3TUUUdFY+wXv/jFgPf2c/0HVLTmBzyIwTkUwkEz4YRD8wlhlE48cW+GGooCStnYJqIcnnjiibDEEks0etXYHPvgx2oLBv5kk00W63PLLbeELbbYolHdhjsRdV1qqaV6Ug3k0xlnnNGVQozDpV/lUSuQwHDJJZds9bjt/ZHUXtqJE2fGGWds26Zx8bAJbt30y7hoQ9070AWIVvnMZz5Tl7Rnz6vGMPKzqaztWUXGcUHdzKHjuKp6nRAQAqMEgYn+zyhvy1NPPRXmnnvu/Hbtb8JzUcAvvPDCsPrqq4cZZpgh5sFzeeCBB4Y99tijnLwJ4Tz++OPDWWedFR588MGw8MILlwosma699tq46nbZZZeFd999N8w777zhvffeC9/+9rcDRhghXc8991z44he/GA477LDwzjvvhDnnnLOsIx5mwkurJo4rr7wy0MZPfepT4f7774919Yys1rEC+6UvfSneYhXp6quvju+8/PLLwwQTTBBXkzx9eqWs733ve3GljTBfVkE/97nPhV//+tex3SuttFJMjvFwwgknRGUVHMB6+umnjyFq3/nOd8ICCywQpp566rJoVm4wyOebb75QV5+DDz44fPCDHwznn39+DO9kVbqpcYaRS30XX3zx8t188fpTzs9+9rNw1VVXhQ9/+MNh5plnLtP985//DBdddFE4+eSTY9/NPvvs4UMf+lD5PP+SpmeFbqaZZoplerp2/HHeeefF0Hf6EGyuu+668JGPfKTkN8pg1ZzVflZu77vvvjDXXHOFI444IsDuRALQVnjH6T//+U/sO+qdK5j05b777hsdN5NMMknMUsWfXlZ6xYkCj2PkUM4+++wTlllmmeDlwMeMgy984QsxG/UmLJfVv8cffzxQH3iBtsL7N910U/jHP/4Rw+sWW2yx9FXxu2PDygRthGcYh/DiOeecE/vn7rvvDgsuuGCYYoopyvxgcuaZZ0aeZAUHI/fRRx+NEQllojZfOhn79D0yAmcYYf3wPvxURay+f/e7341j49lnn414YOj+8Ic/jH0CNvTdxRdfHHkTBxgYIk+g3/zmNwFFPHVksW3ikEMOCUsvvXTsB8KyGfennXZaQOmdY445wpRTThnz03/gxrg/8sgjYz0+8YlPxGdN/hCGCM/tsMMO0clF/xB2O//885fZ6SeIvoZ4Dl9TJxRwZB/8wr299torsMUAox9H3LTTThvz+B/nsUUWWSTyFX1KGaR1R4uP5zp5RJnefhx/vB98wb/d/EDoJ/xL/15xxRURS5fNTcpD3tOfyBLGNnXHcbj++utHLLyt6TXlb8b7Jz/5yZK/vb1DlV/wCm12JyXOlqOPPjqsvPLKZRU8DVh3IlvKAt7/AmZgu8IKK+SPArzDuGbsQowHxt0888wTpptuungvTVM3V9TxouPmfNJpv1A+Y+2pNnI6nQeQqem8ceihh1bO60Qf+byOLPvVr34V5Rb6AHzv812TuRBdAFkHv9Cn/KavnV8Blb5FzjBHQcibvffeOyB/O51LWo3hn/zkJzHaC3kI399xxx2xTugiTimP5zLcxxV1J8IFLJH9bPNgzkaW1I3bVnqJvz+fUyn/+9//flh++eXDRBNN1FYO53NoE97w9+oqBIRAPQLI2Xbzcn0JIz9FFQZDW5qqwMJXRwm5IUT31FNPLVPlq6N49DDmUBpQ7pm00tBDJhUUSoxQymJCuffeewMKoyuvO++8c2DyRfnkGe9wQqCiGKeKrT/j6l7Tb37zm3FSY/Jw4pmvGGA477777tHoIEwSRWLHHXdsGS46+eSTRwOZsjbddNOoRPM99XZjfKOsQaussko0ZHbbbbf4G6MDw5yOcmLSY5KiTnX1wThhggQ/0q699tpx8vGy2l3BlonSlYc0LfV/5JFHIm4LLbRQdEpst912ZZIXX3wxbLTRRvE59XzrrbfCV7/61Zb7mknP6jErdBjU/N54443LfVF1/MFKFu0EG3jk1ltvjQ4UrxBGC6v88AUGJYrIt771rbhCgsEHNkz6KCtOGMMYdUzcOVE+qxqucLTizzwfv3GMvPDCC/FRXg434QfnP66rrrpqQPHDyUL9USIgDBR/P7xV1U+kAxvGEnk///nPR4ONfgVfFHAiLGhLGhpHqByh6bwXYxwn0rHHHluOA8ptR52MfbDwvv/sZz8bHVW8m7pVkRuU8BV1RwlOV5hQ4ljRRSFHVsAPBx10UFTyKA/Flz5ICcMABwzjDbkB7/7tb3+LDh4cWsgWDASIvBjKYILsQYnuhO66664oozwf/eNlUw48iOLrTi34gQgQjH54gfdiqEKs8EGOhSvd8eb7f+AxIi923XXX2D43djESnZrKI9LTfuQzDgDGBoouBnur/mLfHQ5ODHUMNRRreI18TcojDHvbbbeNDirqjsOOrR/togqQA8gbeAv+xWlGGU7dyi/kFLIBwtCDf+AJvkMYvRjl8F6nsiUWkPxJ+ya5Hb8ig3FyOTFHUg94F8L59YMf/CDyad1cQfo6XkzrMpR+qZPTdfNG3bzOuMBxjlxkHmFO8DMwms6FzPfoCzgmcB5Q1pZbbhnnODDiGXODO+y5x5hmSxLOuU77u2oMU1cc/cz9yGCc3Gz/+NGPfsTrItXJcMYpeXBCYbDTd4T3owdRT5xktAvHaRW100tIj1MNvSWdU9nOwm/KrpPD+dxXxxtVddQ9ISAEhEDHCJjCNYjMCzjoXt0NUzILMzpjMlt9LWylojAPafxtRkFh+1/LIn7/+98XpiSVv83gLUwYx9+mDBW2clGYUlM+N69lYRNB/E1eU2TKZ3yxFa7CDOHynikhhSmq5e/8i61WFqa4xdumSBa2KhS/2wRTmCJX2IQTf9skU5ghXNhEVhZhB98UFvJa/s6/0FYzWsvbduBWYZ72wgyveM9W7gpb8S2f24pWYSuK5W/zihY2UcXfvNf2yBbmqY6/6+pjE1VhE2RhxlJZXtMvNinGvGbsDsji9TdlqrwPdmbslr/XWWedso7cpN5mwBSt+MiM78JWL8v8NkFGzMxYivfa8YfXx1aNy/w2YRbLLbdc+duU7MIUnvK3RRLE+toqe7xnhk9hTovCVkLib66mUBemRJZ50i/wB/0O1fFnmo/v9J+tqsfbaTme7mtf+1px6qmnxp8HHHDAAL41pXUAr+S85WX41bExhcdvxXFmoXgDyjEDJLYffrcIgcKMt8IMhTKPGX7xuSld5b12XzoZ+/S9GVxlcWYoxL6z1e7yXvrFx6TzBs9SHM2hUlhkwYAxyhgyoyMWw/hKecNWauJv8iGDzBFWwPtOpszFttsKVLxF/9lKrD/u+GrKc0EfQ8iwVBZwz5S/wozzwqJU+FlY5EWRYmFKaTmebQU3yqcUi5gp+QM2yDfnbR798pe/LGWM80hTeUT7XTZTFmMkHTvcS8kMzcKcJuUtM8BievgMqisP2c1ckcpcc+oUtmpalpl+oQ8Zu2aslLftYLLCjJ/429vbjfwyB1FhK6yxPPqG+oEBcoV+M4dUOcY7lS1lpe2LOTdjuWb8pbfL72bAFeYwiL/N6I58zLvN4Ir3kB8up+rmijpedNycTzrtF8/fTk7XzRvoBfm8boZbbKut8BfmJI7yON6wPxY9UKy11lrxZ9O5kLFijusCOeRkTrjCIkjKcsxZ6I/i1VZ8y37otL+rxjB1tWiFwhxE5Xsscqgw5078DY+3k+EkYlzRR04WtVTYSnOJj49D+KaK6vQSO1OknKfIj/xk3vA5tU4OpzK7CW9U1VH3hIAQaI1AK327dY7R96QKg95sFDTT2ldHsbIJ42KlBE8r3kS8eCaESwOcFRxWY/Aq4vknpI09rBCee7z/rPQS+kxIKauHTql32e+xvwxPJ0T4J151PJtV5KtOvhKGNxLvN/VkRcD34+LBZXWIEFxCcZzw6LY7iREc0j2T6aoTZRB6SKgRIax4R1k5YpXYidVtPNwQaSaddNK48tWkPrybA67Ao1MiLyvfHkbr+ak/YcnpfmpTQMvQPfqD9tC/rPY4sUKVh1LyjDBkm7Tjyqqnpb8JOXdqxx9eHw8nJ09aH1Zx8IazuufEih0hbN7n8CYro6z8zDLLLHHlj5VO+qaKaKOXV8efaX76DA+9vzcth3RgBN8SwgzhcSeMi9A1xhArhx6GyPOct7iXkvMaK4BOeMzBgzHIB2LFB75i1YJQ6RVXXHFA1IOvGHu9vaxWV+rladuNfe/7dBWZfqGNREFUESuthI8TQuyU4kg+VvwYR2YkxFV7ZIqPqXQ8sdrAWOdDPtpOqJ3XnfI9VJ3nvsKSroJ6HZpeqStbACDwz/fCgh38TkQKBA+wsmtKY+wXbwfPCGHMseB+Srxv8803j3zt91kl9j51HvHQ23byyNtPGKcTPMt4dZz8vl8ZW+RjhRH5Rvt4P+nryjPlN/I/K8epzKXurF5XEWHR9GG6xYJQcMYQ5PKiG/lF6DormfAPPGNOg7haDhasgNF3RM1AncqWmOn9P/Rdu/24Xg+SE8bPnMW2FOpB9AqYM+81mSvqeDHlk6H0i+PeSk7T1rp5Ax6F56F8XifaZ5ppponbHmIC+4MsdXkJ39XNhegCzLWc2O4hzpRFub5dIZVt/h7qjsyEOu3vqjHMO4h+YMuNE1ECPkbrZLiPq1ROwQ/mRCjnYNrJOGoVzthODhCpgDwi4sYJ+cgYddlZJ4fBzOfQOt7wd+gqBISAEOgWgZ6FKyPEXBFB+CHQUAYIUyOkxYUhkzIhpIThIXgJrUEJ97wYWSgO3CeEiJA3W9kr21k16aA4sq8GslWXGKrH5FNF1DNVJMzzG5U29t1RtiucCHYmmXRvDuUx2bYyhth7zH4wbwvp0/qCAwoZYUiEv2FgYWw4NqSnLeACboReoZCjRDWpD21zhZqyOiHqmdbb83KfMlMlAAXJ0xJiRf1QGDAy/AOuHprlZXFljzV9miqx6fM6/qiqD5OmY8ieKspP64uyjeLnaXg3oZ7gTPgf/UIIZhXlhmodf6ZlpIpkXg7pCHtDOfCQXBw99Df/IoeQXsLpzdcWi2SM5LyVvovvKa/5M5w0jAXvF64oRX6yOHj5njvPw95y9vmlIXr+rOradOzT9yiHad9gPFCnqjBx3pWWze8UR7AhnBtFGkcZIfcoijiqfM8r4wkFjX2sKIEYxIRxQxgEbBlIifGNoUnbXdH3vbJpuibf8y0AVf2Tt4+tCTjJCD0EK8LGnfK0ft+vjk2615xn9KfzWFqHOnnk7U9lIE6HKmcY70Fm2ap1DE8mL7yNcYtMpc/rysNBBdapQ4NzGFJHEe9JiT7EsdKKquRFp/LLjUvmLOQ3PIahj7zH2GSvvPN0J7Ilr3PaN/kzfns92JeLvEXGgjF8RugyJ5Aj15rMFVXvSvkrfT6UfqnCnf53Gdxk3mg1rzPXM07hE2SHfzBw3dlAW+rmQtIgd1Lj0vmNfeBQigm/8zHdaX/n5fk70IlSgkcdqzoZ7uMqlVO8x/P7Ozhcq2rerZMDtjoSZabzOOXBf8wjvKNODrtc8vrU8UaKg74LASEgBLpBoCcruQg8DkZyIUaF2CPFxMuBPb46ysoZSikGJUYshEGLh9GVUrzSTNQc4ERelHEOOmLvrhuRKBYpMRlSNpMB+dlj14rSyZs0GJkoB6wI4MFdb731YlaMClbtUmJPFN7ndP9w+rxq1Yn3sbICsbcYw8U9okzWHI7hBiNpMKxZ4WYlgrq4clpXn3wioaym5BM3+/hySuvvz7iH9xvCeODE4tRIxAtdNZl6evdQxwLsD/v3MCpQMOr4g8kbB4oTEywKgd9j4k2VctKxwozBnRpt8AwHtxBtgGMkPYTJy+aaGqr8bsefPE8p5TWUMig9rAsnkDtVeEbZrIjzQfFAYcMQZ38mKwDUPzUAyJNSjg3PMCJxlvAep7R/wMtXLXjOM1YOUp70fFXXpmOfvIzfvO85HIyx7/IgfwcYtoqMwEDHAYYS5mPVwuJKo4qykCXwImOTyAxWCZ2qxhT7+hifODPS/vM87a7IKRxgvlpy3XXXxT5jrzCE8yLtB/aK8w5fyQd73ovM48MBZOwd9ygX0iJXWxG8yhhCjjnRv0QHYKBBlNFUHlW1v0pJ93ex4ouDgHe5QswKKsaYvzudI7iXlpfzIs/ZX8yqVJ6PZxDyx/ue37QfGQ1PYQCm7eU5xL1O5Bfjg3EIr7FaCmHkMk5YaWcfcEpNZUuah+9g4byQP+M39WA1mbmBvahgTBuRFTjrmEuhKr7O5646XkxxG0q/5LIol9NN5o1W8zp5abuv1tNmyufD/aZzIW30vfCUASFTwJT9/5SH3E5lFk5Ynvu8TJ5O+pt3pmO4qq7oMhjuHp1QJ8MpMx0fOHXRx1JZk6eh3k51eglyOx1j5MNhCO8zJ9XJ4XwOreMNr5euQkAICIFuEejJSi5CK10dpVIIRZRTVlhckedgHSZpJhYmEBRPDAyeMzmhzBCa7AYBEzEhSB4ahPAmNNU/3ngmGUIwCWFiJYyyWhF1zRV4jFyUGFY8fLLAu8tKG15SFCdCmTm8CCUkF/j+rrxsJjDa4GVyeAoGPwQWHGKFke0GPvdpC5MCh50QjuZUVx/35vqEjMLMATxMbnUExqzGEh6bUl5/nuUTMPXCucB9iD5FQaDdVYSRRjg5h6XAAyiNHKTE++v4o0ohQHljEmaFDsKoYZWASZ4VPIwWjJu8z8GZE64xSpZbbrmYt+oPfeH9V8efef6UH6gjfER7aSfh2RhnXi8OCEHxhrdR4kjHWPCIBJQeDHFCB8EtpypsSIN3H0Pe308IPOOJ3xAGNIYUv3G6sCqF08HbTBpOQmbsVVHTsU9e6oKRiXMBxY02oiy1UuypU756nSprjCcUTsY7DgIMZhwHad19pQV8ccSkpzjDu/AG2LEKicOJUGc3AnmX9w/1h6eoq/M691LC+cUhNTiNwJCIDYxVd/jQHowRTpzlSn8jC1GY6VNWJDkpFuJdhGC77OMePEA6yq8i6os8oY8Z/7yH7RicsI6MycdznTzK28+70/GQ18HLIx08bHtEo0x1DOvKo28wUJFH1J8VSRyd7aIKkCdghmHH3MIp/swDGKF5e6nvUOQXxiXGzbrrrluGacN3GLlsq8mpSrbUyeMqZ1FeLvVgVRteZ0UZoh448ZhLPCS9bq4gXztezHHrtF/In6++53K6ybzh8zoH/dGvPq+zDYa24pgGV/oePgEDKJ8L482KP8gu5inmfcrBoeLOdeYj+In7zCe29zbKF8a46zNeZFV/+7P8mo/hqroyTlhh9tDrOhmejyvaxXh3Zxdt4D2pXEzr5eOWe1V6CQdywf8caoVeglGMw8fHdZ0cTmVGE95I66bvQkAICIFuEOjJSi5CtkqAoiwyAbswJAQIYwSjggkbY4iVLc+LMMfzzyl+KIakQan0CZ2JkZMHUQYJ6/OwPMJ+MSAIW0wNxhyYVooEqw+smrES42FKhDqxOoiSiNLG6gyrjITCtiJwaLXqRB5WmFFsUf5oCxMZRq9P3qTBSEURY1+eKy3cr6tP3gesBqOEpd5cyqki8rLaxCpSSq0mYAxDn4A5xZTwOUJAUSwxylBIWq3MsR8TJZaVedKTDqUcjzBGXDv+aFUfwidR7iFOfSYd4V/gx4mSOGCcx7x94IxytN9++/mtyisKgxs9dfyZFpBP5vQf/3ICI5L6cKI2POv1guf59zC0BX4DH3gAZQvCq0/4Pv0E3rmjpQob8uH0YYXe+xdHBmPHeQuljhPEqRtKEQoMTguvF8YaY5CQxSrK+c7T5GOf+xh8OI4Yz/Q1yhMGylxzzeXZBlyrIiPS/iDMGvlC21D2cZYgN1zeeGH0NQorzqyU7KCWGIXAGEHWUC94EWxyRZ98rJThtEtPFk/LwylFHxKpAj+zAkv4rhOnJoMlMo66wxM4ZbyPkTVEv/CvUyDakzq6kHso8yiVGO05gQ19CabwDv3JXnOPssh5pJ08qmp/bqjk74eHWWHEkQGerOBixFcZ2OTNy0OeE8VAvZBFyBSMfP/XR/n7+I1MZiwRikkews/hbzDN20t6+LVT+UVbGG84OZ3gN9qHAZ5TlWypk8f0Xe4ozstlrsDBAY85IUOJXElXB+vmCvK248Uct077Jc/P+8A9ldNN5g3mdWQhTpp0PsHIZY7B+PXVXMYUTl1/l8uveKPiD7oADkPGGhFT8BgrkxixnhcZhQ6A4wzdgPGIHlAlX5rMJVQjH8NV8hNe8DqQp50MrxqneZk4HDB007NNKNepnRwgDXKTczRwKjCe4R3mcK9jnRxOZXYT3vB66SoEhIAQ6BoBm0QGUdUJVYMSdXGDk/4sBKplCbbKM+D05TyhTU4DbtmqSGFGTWETzYD7vfhhSkVhB031oqhYBmXZKs2Qy+ukPrYqOOT3dJoR7G01rHE2+r9Vf9XxR5OXcCKlrXhUJrUJv+DEVJu0K5/X3azjT/Jzwrg5RAYVZavLg+6lN8zB0bbeptSkyRt/Bw9bvWmZHp40o3vQc07s5NTWXhJ9b0ZnT4qkLzmVlD4ZKoEN5TShujEFhpTXisjfii/JQ374v6oveG4rpANOHuYeBKbpyc1NZUy38ui/b//fX/rVVv0b4/m/nP/7Rhv5dEIWhVCJS9MyOpVfrcptJ1vqeKdVmUO9XzdX1PFi/t6h9EteRv67W9wZK93iiuxoN3e1k1Xt+jtvq/9uNYb9eatrnQxvla/J/XZyIJetZrTGE81THa4XcrhJPZVGCAiBagTGtt1W/db+uluFQU9Wcju1tFm9aEesyrG62orwJkJ4Ywk1wtNpR+aXq1Ot8g3lPqsDHvYzlPx5nm7L6qQ+HiaZ12Fs/Gb1y1cHm5TvYdtVaev4oypPfq+Kf2w4xtOdWakEx3xlLy+j1e86/iTMEn5kFSAnVoDakYcnV6VhxZ9VpaFQFR5pOWkYr983BTSuYBBe3ktq1/edvgdMiIrohuqwScuuG1OsILYrr0n+dvyfr+B73QgJJGySVUSoqj89bXrtVh6lZfGdlUUiHrqhVm1sVyYr591Qp/Irf1cT2VLX93mZ3f6umys6rc9Q+qWuDd3i3m6s1L3bnyPPPRLI76VXeDqnJv2d5/HfQ8WxnVzxsod6bSUH2G/LtgvmM1axia5hFZ1ollSO90IOD7XuyicEhIAQaIXAsBi5rSrT6X0O1CG0l5AjD53ptAylH38QYL8fBxNxUAxhjmmYeK9QYB8wJ+ISpkp420gmWyWJDqSxqVyNZHz6qe6Ew+YnRfdT/UZ7XcaFbBntGI6k9o0v/b300kvHrQCE6tuKbgypJ1yeEHKREBACQqDfEZiAxea8kmPGjCn3webP9FsICAEhIASEgBAQAkJACAgBISAEhh8B2W0hVGHQ+hji4e8z1UAICAEhIASEgBAQAkJACAgBISAEhEBHCMjI7QguJRYCQkAICAEhIASEgBAQAkJACAiBfkZARm4/947qJgSEgBAQAkJACAgBISAEhIAQEAIdISAjtyO4lFgICAEhIASEgBAQAkJACAgBISAE+hkBGbn93DuqmxAQAkJACAgBISAEhIAQEAJCQAh0hICM3I7gUmIhIASEgBAQAkJACAgBISAEhMD4iQD/mOfwww8P7733Xl8DMKL/T25fI6vKCQEhIASEgBAQAkJACAgBISAERjgCGLZHHHFEmGCCCeLn2GOPDZ/73Ofip1+bppXcfu0Z1UsICAEhIASEgBAQAkJACAgBIdAHCCy55JKxFhi8I4Fk5I6EXlIdhYAQEAJCQAgIASEgBISAEBACw4AAK7hLLbVU2HXXXYfh7UN7pYzcoeGmXEJACAgBISAEhIAQEAJCQAgIASHQhwjIyO3DTlGVhIAQEAJCQAgIASEgBISAEBACQmBoCMjIHRpuyiUEhIAQEAJCQAgIASEgBISAEBACfYiATlfuw05RlYSAEBACQkAICAEhIASEgBAQAr1G4Oabbw58mtAuu+wSJpxwZK6Jysht0sNKIwSEgBAQAkJACAgBISAEhIAQGOEILLHEEo1bwIFTI5Vk5I7UnlO9hYAQEAJCQAgIASEgBISAEBACHSDAyiz/43a008hcfx7tvaL2CQEhIASEgBAQAkJACAgBISAEhMCQENBK7pBgUyYhIASEgBAQAkJACAgBISAEhMDoR6AoinDkkUcGrnfccUds8DHHHBP39u60005h4on7z6TsvxqNfj5RC4WAEBACQkAICAEhIASEgBAQAiMGAfbyskeXUOftt98+fsfo7dd9uzJyRwxrqaJCQAgIASEgBISAEBACQkAICIFxiwCG7FJLLTVuX9rl27Qnt0sAlV0ICAEhIASEgBAQAkJACAgBISAE+gcBGbn90xeqiRAQAkJACAgBISAEhIAQEAJCQAh0iYCM3C4BVHYhIASEgBAQAkJACAgBISAEhIAQ6B8EZOT2T1+oJkJACAgBISAEhIAQEAJCQAgIASHQJQIycrsEUNmFgBAQAkJACAgBISAEhIAQEAJCoH8QkJHbP32hmggBISAEhIAQEAJCQAgIASEgBIRAlwjIyO0SQGUXAkJACAgBISAEhIAQEAJCQAgIgf5BQEZu//SFaiIEhIAQEAJCQAgIASEgBISAEBACXSIgI7dLAJVdCAgBISAEhIAQEAJCQAgIASEgBPoHARm5/dMXqokQEAJCQAgIASEgBISAEBACQkAIdImAjNwuAVR2ISAEhIAQEAJCQAgIASEgBISAEOgfBGTk9k9fqCZCQAgIASEgBISAEBACQkAICAEh0CUCMnK7BFDZhYAQEAJCQAgIASEgBISAEBACQqB/EJCR2z99oZoIASEgBISAEBACQkAICAEhIASEQJcIyMjtEkBlFwJCQAgIASEgBISAEBACQkAICIH+QUBGbv/0hWoiBISAEBACQkAICAEhIASEgBAQAl0iICO3SwCVXQgIASEgBISAEBACQkAICAEhIAT6BwEZuf3TF6qJEBACQkAICAEhIASEgBAQAkJACHSJgIzcLgFUdiEgBISAEBACQkAICAEhIASEgBDoHwRk5PZPX6gmQkAICAEhIASEgBAQAkJACAgBIdAlAjJyuwRQ2YWAEBACQkAICAEhIASEgBAQAkKgfxCQkds/faGaCAEhIASEgBAQAkJACAgBISAEhECXCMjI7RJAZRcCQkAICAEhIASEgBAQAkJACAiB/kFARm7/9IVqIgSEgBAQAkJACAgBISAEhIAQEAJdIiAjt0sAlV0ICAEhIASEgBAQAkJACAgBISAE+gcBGbn90xeqiRAQAkJACAgBISAEhIAQEAJCQAh0iYCM3C4BVHYhIASEgBAQAkJACAgBISAEhIAQ6B8EZOT2T1+oJkJACAgBISAEhIAQEAJCQAgIASHQJQIycrsEUNmFgBAQAkJACAgBISAEhIAQEAJCoH8QkJHbP32hmggBISAEhIAQEAJCQAgIASEgBIRAlwjIyO0SQGUXAkJACAgBISAEhIAQEAJCQAgIgf5BYOJWVRkzZkyrR7ovBISAEBACQkAICAEhIASEgBAQAn2AgOy2wZ3Q0shdfvnlB6fWHSEgBISAEBACQkAICAEhIASEgBDoCwQwcMd3u63KyFe4cl+wpyohBISAEBACQkAICAEhIASEgBAQAr1AQEZuL1BUGUJACAgBISAEhIAQEAJCQAgIASHQFwjIyO2LblAlhIAQEAJCQAgIASEgBISAEBACQqAXCMjI7QWKKkMICAEhIASEgBAQAkJACAgBISAE+gIBGbl90Q2qhBAQAkJACAgBISAEhIAQEAJCQAj0AgEZub1AUWUIASEgBISAEBACQkAICAEhIASEQF8gICO3L7pBlRACQkAICAEhIASEgBAQAkJACAiBXiAgI7cXKKoMISAEhIAQEAJCQAgIASEgBISAEOgLBGTk9kU3qBJCQAgIASEgBISAEBACQkAICAEh0AsEZOT2AkWVIQSEgBAQAkJACAgBISAEhIAQEAJ9gcDEfVELVUIICAEhIASEQA8RePnll8N1113XuMQvfOELYfrpp2+cXgmFgBAQAkJACAiB/kVARm7/9o1qJgSEgBAQAkNE4Kqrrgrf/e53G+WecMIJw2233dYorRIJASEgBISAEBAC/Y+AwpX7v49UQyEgBISAEOgQgT/+8Y+Nc3z84x8P0047beP0SigEhIAQEAJCQAj0BoGiKMJ7770XjjjiiHDjjTcGfveCeraSS+UeeeSRMOuss4app556UN0efvjhMNNMM0VF4qGHHgpzzDFH+MAHPjAg3RtvvBH+9re/hXnmmSfe//vf/x74fOITnxiQzn8AAuV+6EMfCjPMMEO8Tdkzzjhj/Hi69Priiy+Gf/zjHwGl5vnnnw///Oc/w0c/+tE0SfzOe1955ZVYl3/961/hqaeeCh/72MfCJJNMMiAtdf7rX/8a5p133jDRRBMNePbmm2+Gp59+OpY/6aSTDng2kn6Aw2uvvRb7rFW9H3300TD77LOHKaaYolUS3R/LCDz22GNx/OXjaiy/No6TOv7opg4vvfRS+M9//hNmnnnmbooZsXmRP7/4xS+ifP36178ePvnJT47YtiArkREur9OGICvvvffesMYaa6S3B3xH3iOHJ574v1MX2CDPP/zhD5f3PMPOO+8cttpqK//Z9lo1Z7XNoIdjfdwL4rGHwNtvvx2eeOKJlrrV2HuzSm6KwLiez9GFL7nkkrDuuusG9NVnn3026rvo101pXM1V2BrI/CrHZN6OpnVXuuFDAPtxk002iRXAOb3EEkuEpZZaqicV6tlKLuFeO+64Yzj22GMHVezoo48OO+20UyDNn//856jEYADm9Ktf/Sp85zvfKW8fdNBBMS3GaBWdddZZ8fktt9wSH3vZhx9+eFXywDu/+tWvxnqS4NZbbw2rrbZanKzTDLxvzTXXLMPXfve734X11luv0rPAgPryl78cfv/736dFhHfffTdsu+224YQTThhkGA9IOAJ+HHfcceGYY45pWVMcAuBY1actM+lBVwj8+te/Dk8++WRZBkYmfcA+xKESk+q5557bNntVmjr+aFtgg4c/+tGPwsUXX9wg5ehLgiMP4X/DDTdEZxkG3kimb3zjG6HVCutPf/rTcPbZZ5fNy3n8L3/5S/jKV74SJphgguh023333cMWW2wRmF+WXnrpcP7555d5MZh5hgxu8iH9+EpVYzrHoipNL8Z93sf5e/W7PQLMuSeeeGL497//3TJhVZo//OEPYeutt26ZZyQ/uP766+NKUNqGqnvp83773ov5vNM2XXrppQHd+a233opZt9lmmwCfQFU8FB8kf8bVXMVCFboOCytVlLejKo3u9RcCzOk77LBDdOZj4PaSembkUqm11147/Pa3vx1gDMJwv/zlL8NJJ50UV3gxSFHUWG3NCeVnySWXLG+78coEmxNepoMPPjjedlDapSfhoYceGldWPf1KK60UJptssnDllVeWxTPA8f5j5G600UbxPnu1PvOZz0TvVpnw/S+sTn/uc5+L7U6f7b///nHi+fGPfxyVsvTZSPsOrmm/5PWn3+jTqtWZPK1+d48AjpXvf//7AyIH4FGiKGabbbYhv+CCCy4IPoZaFVKVpo4/WpXV9L579pqmH03pCNvBuPvZz34WjTXk1UglnGGsHrn8zduBrDz99NPj7Soehw+Qw0TMHHLIIWGuueYKGEk4QdZZZ50BK7k333xzlPUYr3UfVpdbRQvldRyNv6vGdN7OqjTdjvuqPs7fq9/tEbj99tvDGWecEfWYVimr0oxmmYqz7JlnnhkAR9W9AQn67Ecv5vNOm8QC0E033RSmmWaa6ER88MEHS1ldxUN5+eNqrqIuRKstuuiieRXi77QdlQl0s+8QwMhl5ZaF0F7TRP9nlBdKaO7cc8+d3679jYLNih9edRTuu+66K+y6667RwJ1vvvli/lNOOSWWvcIKKwwoj5XP733ve9G7SCgzq7I///nPY9qPfOQjYeGFFx6QnlVjQocZkO6RpGxAQqnxe56JgYGg++AHPxi+9rWvBepDyBttveOOO2KIBp6oXXbZJZb5wx/+sDROMY4xiBdbbDEvbtD15JNPDqxSUCaKGsY9kw/vo9yrr746eikuv/zyWG4aIo3CRrgcBjNEaObee+8d35eHnr7++uthn332icqehwafeuqpcXUEBRDCC5imQYljheS0006LuILvlFNOGdPeeeed4cwzzxwQGkDdwetTn/pULAss9t133zIMHQWSNrJyQrjImDFjAn20/PLLxzLTP17fBRdcMIARYZeslOcC6u677w6s5IPZc889FxZZZJHYl7/5zW+i4ZWmZwUTzOAzeIAJ7ZxzzonlUw7vcmxoH88YREceeWT0SFYptKyAsoqJM+a+++6L/IEhSZtQqs8777wYGj///POXzTvqqKPid8K0oXb14Dmhlnjd6QtCcAlx9/B33g+/g+vjjz8eQ7+rQiivueaauLKHZ/XVV1+N/AYvoezjOIL/MYg4dIdwnjTEFwODyAIwpg8Z45wmS6jIt7/97dg3hPuD/xe/+MWS/6l7qzT0L/yx3XbbxXZRf8YUvAjmTq3695577gngyPuc4H9kAbjidGJc8xulGL7GoULECG1YZpll4vgCF3gLZRwMfMuDl5le22HdpJ9xujFmwJE+eOeddyKPuSOI8H5woN5XXHFFHGtzzjlnrIKPh7wNaf38O151Dk6ir3Hqgcfkk08+ZAxa9YG/L73SB6yew0tgCmapDMbByNhHnjGu6Ueep0Z4ihMyCSMX73tVCDG4IGOWXXbZGBXD6nXO4+DJO5CJP/nJT+IKro8fyv30pz9dOtpoK8/Aue6Dk3LVVVeNzWceIkKId2EI0MZ0DKUY8f3aa6+NMuOyyy6L0TuMaSf49cILL4xl/elPf4pjjTHZjlLZytxGHZB3zFetZBnvueiii2I/UB/4JA0xHK5x346HWsmxFJs62YC8a4exjzVkhPMJ8u34448PnKRdRczVLkvAFEplSbux7f0z1VRTRTnPuGE8tNOl0vdVya5272Mu2WuvvQJbpnCEVe0rb5WG8cNqGHzJ+EUHYhtEqm+0m8+8rbQPnQ++g0cJV0VmMMe1a7vnZ/5BLyNMljkmHT/IGPQn9AzmRrYxINtbyTHmHeYxDDW2MIAdjvf8HnocOh1y2+Uy/Uy/o/tUOeGYFxiHzOHwD9sqmOMeeOCB2Nfod+i8vnjTZB5Jxy2ROuh/Lh+azOfUOaU6HaZOz3SZTj2oD7xBBGYrHkrfPS7nKrCBT3FsVlHajvS5ywP6zfXDOt25lez0ctvpdJ7Gr+30DtJgL6EDM//Au4znfAukl1V1hf/byZqqPP14DzkIET2b6pBN6lqFQU/NZozcz372s1FgoZSh+OJlT5WjVt7f+++/P658oqhApMPQWWihhUK+kovRgpGAsEoFEnk23XTTyCA8d0LwYjRiwHI/zcNAQZEgBOKwww6LBhhXB5eBASO3iw9fZZVV4qsQ9IQtE8bFxIGChCAlrA4hjTEy3XTTxXBpvF4Q5WNYpaugMDuGY9W/s0DIY/gxYCCcAQxq0jthyKAgkh9jlRVp9jqjzCGM2avG5AqhpL7wwgueNV4ZZOzZgXKPIsbFfvvtF4U5kyLh5fybjhTTmPH9P4SEM3lQRwQ47ScMnXtOGAx77LFHFFyLL754xI6JD8Lgoo4psUqOEoegYrLceOONY7+iIFPf1MFBXvDCAIIP4amcUBDWX3/9OHGhEMFvOCyYyFw5wqBxzMiPAofwd0O0rh60l/2UYIBCTX1oN4QywT0mvS996UvxvRjYVcQk6vu7P//5z0dBSDp4n30q8DJjBi8s48+J99NGCH7FkN5tt93ib/raxyi8gUPH+T8msD+t0oA3ygw8DL8x0WO0Msk7tetfxgcKEXg6URZGC0oTRh1KE8IeXsJ4h38wPHCUYZTsueee0YGDgoTSQtiLjy8v0691WNf1M2Fvm222WewrZAJtwwBHqYJwsK2++upRZlA/sIEffbxWtcHrll/dGYPxDG9jvFTlb4JBuz7I34u83MJCfVEmkMc4u8Ac+QZhsCLfUGzgCyZj+tCdPqQhzC3FCX5HdrgjgDQp0S54CeWlFY8TqoycxC+LIwtFDH6A7xlb8IsThsbKK6/c6IP8gMBxgw02iOV632255ZZRcfdy0ytKI44z8AEn5hhkBoRM3XDDDSMfMicix8AjnZfSsviOsZDLVuYSH4tVsox5C/nOuAdbnEKsZDCvQMM17ut4qFUfx0q//6dONtRhnPKUlwseyIAq4n3IEuQNjhHmGJR8+AyqG9v0D5Fsp512WuRFxjyyqFWf143buvdRR8jlgzvJ4833/1SlYfwyP2DQMecssMACUWdDV3Oqm89oK4YpCil6APMPWOGUYx5g3mTsMM9UEflxDB144IHRsGVeRD/DQIVcxjDG6BeiBMVRJGwAADoTSURBVJkD2skxFhjcSEcPRCepukf5jFN0IyfGC3VPnen+jCvzAo4v2oOjm9/Mr+haOEGQQZtvvnmZpW4eYdy6fEDn4TdyCOMGqpvPyxe9/6VOh6nTMx1v10ORJ67TVfFQ/v5xOVcxT7TSx/N2pPXsVHduJzspl+etdLr0vXxvonewIMi4YT5hLuAzPlOqb3eFgxU0iEyZGXSv6Q1TdgrzlBTmJSxMWR2Qzazswgy9wgb0gPv8sImlMCW8vG8dXpgSVZjXrLDBX943Y60w47Ywo7gwz15hk0p8RtmmbBUmtAsTuoUJiTKPKWjF9ttvX5hCVJgCX97ni3kPC1sRLGx1t1huueUKm5AGPDcDrjAFprAJacD9/IcZDIUJ4sIUmsK8ouVjmziKb37zm4UJ6vKeTaSFCfT42wZKYcZK+YwvtsesMMV4wD3/QX1NqMb2c8+MksL2TsR38xt8zGApwIO2mNAegIUJw9gH5pUkeWH7iQvzlMbv/DGlNeJnHsp4z1bpCuoLWVh3YSubha3IxN/8sYEYy0vvlQ/tC/nhBdszVN6m/2xyjL/B14yuwhSC8rmtghVrrbVW/G0rIbFf/CF1oJ9MmYvtg9fgBSdzSMT6wAcQ7TOl2B9XXqmfefTKZ5QNhma8xHsmOCPmtiJepjEFKvIF/QHOdfWAV815UOa3ybIwwRd/H3DAAbEf/aEZTAPa5Pf9agdDFIwzJ1tNjPUzY95vxXFhxm7521YwBpQJrrZ6Wj4350xhE0f5u+pLVRr61wz0wpTaMgtjzIyg+Luuf035j/1lTrGYnqs5IgpTGsry/Avvoj/hUSdznhTIinR8mvJRMO6qqB3Wdf3M2AJTUwDLos3bHetvCmO8Z4p3YQZe+Zw8yDyLXoj3qtpQJs6+wMNmuBXmyCqfVOWvw6CuD8rC3/8C/yMrU5nFuLeV3ZgCmWVKTWFKeJkV+WpGXvxNWxk/YONkCuUAnPy+X2mXyxnu5TzOPWQIcgM84S9z7MVPOi5J5/MM6Zp8zPFHtthmxkDKS8wj5oCJz9M/yCtzqhTm6S9v2wpTAQ9BzAVmoJbPKBO5lcqA8qF9MaW/MMN6wPxDfurvsrVKlpmTtjDvf1kUfcYc5HP4cI37Oh6iwlV9XDbEvtTJhjqMc56ibOb5VNan70NmmGE2oL/RQeB3qG5s0z9mKJVFIsPoP5dt5YP3v9SN27r32QruIPmQv6MqDe1BV0rHr0VRFBZhEbM3mc9oqzmby9fZiahRL/N53OWeOSvLNOkX8lu0xgBZbtF5JX7U0Yzvco4kbxM5ZkZxgS6WUtU9W90t5RVpaQu6VBX5/Iru4WQG6gC91Fb9ow7A87p5hDTwLrqtE/Mn9UbW+/vazeeez691Ogx4ttMz8+f0j+uFVTzk7/XruJqrqAty1/VTf79f83b4fa6d6M6kr5Od7XQ68qfUTu9AVtM3jBknsLdFKf/Z6Ooyv1HiPk1EH5nTNn743ilVYfDfIyq7MpMHZrbBFr3RrEyZcTfgId4pvEIe0pE+5Fnq5ec3nnsbPNEz6mkJVySUjRUjVljd20R6Vunw5PEOwgx4xqooIWN4DfGKe3ovDy+5DegYIoBXMl89xWtEvfACtSP28LL6x2qGr0bjPWM1hLCnNOwAb5l77FKPmZdPW1ZccUX/OeBKfWkjB03gjSVcxgZQ+MEPfhDT8X5Ox2NFi6O4CbdN2+zYs0JG/dJ9FxQAXjxzzxx1YVUHYjUa76V7+7hHOBGezPQe9508v68+GtPGsCRWpSD24RGWRx854UVmxRciOgAvJ8TqAO3kQx3xosIfeOz5QKyo8S4wsskmto9+b0WszJlCFldZPA1l01+OG2XDb+l+V/qNFRr6A69tu3pQLnzFyi/vom9N+fHXxWesfBJCxuoTq6re/jLR+194D+HU9K0TPMrKJjzohPc7rS+rnIR5sUqP15cV/bQO9FM6/ryc9FqVxvuXFUun9N11/ctqCd52VmVmmWWWuGKIR5v65sS78Jb76jr8y6oL4a/0gRNeU7yhVUQ/tMK6ST+zCp5GA/jKgfMK7YHviB4AZ/iE1X4fd3kbSENYTkrwAeGUrDSQj+0FTnn+JhjU9YGX7Vf4nzbCI2YwxsgGVpWcX2gTq9VsUXCiz31Ms2oLj6erIjlOns+vzkf8ruJx7jP+/YAdVlNYrUV2pHxOOspqSsh1wtcgxhyyhhBQ2kcfeL/m5SH34HkiWVj9JdyeCAqIkGMzFAZElPAeygfbKiJ6wRwFA+afVLZWyTLayZhm/jrNVg+diIogiggarnFfx0Ot+tjbwLWdbGiCccpTlAcuzC3oEDn5XM24RR45sbri1G5se/9wCJoT74NHfOz7fa5Nxm2791EGIca5fOB+SlVpqsavOWHKcVQ3n3lb03mVLVFExTnfMY4Y81Xhk7QdvYPtQS7LqTPjGZ6HqCO6JPOuUxM5Rj7GY0pV9ygXXoAIYWdrBqvLVcT8ytzE3OyE3Dbnuf+M8xfRJVDdPOK8+61vfavMD58QQQhx0GndfF5mtC9NdBgwyGUZ48P1zPS596+nr+Kh9P18H1dzFSHUyHzXT/N6pO3In3WiO5O3Tna20+nyd7fTO9ALkf1pJAb6dKpT5eXpd3ME2ltuzcspU6KEoPDkBi4JECoo8DkRWosgIVQUIgQXYwUlCQMKgcnAYwIyr2hkCNIjQH0CSZkbI5eQU0LpCCEjBAymIU2VIs+EizHoSlxaPwYvez/ryDxMkUnZN+KE8EEBw+BMCaHqSjyCJq0TdQYnFzBpPv/OflrSsa+ESZiJgd+2OhsP0SJsCEJoeyi15+XdDDjwqBPGPhlRF/bTIWBcKHp5GFxp/f0+V8+f4kdYD8oqfECZ1Acl/ilTpv2DMmiezlgUkxGGIfgyqaJ8E6YL4TxAEfB8XOETTgWE4BHyu/Idb2Z/zPMTy0udGPAf5XgfpLzl2dN+q6sHeTDGmHwJ8QLD9BRyBByGOw4LQpgYOzgDqqhqwqF+hKKnbaBvvV8IzYK/CUcjDJgJG0eAt4/3VLUxf3+exvvXxy3pMVYxiAjBatK/OBMQ7oxxQtWoKyF+Ofm7vE08Z3xB8EfKA/BIq3Cmdljn7aPstJ9RPpZbbjlul4SR4U4ewhPZ7094MryHoo98whlC31S1gXGIUpl+vI3pu3lhVf46DJr0QdkY+wLfESpP2Dz7iZErGLMooL4fnXqlY5o8yEn6HOIgP0LeU2I7iuOU3ue7t8v5sYrHSUfIL+9BecaJCfbwW07wOqGYTT7ISFfMMXZRvDFO3NBFea8ilHNbNYihmjgWCW/2E/ZxqDLG0/HIvAiPpiHVXi58g8yv4i3nhSpZRvtQ3JBXKf9bFEx09A7XuG/CQ6362DHh2k421GGc8xTl5Q5c7jkxjjAsqowy0tSNbe+fdK6HV9FhUkMufR/fW8muuveRN5cP3MupKg33UplNHuYLH39185m3NZ1XKdPzUx6/GU+pc5/7EO9ijk+dhdxn/nPjhfzMaU5N5Jg7TnzMkLfqHvfRCygTwlhFj0KXqCLmhbRMDFzOFUGuO6VzR/rdn9MeLwPeRV5UYUN68rebz71MvzbRYdL3ky/XM9Pn3r/uPEyf+Tvza57Gx5+3mfS9mKsYU2CD3KuivB55mqa6cxPZ2U6ny9/bSu+AB8G7Sk+vmivycvW7HoH/uSzr0zZKwQBFaZ1rrrkGpWeir/JOWPhQnPxdSMKoGEEoiShYKOTcQ0lnXwSTRi5IeO57DlGmYByUDyYtFDZWTtOV37RylEVsfRXhXUMRraO8PqRHwLpn0vNj5KGcEW+PMoCR58KENByyQLtd2Hu+9MpAZZ8QjgC8nRguCC32yKFgu9JW9X720rE3DAzxpqbvRrljvzMr0lAq7FAqESxpe1AwEdgY21XUyohm4qdPMVxRBFm5cgITPq4gwktMBgg3VsXZ8+QEP7F3N51s8Ej75FHVJ57XrzhY0jZxH2Mah4f3O5Nv+g4cKJTtKwJ19aBO4I3xzYcVcfYwIvggcGc1hg/CFQMfo88PoYiJ3v8Dn/s48fvUJd0PxH3usb8MwtGD4Uy/QwhW9kj7BAQGOCvgn1ZUlSblD8/HexHOKDEo3nX9Sz4UDpxX7LdC4fBDIbxMrlW8xETKhEcep7T//V56bYd1XT+jkDpfUia/kUeOI6s4OJBYyfR0rK5jdEBVbcBgazXWwTJdlajKX4dB0z6IFbQ/7HHFWENx8nFhYZ+DDPWUB5Fp8IcrrTiI/GA7yqVPMOIdJ3+XX/N2VfE4adn7SxQPRjTOIB/n/MZp6E5K/mUQsrEJgS/zBQRvMO8gU2gPxjLGZ6rMe5mkRTaxH5sDsxjXYMfqL3lTuUoejGecBOxfzIn0kPMM3+k3jFgfk/BCijlpkJ/sY0ydQin/D9e4pz/a8RB1b9XHPEuplWyowxjZBGG4OvFfHpDjVQoy4yjlWfIgk5Dt6BB1Y7uqf2hjK56vG7dEZ7WTJdSPd7L/uh3laXhvHr3FPA6/uZOqbj7L21pn9OX1Iz+rcSkxv4M3ulpVHZvoCjhOmLPTyJeqe7wXvqLdGPSMZf9PGmmd/Dv9uMX70Wzc4zdj2dvAmIPf/AyMunmEtuTyAd2UuuMUBZ9287nXy6+MBZfVfi/VYer0zBxv5HEqa3Ie8nekV9KMi7kKHSaNtEjrkLcjfebfm+rOdbKTPm+n0/n7/NpK72DBB3nkkU6kR6/gAKpUL/ZydO0cgZ6v5MLs6QBJq8RkwUoWHnAGJiuPhKBwEinKglNaBooMBjOedQS6h4SQxicQX/n1MGFWchFuGGwcbAAhRHm/r/z6u1D4MVq8LL/PlZVLvDUYnXWU1tnTYqDjPUYAWnx5LI8QFYwjhBIMzmBBmWGVAkUI47fVROzlsqpCeBuYQNSP/BhIqXDk/RiGDH48wxg2rLJ5Goxg2g5WKKoc/kSop/df2iZCLnFQoMBTZ/Kh2DEhenqvn19zYcn9dOLHGGdw0/+USX1QGDk524n+R5nkUAiUudTwQ/nEMIKXwJdQXFZQ+A2lPOLl5Vf4CccCoeocVoRwQ0FL+YHyWGWEX7liPIKHGyft6sEEQ5gTQgvCA0wYqK+I0y7KI/qByY9oBcIeW3mVUUZQhr2N9C2TatoHudKCgwKegehf8MXJ4CtzKCm83z8xYfanKk3KH5680/4lHwoHoecYKvlqlpdbxUvwN7zLOIVoGxMtBlUV1WFd18/wCsouBiCKPDgynhx7x5k+py9RVFnBdF6qakNVPblHXfIoiar8dRg0GWNpHWgD8gQeY2JGJmEceBtbOTbccUVZjFGcOLQB+crppq0cjKTP25XzOGmcGBs4QSjPCWemKwnMBchc+qXJxw1clF3GKeOJcYDBimzzvvN3cWV1l3rgoIQYfxjDPqaRBzjj3EhiTKN0ulMsZkr+YMwwBpB9jB/mLQ6AA3/HvUqW0ffUm/EOMcdxQBn1gZwf+T4ux30dD1Gfdn3Mc6dWsqEOY3iPOQHsaTvOasZtVX/yLlZgOc0ZnCG+4zRnXoUcy1ZjO+8f0uV8HQt6/0/duK17H8WAIe/Bwd2K8jStxq87Jimn3XzG87ytrYw+513ypER6+oT5lj6iPGQp/M+8VFXHJnKMtiIbOPEeXKCqe9yHr3CEoX8SRZU6mHjuVGU4Ud+0bTgN0F98u0LdPIIRTni07cGM9YRHOSgLY6fJfO5182udDlOnZ+Z4p3M47wDDdnw2ruYq5BxOP4zAKsrbUZWmqe7s448yctkJFu10uvy97fQO9EgM79NsuwnzDnMWC1VEornTKS9vtP72Mevt4zfyoWuyggaRTQaD7jW9YZ3f8nAN68S4ud4GZTyQwbzm8YAlm0gGFG+rM4UJwPKeCcCCck3piPfMYIsH7dhqV/xthlxhBnCZnsMOzKtfmPAo75mxW5gQLX/7Fw6u4n1VxIE25lkccBBJVTrqwfvMCBr0mMMvaC+HKFCWhTwOSMOGdJtgChOa8RCndgdjeEYzUgs+Thz0wvv9YBi/b0pYPBSIQyY47MmM2Hgwlz/n8CPazvvNOChMoS0PUyKNrUKVhw/wmwOhbLUmHkxFW8wBEPuFZ1WU5zeGjfWwSaJMbkpaPPAIfPjYHrfCVt3L53yhbhxEZRPJgPum6Ba2dy4eogHGYGfGekxjq0mDDosakPn9H5RpgjO2lYMw6B8OTjLHQ5kcXG2PZ+xHeIKDLNKDwdrVg0LMSIiHUtgkGA8uMg9dyctmpMUDoJxHODSJ/mxF9BH9aau+MQn1zQ+UoL4cBuVkK52FOX5i35HPTvsuzNnijyNPcB8e4uCGKjIHRHxnmibvX/JxmBWH6Dg16V+LBIi4pAcveH6/Vr2LZxwqhxwBW3jcjOV4wITnS691WNf1M+PcJqx4kJqtwscD3cDVDLn4GnPgxb6BVxhXHCzDeHG+bdWGtI7+3Rwtgw4Ca5W/DoMmfeDvRXba5BoPlqLuZhzEcWkGd0zCu9IDorhpK6eFGZpeRGHKRuRJcwJFucZhUebAKHEqE77/JW9XzuN5eg5vMmMlHgzHeEkPLDTDIh7EZw64RleXNcgL2xsd281YRN5YxE3+6vK3ORgjLvAdh1Wlc40pfYWtOES5Cm8il6rmhrIw+2JGckFbkGeMVw6TYs6DWsky3sM8gDygHoy9dPwO17iv4yHaVNfHpIFayYY6jNEV7EyPKPfA0ULvBxyo+N/SB/5FR3A5b8628vAuUrUb21X9w9xrTsRSzg98039/tRu37d7nZXF4EvIHHaMV5Wmqxi8yDV5xajefVbWV+cQMRc9emLM9zpXoezl5fmQS44xDQukfP/yN9FV15H6dHDNjM84BzFHmZCRLUXWP+/AH2PGudlQ1v3JQaVpfi8wqbKW3LKZuHmEuNaM2ygf4Df3GFg1i/qr35fN5+aL3vzTRYdrpmSne3j/pYX45D+XvH1dzFXo+c5IZgnkV4u+0HZUJ7Cbyko9TK925Tna20+m8bL/W6R0cAoq+wFgwJ2VhW9k8a0fXbuy2jl40FhLDwxwwzKFTjF8+fOeAN9efmry2CoMJyJhbymPs39EQqjW2yCodvVh406pCh8bWezstFy8C9eu2jnjk8XZ5eEteD1YMqsK483RD/Y3HEk9oK28lnr4mq9W8n74j9NtDeYdapzwfK5yssnsIYv683W/ahwe3Ksy1XT6e4QXHe+yEl5YVavaP+Oonz2g3Q6VVqAxp2tWDvHgiWeWq4ifCkwnxbVc+74DgF8pr2mfkwbtNvnQlnPsp2QRX/kuk9H76vUmaNL1/b9W/jDG82OxJ8XBqz9P0St+wYsiKfxNqh3W7fs55hRVE+IVoAicwpnxkW6vx5ml7eW2CQas+yOtBn+C9Bs90bOTp6n7D70QkVPF7Xd46HqcvWAlBZvgqLmUSaUE7mxBjId1bTB7GCWFoaZmtyqIOrNizEltFrMqAX51sx4MPrzi/MLYJy2a1yM9XqCrf7zG3UBdkfE7DNe6b8FBdHzeRDXUYdzK3gR1zNX2KLM5pbIztduO2yfuIfmIeaDdvNkmTt5Xf7eazqvRN7pkRFyNciJaC75lPWo2fVuW1k2PwDOMhDT2vusfhT0TCEZHTZKy3qkur++3mEc8D79Jv3b4/n5da6TDwU50s8rrl16HyUF4Ov9vxvKdv1cfkbcfrnr8X1zrZWafT5XVop3eQlsMKu9Grx7bdlren17/p21xXAONO+rsKg2ExcnsNjsoTAkNBgAmOfd6cVEwoO+F+hLejYKYnHw6lbOVpjwDCC0XDvPPRKGFvqyv57XMOz1P2pRKeyhXDBWFKmBuhvITqivoDAcK8mhq5nMNAiORwEgo4p9KaJz/+X1EMC0K92cLCtp5UWR/Oeo7Ld4802TAusRnp77LVtugkJ0x4OIjtDGwhQXYz76dhx8NRn27fKR2mWwRHT/4qA2/0tK5ZS6ow6PnBU82qolRCYPgRYBWHEyAxaPFyWvhS9O6mJ2QPfy1HZw04fA2DEaOE03z72cClBziYhFPa4Q284RZqF/erVh0kNDp7rP9bhQLb1MClNa32Z47LlsL37L9F6ecARlaXMXgxcsdHAxfsR5psGJf8MtLfxX724ZxfOVSOCBMOyxzpBi68IB1mpI8I1X9sI6CV3LGNsMoXAkJACAiBsY4AIXUckNaUbN9PV+HYTd+jdEJACAgBISAExiYCVauYY/N9/Vh2FQZaye3HnlKdhIAQEAJCoCMEOLHeTzzvKKMSCwEhIASEgBAQAqMOgZ7/C6FRh5AaJASEgBAQAkJACAgBISAEhIAQEAIjBgEZuSOmq1RRISAEhIAQEAJCQAgIASEgBISAEKhDQEZuHUJ6LgSEgBAQAkJACAgBISAEhIAQEAIjBgEZuSOmq1RRISAEhIAQEAJCQAgIASEgBISAEKhDoKcHTz399NPh3nvvHdYj4usaPLaf869onnrqqfDxj3+88aueffbZMMkkk4QPfehDjfN0mjDtm7feeivwTv5lTj/To48+GmafffYwxRRTjNNqDtd7e9XIf/3rX+EXv/hFeOSRRwL/C3S0/Zubt99+OzzxxBMDDhn605/+FC655JIw7bTTxn/J0grLn//85/FfRyy00EKtkgz5/lDGfv6yXsqCKpzy943t36+88kp47bXXwhxzzDFWXnXnnXeGO+64I/6P2bHygi4Lfeyxx8Kss84aPvCBD3RZ0tjNPlz1vPjii+P/y15mmWVKvSG9t9xyy8Vxve666471k7DfeOONwL+4aUWzzTZbmGqqqcJwYdWqXmP7/uuvvx7/pdX6668fJp64pypjZdXffPPNgL6SE++eZ5554r8AQqbwPafHH388/uutmWaaacCjJ598Mvztb38L8803X5hhhhkGPONEdk5mz2nmmWeO88nf//73gM70kY98JE8Sf9c9ryvf21t1aN6rr74a+N/Zc889d2jKn/Aw/5oMuSMSAuM9AvaP1wfRtddeO+hekxu77bZbsemmmzZJOmrT/Pa3vy1WXnnllu37/e9/X9xwww0Dnn/5y18uzj///AH3OvlhAro44YQTCjNuWmZL++bcc88tzPhpmbYfHrz44ouFGeGFTSDjtDrD9d5eNfK9994r1ltvvWKzzTYrjjvuuLY80at3jutyrrnmmuLzn/98+Vrk1WKLLVbY/xotrr/++vJ+/uXqq68udthhh/x2z37Xjf0mL+pWFqTvyHFKn42r7/a/kIs999yz8evOOuuswhwYjdObMjyAF1pl7LTcqnLM+VWcc845VY8q75mCWpgiXpjSWfm805tN5HyTMnMsel3PJnUgzSGHHFKstNJKxdFHH12Y4Riz5ffOPvvs4rOf/WxhzpKmxQ45HbjwLj7mBIt957+5Pvjgg8VwYTXkRvUg4y9/+cvC/qdsYYZl16U14eFf//rXhS0SlH3hfWD/zz6+HzmObsDYTwlesf97X9j/yy5vw1v2f6eLT3/60/FDueboLJ/zheeLLrrooPfdeuutMd2uu+5a7LvvvgPypD/qnteVT3vt35lVzl2Mh2222Sa+rgl/ktDHVFpHfR/9CAzVbhtNyFRhEKoaWJWwKl1+75133ilQssdnQhh+73vfawmBeaQLhLFTLyZNDOelllrKi6y8pn2z++67FyeeeGJlun652QuDYShtGa73DqWuVXn+8Ic/FIsvvvioNG69vbnhZCsMxSmnnOKPK6/PPfdc8YUvfKF4/vnnK5/34mbd2K97Ry9kQfqOHKf02bj63onRbisVha20DFBSm9STPLZK0zLpUMvNC0ThxFnYlHrtZGgi5+vqVoVFr+tZVweeY+zYylVx//33l8mr7vHwP//5T5lmXH356U9/WmyyySaDXjccWA2qxDDc6FUfNOHhOqMRXQbDkT5ywrmJY8IiO+It6oueg74Fj6GX8jn11FOj8+Lhhx+O6SzqLhqYOLdbEe9CL2hF7Z43KZ/2YuRuscUWg15B/U877bRB91vx50svvRTLsuiWQXl0Y3QjMFS7bTShUoXBRP9nlC9nE25LeEQnREiLKXlh2WWXjaG3Bx98cJh66qnDhRdeGGyQxpC1BRdcMFx++eXhZz/7WXjggQdiuCz/2xA677zzwpMWUkJox7HHHhtDmKabbrpAyAhE+fvss0/M488JcbIOCjbxxPDMCy64IIZppGEs7777bjAPWDAlOPzxj3+M4SdeJuUaKOGkk04Kl112WSDtvPPOy+1INumGiy66KJx88skxHaGzaUgx6Qmt4vl1110XQ5Rt9SwQ1pOWQ2Fg+u1vfzvcdNNNMfyEMD5bfQq2qhsItTTvW8QFHCiXsBon0p5++umxDVdccUUMx5lzzjnjYzNWw1577RVDWcCOMGlCNlPK++bAAw8MW221Veyf73znO2GBBRaI3z0PZZoxEOtQh6/n8SvpzdMa+4O+nmCCCYIJcH9c9vM999wTjjrqqDJ86Oabb45ttBXtWH/+qTPhQcsvv3zMW1cP+A1eIj8hqWussUbkhbvvvjv86le/CmeccUYwQycsssgi8b5XqO69ns6vL7/8ciyf/iA0Cp6Az53uuuuu+D54jn6jPyaaaKL4mNBKWwkKk002WTjmmGMiT9HPhBczJsz5EZ+lY6+OB/29XA8//PDw3e9+NxD+RNir8ytlMA4ZA/Da9NNPHz784Q/HrK3GVVou3zsZz7/5zW/CLbfcEsw7XhbD2DYjISy99NJRPpiSEeAz2mwTcxwvhOw7teu3n/zkJ+FLX/pSMKUlmEMp3HjjjVE+PPPMM8EUDi9iwJWxx7u/+MUvxvvUhX4gBBEyJSmY1z9YFEb8zZ88TfnAvjQZ+4RUW4RF5D34jH4Fe8jlXToObKUo9s9OO+0U0xCSB+6Envl4jw+SP46TrbbEPk/5G5xWW221WCYyirBeQtfT0FlC2+gDZLStgMSw4imnnLJ8Q8p/yCrCAJ13SNQuP/U3QzvOC4wRUzyjvITXkdWMpYUXXji+CxluRkXkXTP2Y1ikyw1vY6sxTHmEtXpflpW3L+3KbTdW0zLgM/iHLQAPPfRQlCPwEbKtXR/bCk2YccYZo2ynPLZBHHrooVHmTTPNNLVzV1qHVnK+Tt6mZbTCwusJn4HlVVddFfs4nSdTPmDOdNmSlp9+b1cv5lS2UVAmYaCE+YNrfg/ZCf8zn3n4aVqPKn5EBiBj4Xf4Bp1jKNtdmJuQJeYwTJsVeoFVN7JvQGXsR7sxRdqhzovpe5CD3gfILeo///zzl0nACoInoFbta8XDMVPyZ//99w8bb7zxAB0oeRzlITICPjLDMPLOlltuGceWOftjUsYZ8zNyjbBdxiof5CP8gY5IG6688sqom2299dbpK8rvyER0OtTkVG56grrndeVTDu3dYIMN4hy9+uqrl+HUzOPoanvssUeUI/5Orq34kzGB7vT9739/gJ6T5tX30YkANkaqO47OVrZvVRUGE7bP0vyphXaE2267LQoClBsmGRR5Jhj2QGCgorwxCD/1qU9FJY8B7IQCwwTLvjomFvY5MOmhCEGUz+R70EEHRcN3hRVWiIqmhcJFQYZRiSJo4YhR6SUPiijCg3eSHqUWYYhRASG8jzzyyFgfC2cJe++9d9xTzDPz7IWNNtooGsZLLrlknIy/+tWvRmWa50zM2267bbDQ3yg4MbAwHFF6LLSHJAOI/SQuJC2ku0yD4Y0RBF4IXvbR2WpBNPgpgL0pCD4mDm8DAtlx8X211BEHgysD6cvTvgGT/fbbr5z8L7300ijkPT1GEP1AeaRth6/n8at5WIN5T6MRiIKKk2LHHXcs+4N09DN8wd5tCzmNSjVOC+qEQogSjuGN08BxrKuH8xv9SR3WXnvtiOmZZ54ZJwhwh6fA2DygXt3oTGn33jLh+19QoFZdddWonGFk0QYmEyfaZl7ZaMTB47STjxNGP/yNgk87MQQZEximGD8YefAnkzNUx4Nerl99T4/zAkrHCy+8EDbccMPYBxb2FRV0C2WOhiX5qsaVl+dXx7fpeMaZQFtTYiLHQYQ8QNlmbGMsgSfGA33l1K7fqAvGILwx+eSTDxhTGLFVhCyxFe5Au50wjDE6IPiL+mGc8x3C6MWBRT/m1GTs00acXdAqq6wS+5Rx7VQ1DpAFzvPs+/vKV74SHT2MkyrCsN1ll12iXGOPMc4d52/HifGNYYAjC9770Y9+VBZ1++23RxnHXjUUepxTO++8czS+SAT/Oe8wfviN8onzBqrLz3yAgonxicxFIcVIQNaCK7Icgwli7E866aTxO+3FwIHa8QLPMaYY86nhzX2nVuXWjVXPzxWZ7MY4+OCQRGGu62PGN2MRAlvG9pprrhllXp1Mi5mSP1Vyvom8TYpoiTH1ZP8+/AcfMb622267MmuncqiuXrPMMsuAvsYQrbrncsf3UNbxI05E+BOnGXMh/NfKeCkbV/GF/qYsH4tpkm6x6kb2pfXge92Ycvw6nRfT9+AEZRHA+4Bxw5hzwsjGeHVHb7v2VfGwl+NXjEYc7FXYexquyEac+si3b37zm1FnWnHFFWMS9q7i2GbRpcrBgT64zjrrxLSpzI03sj/0N/VGjlRR3fO68r298Cly0Vaay9cgX9kD7rLQH7TjT3ge7Nyx7nl0FQLjLQImsAZR1ZLvoETZjTQ0zgRdYUpVGUJmk14MEUnDS0yxL/fHmTCOzw877LCyVPIQBmKKZrxH+YS+2QAv05iXsiDUwxSG8p5NzoUpcvG3GcUxjDd9bkIphj+ZkViYYTxg/5d5fst9JyYECxOUZbnUh70hjo0pxXGvBPed2I/cbj+uTTaDwt1okxl1XkRhE3kMN/EwLjNSCkJQnQjLI7TFwy6rws88rV/TvvF7frWV0sKMrviTtlAfb3cdvl6GX8HdJpwixYT9eKaIxyQejmkGkGcpzNNZUId0760ZU7GNfq+uHs5vZoSW5ZqRXJhxUdDPTrb6Wqy11lrxZ5P3ej6/HnDAAYUpuf6zsAMhynA7+IJ9omnYJPus4WEnsIVvnI444oi4l9Tr6H3L3j+ojge9HL+aMTMo3NMM/rg319MwFmzVq2B/D1Q1rjytXx1fbxv9y17DVuPZjMX4Ds8P1rzTVm3iLVtlL9/PDTPqC++7un6jLuDsVDWm/JlfTQkrZY3fM2W4DHEGC/Y9Ma7Mex7D2syJ0XKffJOxbytTJW/wTltJL2z1L76+ahzwAP6AZ2gjodVmiMf0VX/MWx/TeJ+QxozYuDef75RhytGAfWuEU5sjjscFYW3sQ0MeOpnSGDGwVfd4C945/vjj/XEMgSdc1xSzRvlTuUPfI49T2cC4d/nOS/KtHHW8QB72u9Jvzlvcyykvt8lYzcuoCrNs0sdgyjihb/nuVCfTPJ1fq+R8nbz1vOk1x8J5kTHrRGgmoZ9OncqhJvXK68G78nv5WK/jR/Zj+rxJeeYYjryBXOyEGBPoL6muQf5eYNWN7MvbUDemwI92uGwlf5Mxlb4n7QPX09I95uYkjXtdCQWG2rWviofTd/HdnGAxjB2dIf2kcsrzEE5uBmhhzjK/Fa/sua0KNR+Q6P0fzM/oBOm7bNW2TFoXOl33vK582sseWsiiCQtbXS51IeY2349bVsi+tOJP0jBWT7WQbNH4h4DbJuNfy//X4ioMJu6VdW8DL3rqKQ/vFWFyvqpI6CS0+eabxyt/WGEiXBdiNQlPYOo9xhPFCpyvflI++T2kEQ8jK46snuJddGK1zVdfWEnE+0s4L6uhpjiWHkJWXlnZZdWQ1V7Cz/BiQ7yLUCdTTOIqsZeN15KwHZvs4kq0KQMDPGbU1T33nie9ggvvcsLTitecMEknPPysEvjJeKx+kY5VJjyXlAFW7lkkBJHv7U4vTfvG3+NXVlnACCLEltUUVm+a4OtlcCU9qzSEEKVeRDzAvvLjKzus3jmx+kS/u6eY+/QNq9rca1IP5zfHjDLsEKJASCBhuk545+EJqO69nie9stpKuBZhoIS1srrj5eHN3n777UueJx/1h8cg72tCn5wI9bRJKfIU9+gHeIiQkzoe9DLSK57flBcIkWVVI13NICSROrMKCuXjKi3Pvzu+Tccz/eA8xWrDD37wg/jxd4IjeJmSFvC+s+rvVNdv1CX18vM7HVNeTnqlz4goSImQXFZkqZ8ZcoFV0d/97ncx/M8UuzgGiQjIqenYJ7IEGcKYYtyykuftrBoH8Icp6LE/CLk3ZbTlaZ7UiTBoIkm8T7hHdIETuCDz0hNBkS0e0ku4PVsBUixdptBPzjt22IsXGXnZHJHxtzlo2uYnUSp3KJNQQXB4ysKqWKkmSsUxYbX5vvvuC5TrVMcLpCPqgXHmvOV5/VpVbt1Y9bzplbbksr2uj1lFYjUJPmCbiq8qNZFp6bv5nsv5JvI2L6MKC3iRsGRWmJ3MEVHySadyqEm9CMPM+7rqXjrW6/iRiCzahy7BB+JUWuYz1yG8fXVX3stWC9c1PH0vsOpG9nk9/Fo3pmgHelgn86KX7de0D8CVslyGkIbnRAihs0Dt2pfzcMyQ/aE8VjRZ3U8p366B/EB3QteYa6650qRxvCFr6shXUdEr01Oj0zB9+J8oxFbU7nmT8lN80SnQ94heIcILvNFBcyJPFX8yhphDzIGWZ9FvITDeItCTcGUmNg8hBEkGvnmwSlD5zf4HjA4IZYsJwxUsBi2hhqmigmFkqxRxX4aXnyoZhBNCKMooTf5BuPm+DPPsxnBijFM3dDEsICYwlFnCRtnfQCiweevjM0JgENoIKS+Xq3n8YugK4S6EgqaGJfu2UgxiQckfVzDSNriymwpwBBv7NDGmCUn+2te+Fo0U0oIPiiiTCsYKBLZpmckr41fHzrHOn7tBwrsIdcQgoe1N8E3LIj2TX9oWnrPXCmUQSgU6vwkjJTzaw4y4B6EAeZua1CPnN8rlvfRP2n8YdxguTd7735oM/IsRCz7sJ8cRQJiU+ZBiefQPYakpUQffW+197fuWSEe9037hNzzLxF3Hg+l7/HvOC+wzB1vnFdJh2IEJ9XLecKy9nPya48vvduMZnmJcEjaGUc+YTENucU5hmBI+S/0IV4fq+o00aRtdMW5Xf8Y+WwgI103JjVxCpTH2MAZxSFAme5vZg5ni5nmbjH3C2CnTVrRiSLSHYnpf5+OAsuEPyJ0DyJ5WBE5V/JamByfOLEgJ2eJ1IJy8il9RUjEa4R1kYuqwSsuqy++8xfsYI4T1Y4RzpgDhduBNKB58BOUKcBNeIB/OgLwd3HeqKrcKu3Sset70mvdZkz5G5jMGmBtwpjg1kWme1q8p33Ovibz1vH7NseA+7WKuTnkdPvEx1akcalKv3BlHParupW2u40ecqxgIyDb/4DiylTCK74jAxNufZuwFVt3IvrQuTcYU+KV6WNMxlb4n7YN8DJAufc7vVu2rSsu9nCiPkHPm1vSTGp6EM29hWx9s9TUuTuDETAkZnp4rkD7jbAh3uvMuQpFxAqfv4jwPyI1Ul5lpOU2e15VPGSl+yFrahcMVfQwZU/XuVvyJMw0dx3WOvL76LQTGRwR6spLLpOgevlS5cUBz4YgxiKGbrpz6d8+DNwslDWWIFRYv35/zHgQ4BwI4UWaqlKHgstqG8YbgQ/FkNQ9Dg2esgnBwDd52JkP2/LLai2KCAGV/r1NaNpNnLkSpIwZ0lVCiDBQMlMfUME6VTn9PKvRY4UXpRBF3JQSPO8a2E9iyV7gVpX1TlQbDlNUQViftX8+U/3u0Cb5peSh0rMCmxD4vBK/vS6VtCHEncMSgTvOxwo9Cw8ERUF09qviN/gMv2uSEUsCH+xg9de/1fOkVnsGzyocJCIMZowQFlvLSFQOMPPYFeR3yMcDqHpM1DgunNE0dD3qe9Er+dFUTnk+97qRlDzmGBc6dqnGVlsf3KnzTepImH88oJOCM4srqPhEPTowjHEyMNz6sqHPwEQ6Eun7L61KlGPt7/IqjDJmQjjueMX7pOxxbRElAGLlEhbC66I6y+CD502Tss7cfpcn3AKNc4l13xTkfBxTPOGVss5IJFqxi4viDr3ICJyjlN7BBgUOO5jiRlnGF8u8H6VSNV/asU2f6h3fkvMO5AMgwHG11+dkD7TKbw1fA2UKJyrFuocyDnHWp7KzjBdoEL2E0o1i3IrBOy2VM1I3VvCzy4HjD+eFU18eMEdpIpAbOAhw5zDMQ/VM3d/l7/Ep5qZyvwj+Xt57XrzkW3KfcNMLK73EWA0Q/tJsLY6LkT5N6UQ8fC541v5fzcB0/4rxjnk/laTpn+3vqrr7fkbMVcuoWq25kX14XdJV2YyrHj/xNxlT6nrwMnLspvqymgomvdLZrH+XmPJy+i+91RiVpqBPnOSArOagOGcVed+pC9BeEU4kVzZyYL4hMQWZA1CeVDXl6eHJc7MdN68AYJyoBWdFuP24VfyL7cfaIhIAQ+B8CPVnJTY01X7FKFaR8cuW3r1i5IEUp4sAaDAYUYww7DjGC0vK96hi/GI6sBEKEKaPge+gyg53wD8on/IpJgYmQyRVjFIGA9x5CeGP8+ooiZZMfxRBCOHKQBekgDjnBeCMPgh3vNULJQ2xjouwPCibhaoQ7YmxB+cTOvVTwYowh6EjPBGV7QqPwThUEyuU5k3MVVWGXpkMRJQ0H8fiprjyvwzctw9NzWin9zyRDvzChMAFixHo/pwKdsGsMC/oaHJmoMHzoJ09XV48qfmMVHAOAkEzKRfGifzxktcl78/ZxOBSKH7xEX2AYwUOsHlAeRtNpdpIjzwnDxDuMoeRGBf2a9ht93241tI4H8/q5Ip6+A2cOBibbBcCUOrOy6kpJHW/wjip8241n8hD6BS5gBuZ+KBB8ypjE+Icw8glf9XFX1295XarGTyw4+cM76J80HI3H9BdhwbYPMBpt3CNSAvnBFoZW1GTs+7ilDOQSGBA2SX9XjQPS0RZkDMRpmhiUrHRXETghFzgtG/6mjWzbwGkB5ThxD/7Dw+/h9fAXcpb6sGqAEQ7fusGDU4CDt3AS0G/wDgfmudFdlz+XY2CL4wNHEY4WVit8jFM/5BjP4WOojhdIw2oyBnce2sgzp7zcJmPV8/oVhxTj2j/cr+tj5gr6E/mGkUh7KQeqk2kxUfaHdqRynjLaydsse/yZY0HfU8+0H3JnCO9pNxfm72lSr5Q3PH9+L+fhOn5E1uEYh3+Yf9gmgFxxfoLX6YM6Yk6Hx/2gMU/fLVbdyj6vh1/hv3ZjKsePfE3GlJfPlfnBHVX8BkvC7nGmcWU+ZDxx4GFd+8if8zD3UkIG0s+UhQPSP+hMEDoO8yoHMbkThgghZHK6mosOgaOcbQnMfcgwHHjcx3noWzx4H3X39/iVcQ7Bk2x98/t+bfq8rnye50Y0uhJ6LIY4DgWXt7FC9qcVf8LzLIrAFyIhIAQSBEygDKKqzbuDEiU3zKtWHtLCIQAmgMqnpjjFQ2osRKS8Z5638kATP9jATvCLh+aYwlJYKOiAA6HS8stC7AvvsskoHpJh+3vjoSs22GMSW3EpbGUyHmJAmRwuYIZpmd3Cl+OhDGaoxMOpTIkrn5kwL0zZiwe38JwDY6hnSvwPNg52oWwOu+IgrXb/H9dWu+IhQ7ZKVJiRFA+44vAeM7jLYk1ZjQcpcKARxEEyvMMEbTzAiMNKLPRzwEFbFlYZ83DISxW1ws7T2mQVD+cwY8Nvldd2+JaJki/UATw47IKDe0zpLp/yPT0wyB9wGBRtspXFmMdWZQYd3tWuHjm/ebmmmMV+pS58ONwr5cEm7/WyuJpDJfKBt48+p7+cOFjIVmdiO0y5LWzlxh9V9jX8Y6HPZRpbGY7Y2QQa7zXhwTKzfTGv/qD/lUwZtjIXxxXjxELfC1NQymx1vEHCHN+68eyF879rOSjGjDC/Fa+maBYcEMO44mMr3YUZ4GWadv2W1yU/qKYsJPlizpYoI5Jb8asZ+3E8mvJaPmKcM47rqG7sc3AJB6LA17bqH/9fozl8YrFV4wBZlcsCc0TEg6VMsausDofFwW/wNrLPFLoS6xwnCqBd1MvJHHyF7Y2P8oX8ZljH/13qz80xVHCQnhnGUf6Y4lWYE8ofF3X5U97iIDVz9kRZDCYcMEi9bXW0LM8M31gX8HJqxwuME9tzPEDGeL70WlVuu7Ga5vXvYEG9kN0+D3TSx9TVnDvlAXyUSx+1mrv8vem1Ss63k7dpXv+eY1HFixwEBl84dSqHyNeuXmAJT9mq3f+3dye4jcIAFEA10py0F+3ROv5ITsExAVKa4s5DqgKExTyMg9fWU0zxrl3XxuGt+JhBphIf6m9y0rpSs307R8kw3A1QdPtyNpNBwnqDFp1h9ZW0bxbEaXbrmWr96v6Pnqm6Tf3MfZy/yyVu5F0kv4EZcCkDI5VMZ938Y+v6enH4tnOZyTHzjLV/ea/K70hpWfeRAQPbNDHhKAU1i0HtSqZ2Gngyx0rcKi1rFulNyXDfnaeet7S8mYKV9LWuq59J1+v72aPv9xw/11sK3+cE03zeCTMoaik0v/tuLX4mbU7cL5n7u32s+D8EjubbfqNKz+BPLnSW551my8idU9Pedv13LJfEeOojkRK28kM2nWLefHXrnKnJSHPAWjrXbp9SwDS/mzftq9ukZDC1cmkS3JvKD/xUepgS0N50NLwpbcsx26bOvWPXdamBSwliSlRT09GbEo6U6s6bave2e2bdlm97zNTU5Bpr/+v2+95yzpF+MmkKuTYdDUc9Tmq5UtvTs9lz3nqc+pl7kdqwtmawfp+Bnh5dR91u7+dWHNxznNRCpCYxtUqvmGKQfk2paUyz6HZKkpMao5TAtyXVddtH961us+czz3gGuEurjxicNW09+0l38uzWWuyzztsepzq26/cuZ3CepG9raUviTp6dXvqZc2ztX8ORtC+12kmn1+5DvBI3kpbNp15cSI17eZGcupvMt+3Nrx336LOapuqpuarTV+/x0TStl84fTW/XLOo1rX0eTYeOhmvtvO36PfExLabqIF91/1j3fgPq92d+PrI6M+3b80ytXVfvmWq3zaBVGdyuFFjdvopjrmHt92/r+npx+Hbwb5hJK5U8s6+6999wCbsP+co4vjtQNnyZwCvzbS+7qIMn6hn8eCY3CWj6aabZoIkAgXEFUlCRZvxpNpX+7+lbeoUp/ZzS9WHeLPMK4RKG5wTSxDYDWaV56lqG+bkj24sAgRTkputN/jNBXho9Y+IEgesL9DJ41w/1uSHsGZwy8NSzwczgPCnZm/cjfPZY9iNA4GcF0oczL0YZkCv9N68y5YUtrTlMv0Mg/2JLBvd33EtXcT2BjAWQ/uPpxyyDe737I0QECOwX+NFMbpoUlb4u+0NrSwIELiuQkXnriMJXCuRaU9srhVFY9gu0TVH372lLAgS2BN7e3rY28T0BAgSGEOh38hwi6AJJgAABAgQIECBAgAABAgSWAjK5Sw9LBAgQIECAAAECBAgQIDCwgEzuwDdP0AkQIECAAAECBAgQIEBgKSCTu/SwRIAAAQIECBAgQIAAAQIDC8jkDnzzBJ0AAQIECBAgQIAAAQIElgIyuUsPSwQIECBAgAABAgQIECAwsIBM7sA3T9AJECBAgAABAgQIECBAYCkgk7v0sESAAAECBAgQIECAAAECAwvI5A588wSdAAECBAgQIECAAAECBJYCMrlLD0sECBAgQIAAAQIECBAgMLCATO7AN0/QCRAgQIAAAQIECBAgQGAp8He5+Ln0/v7+uWCOAAECBAgQIECAAAECBC4nIN92f0v+fJTpfrU1BAgQIECAAAECBAgQIEBgPAHNlce7Z0JMgAABAgQIECBAgAABAisCMrkrMFYTIECAAAECBAgQIECAwHgCMrnj3TMhJkCAAAECBAgQIECAAIEVgX9kVzJdQzrd2wAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Review Analyzer\n",
    "\n",
    "The goal of this project is to design a classifier to use for sentiment analysis of product reviews. Our training set consists of reviews written by Amazon customers for various food products. The reviews, originally given on a 5 point scale, have been adjusted to a +1 or -1 scale, representing a positive or negative review, respectively.\n",
    "\n",
    "Below are two example entries from our dataset. Each entry consists of the review and its label. The two reviews were written by different customers describing their experience with a sugar-free candy.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In order to automatically analyze reviews, you will need to complete the following tasks:\n",
    "\n",
    "1) Implement and compare three types of linear classifiers: the perceptron algorithm, the average perceptron algorithm, and the Pegasos algorithm.\n",
    "\n",
    "2) Use your classifiers on the food review dataset, using some simple text features.\n",
    "\n",
    "3) Experiment with additional features and explore their impact on classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement the basic hinge loss calculation on a single data-point. Instead of the entire feature matrix, you are given one row, representing the feature vector of a single data sample, and its label of +1 or -1 representing the ground truth sentiment of the data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_utils as T\n",
    "\n",
    "def hinge_loss_single(feature_vector, label, theta, theta_0):\n",
    "    \"\"\"\n",
    "    Finds the hinge loss on a single data point given specific classification\n",
    "    parameters.\n",
    "\n",
    "    Args:\n",
    "        `feature_vector` - numpy array describing the given data point (X).\n",
    "        `label` - float, the correct classification of the data\n",
    "            point (y).\n",
    "        `theta` - numpy array describing the linear classifier (weights).\n",
    "        `theta_0` - float representing the offset parameter (bias).\n",
    "    Returns:\n",
    "        the hinge loss, as a float, associated with the given data point and\n",
    "        parameters.\n",
    "    \"\"\"\n",
    "    hinge_loss = max(0, 1 - label * (np.dot(theta, feature_vector) + theta_0))\n",
    "    return hinge_loss\n",
    "\n",
    "\n",
    "def test_hinge_loss_single():\n",
    "    ex_name = \"Hinge loss single\"\n",
    "\n",
    "    feature_vector = np.array([1, 2])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), -0.2\n",
    "    exp_res = 1 - 0.8\n",
    "    if T.check_real(\n",
    "            ex_name, hinge_loss_single,\n",
    "            exp_res, feature_vector, label, theta, theta_0):\n",
    "        return\n",
    "    T.log(T.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "test_hinge_loss_single()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement the complete hinge loss for a full set of data. Your input will be a full feature matrix this time, and you will have a vector of corresponding labels. The mth row of the feature matrix corresponds to the mth element of the labels vector. This function should return the appropriate loss of the classifier on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hinge_loss_full(feature_matrix, labels, theta, theta_0):\n",
    "    \"\"\"\n",
    "    Finds the hinge loss for given classification parameters averaged over a\n",
    "    given dataset\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point (X).\n",
    "        `labels` - numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix (y).\n",
    "        `theta` - numpy array describing the linear classifier (weights).\n",
    "        `theta_0` - real valued number representing the offset parameter (bias).\n",
    "    Returns:\n",
    "        the hinge loss, as a float, associated with the given dataset and\n",
    "        parameters.  This number should be the average hinge loss across all of\n",
    "    \"\"\"\n",
    "\n",
    "    hinge_loss = 0\n",
    "    m = len(feature_matrix) # number of data points\n",
    "    for i in range(m):\n",
    "        hinge_loss += max(0, 1 - labels[i] * (np.dot(theta, feature_matrix[i]) + theta_0))\n",
    "    \n",
    "    return hinge_loss / m\n",
    "\n",
    "\n",
    "def test_hinge_loss():\n",
    "    ex_name = \"Hinge loss full\"\n",
    "\n",
    "    feature_vector = np.array([[1, 2], [1, 2]])\n",
    "    label, theta, theta_0 = np.array([1, 1]), np.array([-1, 1]), -0.2\n",
    "    exp_res = 1 - 0.8\n",
    "    if T.check_real(\n",
    "            ex_name, hinge_loss_full,\n",
    "            exp_res, feature_vector, label, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    T.log(T.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "test_hinge_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Algorithm\n",
    "\n",
    "Implement the single step update for the perceptron algorithm (implemented with 0 - 1 loss).\n",
    "\n",
    "$$\n",
    "L(y, f(x)) = 1 \\space\\space if \\space\\space y \\space != f(x) \\space\\space else \\space 0\n",
    "$$\n",
    "\n",
    "\n",
    "**Note** In numerical computations, due to the limitations of floating-point precision, very small numbers close to zero might not be exactly zero. This can lead to unexpected results in comparisons. To mitigate this, you can define a small positive number, `epsilon = 1e-8`, and treat any number smaller than `epsilon` in absolute value as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_utils as T\n",
    "# ln -s /Users/n03an/Documents/projects/AI/edx/MIT-MachineLearning-MicroMasters/test/utils.py test_utils.py \n",
    "\n",
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    \"\"\"\n",
    "    Updates the classification parameters `theta` and `theta_0` via a single\n",
    "    step of the perceptron algorithm.  Returns new parameters rather than\n",
    "    modifying in-place.\n",
    "\n",
    "    Args:\n",
    "        feature_vector - A numpy array describing a single data point.\n",
    "        label - The correct classification of the feature vector.\n",
    "        current_theta - The current theta being used by the perceptron\n",
    "            algorithm before this update.\n",
    "        current_theta_0 - The current theta_0 being used by the perceptron\n",
    "            algorithm before this update.\n",
    "    Returns a tuple containing two values:\n",
    "        the updated feature-coefficient parameter `theta` as a numpy array\n",
    "        the updated offset parameter `theta_0` as a floating point number\n",
    "    \"\"\"\n",
    "    theta = current_theta\n",
    "    theta_0 = current_theta_0\n",
    "\n",
    "    if label * (np.dot(current_theta, feature_vector) + current_theta_0) <= 1e-7:\n",
    "        theta += label * feature_vector\n",
    "        theta_0 += label\n",
    "\n",
    "    return (theta, theta_0)\n",
    "\n",
    "def test_perceptron_single(name):\n",
    "    feature_vector = np.array([1, 2])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), -1.5\n",
    "    exp_res = (np.array([0, 3]), -0.5)\n",
    "\n",
    "    # response = perceptron_single_step_update(feature_vector, label, theta, theta_0)\n",
    "\n",
    "    if T.check_tuple(\n",
    "            name , perceptron_single_step_update,\n",
    "            exp_res, feature_vector, label, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    T.log(T.green(\"PASS\"), name, \"\")\n",
    "\n",
    "def test_perceptron_single_boundary(name):\n",
    "    feature_vector = np.array([1, 2])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), -1\n",
    "    exp_res = (np.array([0, 3]), 0)\n",
    "    if T.check_tuple(\n",
    "            name , perceptron_single_step_update,\n",
    "            exp_res, feature_vector, label, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    T.log(T.green(\"PASS\"), name, \"\")\n",
    "\n",
    "test_perceptron_single(\"Perceptron single update\")\n",
    "test_perceptron_single_boundary(\"Perceptron single update (boundary case)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_utils as test\n",
    "import utils as U\n",
    "\n",
    "def perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
    "    iterations through the data set: we do not stop early.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns a tuple containing two values:\n",
    "        the feature-coefficient parameter `theta` as a numpy array\n",
    "            (found after T iterations through the feature matrix)\n",
    "        the offset parameter `theta_0` as a floating point number\n",
    "            (found also after T iterations through the feature matrix).\n",
    "    \"\"\"\n",
    "\n",
    "    # In practice, people typically just randomly shuffle indices to do stochastic optimization.\n",
    "    # def get_order(n_samples):\n",
    "    #     \"\"\"Generate a random order for the samples.\"\"\"\n",
    "    #     order = list(range(n_samples))\n",
    "    #     np.random.shuffle(order)\n",
    "    #     return order\n",
    "    \n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    theta = np.zeros(nfeatures)\n",
    "    theta_0 = 0.0\n",
    "\n",
    "    for _ in range(T):\n",
    "        for i in U.get_order(nsamples):\n",
    "            theta, theta_0 = perceptron_single_step_update(\n",
    "            feature_matrix[i], labels[i], theta, theta_0)\n",
    "\n",
    "    return (theta, theta_0)\n",
    "\n",
    "def test_perceptron():\n",
    "    ex_name = \"Perceptron\"\n",
    "\n",
    "    feature_matrix = np.array([[1, 2]])\n",
    "    labels = np.array([1])\n",
    "    T = 1\n",
    "    exp_res = (np.array([1, 2]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "    else:\n",
    "        test.log(test.green(\"PASS\"), ex_name + f\": feature: {feature_matrix}, labels: {labels}, T: {T}\", \"\")\n",
    "\n",
    "    feature_matrix = np.array([[1, 2], [-1, 0]])\n",
    "    labels = np.array([1, 1])\n",
    "    T = 1\n",
    "    exp_res = (np.array([0, 2]), 2)\n",
    "    if test.check_tuple(\n",
    "            ex_name, perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "    else:\n",
    "        test.log(test.green(\"PASS\"), ex_name + f\": feature: {feature_matrix}, labels: {labels}, T: {T}\", \"\")\n",
    "\n",
    "    feature_matrix = np.array([[1, 2]])\n",
    "    labels = np.array([1])\n",
    "    T = 2\n",
    "    exp_res = (np.array([1, 2]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "    else:\n",
    "        test.log(test.green(\"PASS\"), ex_name + f\": feature: {feature_matrix}, labels: {labels}, T: {T}\", \"\")\n",
    "\n",
    "    feature_matrix = np.array([[1, 2], [-1, 0]])\n",
    "    labels = np.array([1, 1])\n",
    "    T = 2\n",
    "    exp_res = (np.array([0, 2]), 2)\n",
    "    if test.check_tuple(\n",
    "            ex_name, perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "    else:\n",
    "        test.log(test.green(\"PASS\"), ex_name + f\": feature: {feature_matrix}, labels: {labels}, T: {T}\", \"\")\n",
    "\n",
    "test_perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Perceptron Algorithm\n",
    "\n",
    "$$\n",
    "\\theta _{final} = \\frac{1}{nT}(\\theta ^{(1)} + \\theta ^{(2)} + ... + \\theta ^{(nT)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_utils as test\n",
    "import utils as U\n",
    "\n",
    "def average_perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
    "    iterations through the data set: we do not stop early.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns a tuple containing two values:\n",
    "        the average feature-coefficient parameter `theta` as a numpy array\n",
    "            (averaged over T iterations through the feature matrix)\n",
    "        the average offset parameter `theta_0` as a floating point number\n",
    "            (averaged also over T iterations through the feature matrix).\n",
    "    \"\"\"\n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    theta = np.zeros(nfeatures) #weights or coefficients\n",
    "    theta_0 = 0.0 # bias or offset\n",
    "    theta_sum = np.zeros(nfeatures)\n",
    "    theta_0_sum = 0.0\n",
    "\n",
    "    for _ in range(T):\n",
    "        for i in U.get_order(nsamples):\n",
    "            theta, theta_0 = perceptron_single_step_update(\n",
    "            feature_matrix[i], labels[i], theta, theta_0)\n",
    "            theta_sum += theta\n",
    "            theta_0_sum += theta_0\n",
    "\n",
    "    num_updates = T * nsamples\n",
    "    return (theta_sum / num_updates, theta_0_sum / num_updates)\n",
    "\n",
    "def test_average_perceptron():\n",
    "    ex_name = \"Average perceptron\"\n",
    "\n",
    "    feature_matrix = np.array([[1, 2]])\n",
    "    labels = np.array([1])\n",
    "    T = 1\n",
    "    exp_res = (np.array([1, 2]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, average_perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "\n",
    "    feature_matrix = np.array([[1, 2], [-1, 0]])\n",
    "    labels = np.array([1, 1])\n",
    "    T = 1\n",
    "    exp_res = (np.array([-0.5, 1]), 1.5)\n",
    "    if test.check_tuple(\n",
    "            ex_name, average_perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "\n",
    "    feature_matrix = np.array([[1, 2]])\n",
    "    labels = np.array([1])\n",
    "    T = 2\n",
    "    exp_res = (np.array([1, 2]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, average_perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "\n",
    "    feature_matrix = np.array([[1, 2], [-1, 0]])\n",
    "    labels = np.array([1, 1])\n",
    "    T = 2\n",
    "    exp_res = (np.array([-0.25, 1.5]), 1.75)\n",
    "    if test.check_tuple(\n",
    "            ex_name, average_perceptron,\n",
    "            exp_res, feature_matrix, labels, T):\n",
    "        return\n",
    "\n",
    "    test.log(test.green(\"PASS\"), ex_name, \"\")\n",
    "    \n",
    "\n",
    "test_average_perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasos Algorithm\n",
    "\n",
    "[Original Paper](https://courses.edx.org/assets/courseware/v1/16f13f7ac37ae86ebe0372f2410bcec4/asset-v1:MITx+6.86x+2T2024+type@asset+block/resources_pegasos.pdf)\n",
    "\n",
    "Pegasos Update Rule\n",
    "$$\\displaystyle \\left(x^{(i)}, y^{(i)}, \\lambda , \\eta , \\theta \\right): \\\\\n",
    "if \\space y^{(i)}(\\theta \\cdot x^{(i)}) \\leq 1 \\space\\space then \\\\\n",
    "\\space\\space\\space update \\space\\space \\theta = (1 - \\eta \\lambda ) \\theta + \\eta y^{(i)}x^{(i)} \\\\\n",
    "else: \\\\\n",
    "\\space\\space\\space update \\space\\space \\theta = (1 - \\eta \\lambda ) \\theta\n",
    "$$\n",
    "\n",
    "The $\\eta$ is a decaying factor that will decrease overtime. The $\\lambda$ is a regularizing parameter.\n",
    "\n",
    "Implement the single step update for the Pegasos algorithm. This function is very similar to the function that you implemented in Perceptron Single Step Update, except that it should utilize the Pegasos parameter update rules instead of those for perceptron. The function will also be passed a $\\eta$ and $\\lambda$ value to use for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_utils as test\n",
    "\n",
    "def pegasos_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        L,\n",
    "        eta,\n",
    "        theta,\n",
    "        theta_0):\n",
    "    \"\"\"\n",
    "    Updates the classification parameters `theta` and `theta_0` via a single\n",
    "    step of the Pegasos algorithm.  Returns new parameters rather than\n",
    "    modifying in-place.\n",
    "\n",
    "    Args:\n",
    "        `feature_vector` - A numpy array describing a single data point.\n",
    "        `label` - The correct classification of the feature vector.\n",
    "        `L` - The lamba value being used to update the parameters.\n",
    "        `eta` - Learning rate to update parameters.\n",
    "        `theta` - The old theta being used by the Pegasos\n",
    "            algorithm before this update.\n",
    "        `theta_0` - The old theta_0 being used by the\n",
    "            Pegasos algorithm before this update.\n",
    "    Returns:\n",
    "        a tuple where the first element is a numpy array with the value of\n",
    "        theta after the old update has completed and the second element is a\n",
    "        real valued number with the value of theta_0 after the old updated has\n",
    "        completed.\n",
    "    \"\"\"\n",
    "    mult = 1 - (eta * L)\n",
    "\n",
    "    if label * (np.dot(feature_vector, theta) + theta_0) <= 1:\n",
    "        return ((mult * theta) + (eta * label * feature_vector),\n",
    "                (theta_0) + (eta * label))\n",
    "\n",
    "    return (mult * theta, theta_0)\n",
    "\n",
    "\n",
    "def check_pegasos_single_update():\n",
    "    ex_name = \"Pegasos single update\"\n",
    "\n",
    "    feature_vector = np.array([1, 2])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), -1.5\n",
    "    L = 0.2\n",
    "    eta = 0.1\n",
    "    exp_res = (np.array([-0.88, 1.18]), -1.4)\n",
    "    if test.check_tuple(\n",
    "            ex_name, pegasos_single_step_update,\n",
    "            exp_res,\n",
    "            feature_vector, label, L, eta, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    feature_vector = np.array([1, 1])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), 1\n",
    "    L = 0.2\n",
    "    eta = 0.1\n",
    "    exp_res = (np.array([-0.88, 1.08]), 1.1)\n",
    "    if test.check_tuple(\n",
    "            ex_name +  \" (boundary case)\", pegasos_single_step_update,\n",
    "            exp_res,\n",
    "            feature_vector, label, L, eta, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    feature_vector = np.array([1, 2])\n",
    "    label, theta, theta_0 = 1, np.array([-1, 1]), -2\n",
    "    L = 0.2\n",
    "    eta = 0.1\n",
    "    exp_res = (np.array([-0.88, 1.18]), -1.9)\n",
    "    if test.check_tuple(\n",
    "            ex_name, pegasos_single_step_update,\n",
    "            exp_res,\n",
    "            feature_vector, label, L, eta, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    test.log(test.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "check_pegasos_single_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pegasos Algorithm\n",
    "\n",
    "Finally you will implement the full Pegasos algorithm. You will be given the same feature matrix and labels array as you were given in Full Perceptron Algorithm. You will also be given $T$, the maximum number of times that you should iterate through the feature matrix before terminating the algorithm. Initialize $\\theta$ and $\\theta _0$ to zero. For each update, set $\\eta = \\frac{1}{\\sqrt{t}}$ where $t$ is a counter for the number of updates performed so far (between $1$ and $nT$ inclusive). This function should return a tuple in which the first element is the final value of $\\theta$ and the second element is the value of $\\theta _0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_utils as test\n",
    "import utils as U\n",
    "\n",
    "def pegasos(feature_matrix, labels, T, L):\n",
    "    \"\"\"\n",
    "    Runs the Pegasos algorithm on a given set of data. Runs T iterations\n",
    "    through the data set, there is no need to worry about stopping early.  For\n",
    "    each update, set learning rate = 1/sqrt(t), where t is a counter for the\n",
    "    number of updates performed so far (between 1 and nT inclusive).\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.  Do\n",
    "    not copy paste code from previous parts.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - A numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - An integer indicating how many times the algorithm\n",
    "            should iterate through the feature matrix.\n",
    "        `L` - The lamba value being used to update the Pegasos\n",
    "            algorithm parameters.\n",
    "\n",
    "    Returns:\n",
    "        a tuple where the first element is a numpy array with the value of the\n",
    "        theta, the linear classification parameter, found after T iterations\n",
    "        through the feature matrix and the second element is a real number with\n",
    "        the value of the theta_0, the offset classification parameter, found\n",
    "        after T iterations through the feature matrix.\n",
    "    \"\"\"\n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    theta = np.zeros(nfeatures)\n",
    "    theta_0 = 0\n",
    "    num_updates = 0\n",
    "\n",
    "    for _ in range(T):\n",
    "        for i in U.get_order(nsamples):\n",
    "            num_updates += 1\n",
    "            eta = 1.0 / np.sqrt(num_updates)\n",
    "            (theta, theta_0) = pegasos_single_step_update(feature_matrix[i],\n",
    "                                                          labels[i], L, eta,\n",
    "                                                          theta, theta_0)\n",
    "\n",
    "    return (theta, theta_0)\n",
    "\n",
    "def check_pegasos():\n",
    "    ex_name = \"Pegasos\"\n",
    "\n",
    "    feature_matrix = np.array([[1, 2]])\n",
    "    labels = np.array([1])\n",
    "    T = 1\n",
    "    L = 0.2\n",
    "    exp_res = (np.array([1, 2]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, pegasos,\n",
    "            exp_res, feature_matrix, labels, T, L):\n",
    "        return\n",
    "\n",
    "    feature_matrix = np.array([[1, 1], [1, 1]])\n",
    "    labels = np.array([1, 1])\n",
    "    T = 1\n",
    "    L = 1\n",
    "    exp_res = (np.array([1-1/np.sqrt(2), 1-1/np.sqrt(2)]), 1)\n",
    "    if test.check_tuple(\n",
    "            ex_name, pegasos,\n",
    "            exp_res, feature_matrix, labels, T, L):\n",
    "        return\n",
    "\n",
    "    test.log(test.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "check_pegasos()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given above 3 learning algorithms i.e. **Perceptron**, **Average Perceptron** and **Pegasos**, you should qualitatively verify your implementations.\n",
    "\n",
    "Train your model using dataset `toy_data.txt` and $T = 10$, $\\lambda = 0.2$. Compute $\\theta$ and $\\theta _0$ using all 3 learning algorithms, while plotting the data using `plot_toy_data` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as U\n",
    "\n",
    "toy_features, toy_labels = toy_data = U.load_toy_data('data/toy_data.tsv')\n",
    "\n",
    "T = 10\n",
    "L = 0.2\n",
    "\n",
    "thetas_perceptron = perceptron(toy_features, toy_labels, T)\n",
    "thetas_avg_perceptron = average_perceptron(toy_features, toy_labels, T)\n",
    "thetas_pegasos = pegasos(toy_features, toy_labels, T, L)\n",
    "\n",
    "def plot_toy_results(algo_name, thetas):\n",
    "    print('theta for', algo_name, 'is', ', '.join(map(str,list(thetas[0]))))\n",
    "    print('theta_0 for', algo_name, 'is', str(thetas[1]))\n",
    "    U.plot_toy_data(algo_name, toy_features, toy_labels, thetas)\n",
    "\n",
    "plot_toy_results('Perceptron', thetas_perceptron)\n",
    "plot_toy_results('Average Perceptron', thetas_avg_perceptron)\n",
    "plot_toy_results('Pegasos', thetas_pegasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Implement a classification function that uses $\\theta$ and $\\theta _0$ to classify a set of data points. You are given the feature matrix, $\\theta$, and $\\theta _0$ as defined in previous sections. This function should return a numpy array of -1s and 1s. If a prediction is **greater than zero**, it should be considered a positive classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_utils as T\n",
    "\n",
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses given parameters to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `theta` - numpy array describing the linear classifier.\n",
    "        `theta_0` - real valued number representing the offset parameter.\n",
    "\n",
    "    Returns:\n",
    "        a numpy array of 1s and -1s where the kth element of the array is the\n",
    "        predicted classification of the kth row of the feature matrix using the\n",
    "        given theta and theta_0. If a prediction is GREATER THAN zero, it\n",
    "        should be considered a positive classification.\n",
    "    \"\"\"\n",
    "    (nsamples, _) = feature_matrix.shape\n",
    "    predictions = np.zeros(nsamples)\n",
    "\n",
    "    for i in range(nsamples):\n",
    "        if (np.dot(theta, feature_matrix[i]) + theta_0) <= 1e-7:\n",
    "            predictions[i] = -1\n",
    "        else:\n",
    "            predictions[i] = 1\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "    # alternate single line solution\n",
    "    # return (feature_matrix @ theta + theta_0 > 1e-7) * 2.0 - 1\n",
    "\n",
    "def check_classify():\n",
    "    ex_name = \"Classify\"\n",
    "\n",
    "    feature_matrix = np.array([[1, 1], [1, 1], [1, 1]])\n",
    "    theta = np.array([1, 1])\n",
    "    theta_0 = 0\n",
    "    exp_res = np.array([1, 1, 1])\n",
    "    if T.check_array(\n",
    "            ex_name, classify,\n",
    "            exp_res, feature_matrix, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    feature_matrix = np.array([[-1, 1]])\n",
    "    theta = np.array([1, 1])\n",
    "    theta_0 = 0\n",
    "    exp_res = np.array([-1])\n",
    "    if T.check_array(\n",
    "            ex_name + \" (boundary case)\", classify,\n",
    "            exp_res, feature_matrix, theta, theta_0):\n",
    "        return\n",
    "\n",
    "    T.log(T.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "check_classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "#### Training Accuracy\n",
    "Training accuracy is the measure of how well the classification model predicts the labels of the data it was trained on. It is calculated after the model has been trained on the training dataset.High training accuracy indicates that the model has learned well from the training data. However, very high training accuracy might also suggest that the model is overfitting, meaning it has learned the training data too well, including its noise and outliers, which could lead to poor performance on unseen data.\n",
    "\n",
    "#### Validation Accuracy\n",
    "Validation accuracy measures how well the classification model predicts the labels of a new dataset that was not used during training. This dataset is known as the validation set. Validation accuracy is a better indicator of how the model will perform on unseen data. It helps in understanding the model's generalization ability. If the validation accuracy is significantly lower than the training accuracy, it might indicate overfitting.\n",
    "\n",
    "**example** If a model correctly predicts the labels of 95 out of 100 samples in the training set, the training accuracy is 95%. If it correctly predicts the labels of 90 out of 100 samples in a validation set, the validation accuracy is 90%.\n",
    "\n",
    "\n",
    "#### Given accuracy function\n",
    "```\n",
    "def accuracy(preds, targets):\n",
    "\t\"\"\"\n",
    "\tGiven length-N vectors containing predicted and target labels,\n",
    "\treturns the percentage and number of correct predictions.\n",
    "\t\"\"\"\n",
    "\treturn (preds == targets).mean()\n",
    "```\n",
    "\n",
    "The accuracy function takes a numpy array of predicted labels and a numpy array of actual labels and returns the prediction accuracy. You should use this function along with the functions that you have implemented thus far in order to implement classifier_accuracy.\n",
    "\n",
    "The classifier_accuracy function should take 6 arguments:\n",
    "\n",
    "- a classifier function that, itself, takes arguments (feature_matrix, labels, **kwargs)\n",
    "- the training feature matrix\n",
    "- the validation feature matrix\n",
    "- the training labels\n",
    "- the valiation labels\n",
    "- a **kwargs argument to be passed to the classifier function\n",
    "\n",
    "This function should train the given classifier using the training data and then compute compute the classification accuracy on both the train and validation data. The return values should be a tuple where the first value is the training accuracy and the second value is the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_utils as test\n",
    "\n",
    "def accuracy(preds, targets):\n",
    "    \"\"\"\n",
    "    Given length-N vectors containing predicted and target labels,\n",
    "    returns the fraction of predictions that are correct.\n",
    "    \"\"\"\n",
    "    return (preds == targets).mean()\n",
    "\n",
    "def classifier_accuracy(\n",
    "        classifier,\n",
    "        train_feature_matrix,\n",
    "        val_feature_matrix,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier and computes accuracy.  The classifier is\n",
    "    trained on the train data.  The classifier's accuracy on the train and\n",
    "    validation data is then returned.\n",
    "\n",
    "    Args:\n",
    "        `classifier` - A learning function that takes arguments\n",
    "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
    "        `train_feature_matrix` - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        `val_feature_matrix` - A numpy matrix describing the validation\n",
    "            data. Each row represents a single data point.\n",
    "        `train_labels` - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the training\n",
    "            feature matrix.\n",
    "        `val_labels` - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the validation\n",
    "            feature matrix.\n",
    "        `kwargs` - Additional named arguments to pass to the classifier\n",
    "            (e.g. T or L)\n",
    "\n",
    "    Returns:\n",
    "        a tuple in which the first element is the (scalar) accuracy of the\n",
    "        trained classifier on the training data and the second element is the\n",
    "        accuracy of the trained classifier on the validation data.\n",
    "    \"\"\"\n",
    "    # get weights and bias using classifier algorithm\n",
    "    theta, theta_0 = classifier(train_feature_matrix, train_labels, **kwargs)\n",
    "\n",
    "    train_predictions = classify(train_feature_matrix, theta, theta_0)\n",
    "    val_predictions = classify(val_feature_matrix, theta, theta_0)\n",
    "\n",
    "    train_accuracy = accuracy(train_predictions, train_labels)\n",
    "    val_accuracy = accuracy(val_predictions, val_labels)\n",
    "\n",
    "    return (train_accuracy, val_accuracy)\n",
    "\n",
    "\n",
    "def check_classifier_accuracy():\n",
    "    ex_name = \"Classifier accuracy\"\n",
    "\n",
    "    train_feature_matrix = np.array([[1, 0], [1, -1], [2, 3]])\n",
    "    val_feature_matrix = np.array([[1, 1], [2, -1]])\n",
    "    train_labels = np.array([1, -1, 1])\n",
    "    val_labels = np.array([-1, 1])\n",
    "    exp_res = 1, 0\n",
    "    T=1\n",
    "    if test.check_tuple(\n",
    "            ex_name, classifier_accuracy,\n",
    "            exp_res,\n",
    "            perceptron,\n",
    "            train_feature_matrix, val_feature_matrix,\n",
    "            train_labels, val_labels,\n",
    "            T=T):\n",
    "        return\n",
    "\n",
    "    train_feature_matrix = np.array([[1, 0], [1, -1], [2, 3]])\n",
    "    val_feature_matrix = np.array([[1, 1], [2, -1]])\n",
    "    train_labels = np.array([1, -1, 1])\n",
    "    val_labels = np.array([-1, 1])\n",
    "    exp_res = 1, 0\n",
    "    T=1\n",
    "    L=0.2\n",
    "    if test.check_tuple(\n",
    "            ex_name, classifier_accuracy,\n",
    "            exp_res,\n",
    "            pegasos,\n",
    "            train_feature_matrix, val_feature_matrix,\n",
    "            train_labels, val_labels,\n",
    "            T=T, L=L):\n",
    "        return\n",
    "\n",
    "    test.log(test.green(\"PASS\"), ex_name, \"\")\n",
    "\n",
    "check_classifier_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import test_utils as test\n",
    "\n",
    "def extract_words(text):\n",
    "    \"\"\"\n",
    "    Helper function for `bag_of_words(...)`.\n",
    "    Args:\n",
    "        a string `text`.\n",
    "    Returns:\n",
    "        a list of lowercased words in the string, where punctuation and digits\n",
    "        count as their own words.\n",
    "    \"\"\"\n",
    "    for c in punctuation + digits:\n",
    "        text = text.replace(c, ' ' + c + ' ')\n",
    "    return text.lower().split()\n",
    "\n",
    "def bag_of_words(texts):\n",
    "    \"\"\"\n",
    "    NOTE: feel free to change this code as guided by Section 3 (e.g. remove\n",
    "    stopwords, add bigrams etc.)\n",
    "\n",
    "    Args:\n",
    "        `texts` - a list of natural language strings.\n",
    "    Returns:\n",
    "        a dictionary that maps each word appearing in `texts` to a unique\n",
    "        integer `index`.\n",
    "    \"\"\"\n",
    "\n",
    "    # stopwords = np.loadtxt(\"stopwords.txt\", dtype=\"str\")\n",
    "    \n",
    "    dictionary = {}  # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            # if word in stopwords:\n",
    "            #     continue\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, dictionary):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        `reviews` - a list of natural language strings\n",
    "        `indices_by_word` - a dictionary of uniquely-indexed words.\n",
    "    Returns:\n",
    "        a matrix representing each review via bag-of-words features.  This\n",
    "        matrix thus has shape (n, m), where n counts reviews and m counts words\n",
    "        in the dictionary.\n",
    "    \"\"\"\n",
    "    num_reviews = len(reviews)\n",
    "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
    "\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word in dictionary:\n",
    "                feature_matrix[i, dictionary[word]] = 1\n",
    "                # feature_matrix[i, dictionary[word]] += 1\n",
    "    return feature_matrix\n",
    "\n",
    "# Tests\n",
    "def check_extract_bow_feature_vectors():\n",
    "    ex_name = \"Extract bow feature vectors\"\n",
    "    texts = [\n",
    "        \"He loves her \",\n",
    "        \"He really really loves her\"]\n",
    "    keys = [\"he\", \"loves\", \"her\", \"really\"]\n",
    "    dictionary = {k:i for i, k in enumerate(keys)}\n",
    "    exp_res = np.array(\n",
    "        [[1, 1, 1, 0],\n",
    "        [1, 1, 1, 1]])\n",
    "    non_bin_res = np.array(\n",
    "        [[1, 1, 1, 0],\n",
    "        [1, 1, 1, 2]])\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = extract_bow_feature_vectors(texts, dictionary)\n",
    "    except NotImplementedError:\n",
    "        test.log(test.red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return\n",
    "\n",
    "    if not type(res) == np.ndarray:\n",
    "        test.log(test.red(\"FAIL\"), ex_name, \": does not return a numpy array, type: \", type(res))\n",
    "        return\n",
    "    if not len(res) == len(exp_res):\n",
    "        test.log(test.red(\"FAIL\"), ex_name, \": expected an array of shape \", exp_res.shape, \" but got array of shape\", res.shape)\n",
    "        return\n",
    "\n",
    "    test.log(test.green(\"PASS\"), ex_name)\n",
    "\n",
    "    if (res == exp_res).all():\n",
    "        test.log(test.yellow(\"WARN\"), ex_name, \": uses binary indicators as features\")\n",
    "    elif (res == non_bin_res).all():\n",
    "        test.log(test.green(\"PASS\"), ex_name, \": correct non binary features\")\n",
    "    else:\n",
    "        test.log(test.red(\"FAIL\"), ex_name, \": unexpected feature matrix\")\n",
    "        return\n",
    "    \n",
    "check_extract_bow_feature_vectors()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accurancy for each Algorithm\n",
    "\n",
    "Report the training and validation accuracies of each algorithm with $T = 10$ and $\\lambda$ = 0.01. The $\\lambda$ value only (applies to Pegasos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as U\n",
    "\n",
    "train_data = U.load_data('data/reviews_train.tsv')\n",
    "val_data = U.load_data('data/reviews_val.tsv')\n",
    "test_data = U.load_data('data/reviews_test.tsv')\n",
    "\n",
    "train_texts, train_labels = zip(*((sample['text'], sample['sentiment']) for sample in train_data))\n",
    "val_texts, val_labels = zip(*((sample['text'], sample['sentiment']) for sample in val_data))\n",
    "test_texts, test_labels = zip(*((sample['text'], sample['sentiment']) for sample in test_data))\n",
    "\n",
    "dictionary = bag_of_words(train_texts)\n",
    "\n",
    "train_bow_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "val_bow_features = extract_bow_feature_vectors(val_texts, dictionary)\n",
    "test_bow_features = extract_bow_feature_vectors(test_texts, dictionary)\n",
    "\n",
    "\n",
    "perceptron_accuracy = classifier_accuracy(\n",
    "    classifier=perceptron, \n",
    "    train_feature_matrix=train_bow_features, \n",
    "    val_feature_matrix=val_bow_features, \n",
    "    train_labels=train_labels, \n",
    "    val_labels=val_labels, \n",
    "    T=10\n",
    ")\n",
    "\n",
    "average_perceptron_accuracy = classifier_accuracy(\n",
    "    classifier=average_perceptron, \n",
    "    train_feature_matrix=train_bow_features, \n",
    "    val_feature_matrix=val_bow_features, \n",
    "    train_labels=train_labels, \n",
    "    val_labels=val_labels, \n",
    "    T=10\n",
    ")\n",
    "\n",
    "pegasos_accuracy = classifier_accuracy(\n",
    "    classifier=pegasos, \n",
    "    train_feature_matrix=train_bow_features, \n",
    "    val_feature_matrix=val_bow_features, \n",
    "    train_labels=train_labels, \n",
    "    val_labels=val_labels, \n",
    "    T=10, \n",
    "    # T=25, \n",
    "    L=0.01\n",
    ")\n",
    "\n",
    "print(\"Perceptron: train accuracy = %.5f, validation accuracy = %.5f\" % perceptron_accuracy)\n",
    "print(\"Average Perceptron: train accuracy = %.5f, validation accuracy = %.5f\" % average_perceptron_accuracy)\n",
    "print(\"Pegasos: train accuracy = %.5f, validation accuracy = %.5f\" % pegasos_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "You finally have your algorithms up and running, and a way to measure performance! But, it's still unclear what values the hyperparameters like $T$ and $\\lambda$ should have. In this section, you'll tune these hyperparameters to maximize the performance of each model.\n",
    "\n",
    "One way to tune your hyperparameters for any given Machine Learning algorithm is to perform a grid search over all the possible combinations of values. If your hyperparameters can be any real number, you will need to limit the search to some finite set of possible values for each hyperparameter. For efficiency reasons, often you might want to tune one individual parameter, keeping all others constant, and then move onto the next one; Compared to a full grid search there are many fewer possible combinations to check, and this is what you'll be doing for the questions below.\n",
    "\n",
    "Below example show various values of $T$ and $\\lambda$ to derive the best value. For pegasos algorithm, first it will fix $\\lambda = 0.01$ to tune $T$ and then use best $T$ to tune $\\lambda$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "\n",
    "def tune(train_fn, param_vals, train_feats, train_labels, val_feats, val_labels):\n",
    "    train_accs = np.ndarray(len(param_vals))\n",
    "    val_accs = np.ndarray(len(param_vals))\n",
    "\n",
    "    for i, val in enumerate(param_vals):\n",
    "        theta, theta_0 = train_fn(train_feats, train_labels, val)\n",
    "\n",
    "        train_preds = classify(train_feats, theta, theta_0)\n",
    "        train_accs[i] = accuracy(train_preds, train_labels)\n",
    "\n",
    "        val_preds = classify(val_feats, theta, theta_0)\n",
    "        val_accs[i] = accuracy(val_preds, val_labels)\n",
    "\n",
    "    return train_accs, val_accs\n",
    "\n",
    "def parameter_tuning():\n",
    "    data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
    "\n",
    "    # values of T and lambda to try\n",
    "    Ts = [1, 5, 10, 15, 25, 50] #number of iterations\n",
    "    Ls = [0.001, 0.01, 0.1, 1, 10] #regularization parameter\n",
    "\n",
    "    pct_tune_results = tune(perceptron, Ts, *data)\n",
    "    print('perceptron valid:', list(zip(Ts, pct_tune_results[1])))\n",
    "    print('best = {:.4f}, T={:.4f}'.format(np.max(pct_tune_results[1]), Ts[np.argmax(pct_tune_results[1])]))\n",
    "\n",
    "    avg_pct_tune_results = tune(average_perceptron, Ts, *data)\n",
    "    print('avg perceptron valid:', list(zip(Ts, avg_pct_tune_results[1])))\n",
    "    print('best = {:.4f}, T={:.4f}'.format(np.max(avg_pct_tune_results[1]), Ts[np.argmax(avg_pct_tune_results[1])]))\n",
    "\n",
    "    # fix values for L and T while tuning Pegasos T and L, respective\n",
    "    fix_L = 0.01\n",
    "    def train_fn(features, labels, T):\n",
    "        return pegasos(features, labels, T, fix_L)\n",
    "    peg_tune_results_T = tune(train_fn, Ts, *data) \n",
    "    print('Pegasos valid: tune T', list(zip(Ts, peg_tune_results_T[1])))\n",
    "    print('best = {:.4f}, T={:.4f}'.format(np.max(peg_tune_results_T[1]), Ts[np.argmax(peg_tune_results_T[1])]))\n",
    "\n",
    "    fix_T = Ts[np.argmax(peg_tune_results_T[1])]\n",
    "    def train_fn(features, labels, L):\n",
    "        return pegasos(features, labels, fix_T, L)\n",
    "    peg_tune_results_L = tune(train_fn, Ls, *data)\n",
    "    print('Pegasos valid: tune L', list(zip(Ls, peg_tune_results_L[1])))\n",
    "    print('best = {:.4f}, L={:.4f}'.format(np.max(peg_tune_results_L[1]), Ls[np.argmax(peg_tune_results_L[1])]))\n",
    "\n",
    "    utils.plot_tune_results('Perceptron', 'T', Ts, *pct_tune_results)\n",
    "    utils.plot_tune_results('Avg Perceptron', 'T', Ts, *avg_pct_tune_results)\n",
    "    utils.plot_tune_results('Pegasos', 'T', Ts, *peg_tune_results_T)\n",
    "    utils.plot_tune_results('Pegasos', 'L', Ls, *peg_tune_results_L)\n",
    "\n",
    "parameter_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on the test set\n",
    "\n",
    "After you have chosen your best method (perceptron, average perceptron or Pegasos) and parameters, use this classifier to compute testing accuracy on the test set.\n",
    "\n",
    "We have supplied the feature matrix and labels in `main.py` as `test_bow_features` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg_best_T = 25\n",
    "peg_best_L = 0.01\n",
    "peg_train_accuracy, peg_test_accuracy = classifier_accuracy(pegasos,\n",
    "                                                            train_bow_features,\n",
    "                                                            test_bow_features,\n",
    "                                                            train_labels,\n",
    "                                                            test_labels,\n",
    "                                                            T=peg_best_T,\n",
    "                                                            L=peg_best_L)\n",
    "\n",
    "print(peg_test_accuracy)\n",
    "peg_theta, peg_theta_0 = pegasos(train_bow_features, train_labels,\n",
    "                                    peg_best_T, peg_best_L)\n",
    "print(peg_theta, peg_theta_0)\n",
    "peg_test_preds = classify(test_bow_features, peg_theta, peg_theta_0)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Assign to best_theta, the weights (and not the bias!) learned by your most\n",
    "# accurate algorithm with the optimal choice of hyperparameters.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "best_theta = peg_theta  # Your code here\n",
    "wordlist = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "sorted_word_features = utils.most_explanatory_word(best_theta, wordlist)\n",
    "print(\"Most Explanatory Word Features\")\n",
    "print(sorted_word_features[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

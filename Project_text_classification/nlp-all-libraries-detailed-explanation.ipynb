{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe07ea2d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.022744,
     "end_time": "2024-07-27T00:54:23.141142",
     "exception": false,
     "start_time": "2024-07-27T00:54:23.118398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NLP Python Libraries\n",
    "\n",
    "1. [NLTK (Natural Language Toolkit)](#1)\n",
    "2. [spaCy](#2)\n",
    "3. [TextBlob](#3)\n",
    "4. [Hugging Face Transformer](#4)\n",
    "5. Gensim\n",
    "6. Textacy\n",
    "7. VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "8. AllenNLP\n",
    "9. Stanza\n",
    "10. Pattern\n",
    "11. PyNLPl \n",
    "12. flair Library\n",
    "13. FastText\n",
    "14. Polyglot\n",
    "15. Regex (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a17d4",
   "metadata": {
    "papermill": {
     "duration": 0.022382,
     "end_time": "2024-07-27T00:54:23.187416",
     "exception": false,
     "start_time": "2024-07-27T00:54:23.165034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. NLTK (Natural Language Toolkit) <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63d033",
   "metadata": {
    "papermill": {
     "duration": 0.022006,
     "end_time": "2024-07-27T00:54:23.231533",
     "exception": false,
     "start_time": "2024-07-27T00:54:23.209527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f5e8f",
   "metadata": {
    "papermill": {
     "duration": 0.022135,
     "end_time": "2024-07-27T00:54:23.275934",
     "exception": false,
     "start_time": "2024-07-27T00:54:23.253799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Installation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b155cb5",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:23.321967Z",
     "iopub.status.busy": "2024-07-27T00:54:23.321576Z",
     "iopub.status.idle": "2024-07-27T00:54:37.118313Z",
     "shell.execute_reply": "2024-07-27T00:54:37.117062Z"
    },
    "papermill": {
     "duration": 13.823039,
     "end_time": "2024-07-27T00:54:37.121077",
     "exception": false,
     "start_time": "2024-07-27T00:54:23.298038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35043fc",
   "metadata": {
    "papermill": {
     "duration": 0.022853,
     "end_time": "2024-07-27T00:54:37.166716",
     "exception": false,
     "start_time": "2024-07-27T00:54:37.143863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Accessing Additional Resources:**\n",
    "\n",
    "To incorporate the usage of additional resources, such as recourses of languages other than English – you can run the following in a python script. It has to be done only once when you are running it for the first time in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8500593e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:37.215002Z",
     "iopub.status.busy": "2024-07-27T00:54:37.213961Z",
     "iopub.status.idle": "2024-07-27T00:54:56.956308Z",
     "shell.execute_reply": "2024-07-27T00:54:56.955082Z"
    },
    "papermill": {
     "duration": 19.768824,
     "end_time": "2024-07-27T00:54:56.958840",
     "exception": false,
     "start_time": "2024-07-27T00:54:37.190016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "clear_output()     # This function clears the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262def48",
   "metadata": {
    "papermill": {
     "duration": 0.022444,
     "end_time": "2024-07-27T00:54:57.004084",
     "exception": false,
     "start_time": "2024-07-27T00:54:56.981640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "let’s perform some basic operations on text data using NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a23efc",
   "metadata": {
    "papermill": {
     "duration": 0.02223,
     "end_time": "2024-07-27T00:54:57.049106",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.026876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tokenization\n",
    "Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaabe425",
   "metadata": {
    "papermill": {
     "duration": 0.022504,
     "end_time": "2024-07-27T00:54:57.094197",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.071693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Word Tokenization**\n",
    "\n",
    "It involves breaking down the text into words.\n",
    "\n",
    "* \"I study Machine Learning on Kaggle.\" \n",
    "\n",
    "* ['I', 'study', 'Machine', 'Learning', 'on', 'Kaggle', '.']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323efe7",
   "metadata": {
    "papermill": {
     "duration": 0.022806,
     "end_time": "2024-07-27T00:54:57.201211",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.178405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sentence Tokenization**\n",
    "\n",
    "It involves breaking down the text into individual sentences. \n",
    "\n",
    "\n",
    "* \"I study Machine Learning on Kaggle. Currently, I'm studying NLP\"\n",
    "\n",
    "*  ['I study Machine Learning on Kaggle.', 'Currently, I'm studying NLP.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcbedb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:57.249432Z",
     "iopub.status.busy": "2024-07-27T00:54:57.249016Z",
     "iopub.status.idle": "2024-07-27T00:54:57.266304Z",
     "shell.execute_reply": "2024-07-27T00:54:57.264876Z"
    },
    "papermill": {
     "duration": 0.044662,
     "end_time": "2024-07-27T00:54:57.268993",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.224331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kaggle', 'is', 'a', 'great', 'learning', 'platform', '.', 'It', 'is', 'one', 'of', 'the', 'best', 'for', 'Data', 'Science', 'students', '.']\n",
      "['Kaggle is a great learning platform.', 'It is one of the best for Data Science students.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using NLTK\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = \"Kaggle is a great learning platform. It is one of the best for Data Science students.\"\n",
    "print(word_tokenize(sent))\n",
    "print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249381f",
   "metadata": {
    "papermill": {
     "duration": 0.022773,
     "end_time": "2024-07-27T00:54:57.314741",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.291968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Stemming and Lemmatization \n",
    "\n",
    "When working with Natural Language, we are not much interested in the form of words – rather, we are concerned with the meaning that the words intend to convey. Thus, we try to map every word of the language to its root/base form. This process is called canonicalization. \n",
    "\n",
    "E.g. The words ‘play’, ‘plays’, ‘played’, and ‘playing’ convey the same action – hence, we can map them all to their base form i.e. ‘play’.\n",
    "\n",
    "Now, there are two widely used canonicalization techniques: Stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21cfe3",
   "metadata": {
    "papermill": {
     "duration": 0.022507,
     "end_time": "2024-07-27T00:54:57.360512",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.338005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f3a5f",
   "metadata": {
    "papermill": {
     "duration": 0.022689,
     "end_time": "2024-07-27T00:54:57.407122",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.384433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Stemming generates the base word from the inflected word by removing the affixes of the word. It has a set of pre-defined rules that govern the dropping of these affixes. It must be noted that stemmers might not always result in semantically meaningful base words.  Stemmers are faster and computationally less expensive than lemmatizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737d2155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:57.469839Z",
     "iopub.status.busy": "2024-07-27T00:54:57.468340Z",
     "iopub.status.idle": "2024-07-27T00:54:57.478635Z",
     "shell.execute_reply": "2024-07-27T00:54:57.476657Z"
    },
    "papermill": {
     "duration": 0.045619,
     "end_time": "2024-07-27T00:54:57.482059",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.436440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "print(porter.stem(\"play\"))\n",
    "print(porter.stem(\"playing\"))\n",
    "print(porter.stem(\"plays\"))\n",
    "print(porter.stem(\"played\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a504fc",
   "metadata": {
    "papermill": {
     "duration": 0.027811,
     "end_time": "2024-07-27T00:54:57.544036",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.516225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that all the variations of the word ‘play’ have been reduced to the same word  – ‘play’. In this case, the output is a meaningful word, ‘play’. However, this is not always the case. Let us take an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae1d721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:57.601585Z",
     "iopub.status.busy": "2024-07-27T00:54:57.600804Z",
     "iopub.status.idle": "2024-07-27T00:54:57.607505Z",
     "shell.execute_reply": "2024-07-27T00:54:57.606262Z"
    },
    "papermill": {
     "duration": 0.039302,
     "end_time": "2024-07-27T00:54:57.610151",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.570849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  commun\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "print('Output: ',porter.stem(\"Communication\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69183dc",
   "metadata": {
    "papermill": {
     "duration": 0.028855,
     "end_time": "2024-07-27T00:54:57.668110",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.639255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The stemmer reduces the word ‘communication’ to a base word ‘commun’ which is meaningless in itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f051c8e",
   "metadata": {
    "papermill": {
     "duration": 0.028712,
     "end_time": "2024-07-27T00:54:57.726455",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.697743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50ba27",
   "metadata": {
    "papermill": {
     "duration": 0.028103,
     "end_time": "2024-07-27T00:54:57.784070",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.755967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What is Lemmatization?**\n",
    "\n",
    "Lemmatization is a linguistic process that involves reducing words to their base or canonical form, which is called the lemma. The lemma represents the dictionary form or citation form of a word. For example, the lemma of \"running\" is \"run\", and the lemma of \"better\" is \"good\".\n",
    "\n",
    "**Purpose of Lemmatization**\n",
    "\n",
    "The main purpose of lemmatization is to normalize words so that different forms of the same word are treated as one entity. This is particularly useful in text analysis tasks such as:\n",
    "\n",
    "* Information Retrieval: Ensuring that searches retrieve all relevant documents regardless of the word form used in the query.\n",
    "\n",
    "* Text Mining and Analytics: Improving the accuracy of analysis by consolidating different forms of a word.\n",
    "\n",
    "* Machine Learning: Enhancing the performance of models by reducing the vocabulary size and capturing the semantic meaning more accurately.\n",
    "\n",
    "**Lemmatization vs Stemming**\n",
    "\n",
    "Lemmatization is often compared to stemming, another technique used to reduce words to their base forms. While stemming chops off prefixes and suffixes of words to derive the root, lemmatization uses lexical knowledge bases to obtain the correct base form of words. Therefore, lemmatization tends to be more precise and context-aware compared to stemming.\n",
    "\n",
    "**How Lemmatization Works**\n",
    "\n",
    "Lemmatization typically involves identifying the part of speech (POS) of a word and applying morphological analysis to determine its lemma. For instance, verbs, nouns, adjectives, and adverbs may have different lemmatization rules in a language. Tools like NLTK (Natural Language Toolkit) in Python and other NLP libraries provide lemmatization functionalities that leverage linguistic databases such as WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da157e88",
   "metadata": {
    "papermill": {
     "duration": 0.027732,
     "end_time": "2024-07-27T00:54:57.840862",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.813130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example Sentence:**\n",
    "\n",
    "\"The cats are chasing mice.\"\n",
    "\n",
    "In lemmatization, words are transformed to their base form (lemma):\n",
    "\n",
    "1. Tokenization: Split the sentence into individual words: [\"The\", \"cats\", \"are\", \"chasing\", \"mice\"].\n",
    "\n",
    "2. Part-of-Speech Tagging: Determine the grammatical category of each word:\n",
    "\n",
    "    * \"The\" (Determiner)\n",
    "    * \"cats\" (Noun, plural)\n",
    "    * \"are\" (Verb, present tense)\n",
    "    * \"chasing\" (Verb, gerund or present participle)\n",
    "    * \"mice\" (Noun, plural)\n",
    "\n",
    "3. Lemmatization: Convert each word to its base form:\n",
    "\n",
    "    * \"The\" -> \"The\" (unchanged for articles)\n",
    "    * \"cats\" -> \"cat\"\n",
    "    * \"are\" -> \"be\"\n",
    "    * \"chasing\" -> \"chase\"\n",
    "    * \"mice\" -> \"mouse\"\n",
    "\n",
    "After lemmatization, the sentence becomes: \"The cat be chase mouse.\"\n",
    "\n",
    "Lemmatization helps in standardizing words so that variations of the same word (like \"cat\" and \"cats\") are treated as the same token, which is useful in various natural language processing tasks like text classification, information retrieval, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e18a2d",
   "metadata": {
    "papermill": {
     "duration": 0.028094,
     "end_time": "2024-07-27T00:54:57.899371",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.871277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Part of Speech Tagging\n",
    "Part of Speech (POS) tagging refers to assigning each word of a sentence to its part of speech. It is significant as it helps to give a better syntactic overview of a sentence. \n",
    "\n",
    "The parts of speech include nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and interjections.\n",
    "\n",
    "The primary goal of POS tagging is to analyze the structure of a sentence and understand how words relate to each other syntactically. This information is crucial for many downstream NLP tasks, such as syntactic parsing, named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "**Full Form of POS tags**\n",
    "\n",
    "* Verb (VBZ)\n",
    "* Preposition (IN)\n",
    "* Determiner (DT)\n",
    "* Adjective (JJ)\n",
    "* Noun (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d5340",
   "metadata": {
    "papermill": {
     "duration": 0.028678,
     "end_time": "2024-07-27T00:54:57.956837",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.928159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example of POS Tagging**\n",
    "\n",
    "Consider the sentence: “The quick brown fox jumps over the lazy dog.”\n",
    "\n",
    "After performing POS Tagging:\n",
    "\n",
    "* “The” is tagged as determiner (DT)\n",
    "* “quick” is tagged as adjective (JJ)\n",
    "* “brown” is tagged as adjective (JJ)\n",
    "* “fox” is tagged as noun (NN)\n",
    "* “jumps” is tagged as verb (VBZ)\n",
    "* “over” is tagged as preposition (IN)\n",
    "* “the” is tagged as determiner (DT)\n",
    "* “lazy” is tagged as adjective (JJ)\n",
    "* “dog” is tagged as noun (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc872407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:58.016256Z",
     "iopub.status.busy": "2024-07-27T00:54:58.015817Z",
     "iopub.status.idle": "2024-07-27T00:54:58.164983Z",
     "shell.execute_reply": "2024-07-27T00:54:58.163622Z"
    },
    "papermill": {
     "duration": 0.182487,
     "end_time": "2024-07-27T00:54:58.168023",
     "exception": false,
     "start_time": "2024-07-27T00:54:57.985536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "tags = pos_tag(tokenized_text)\n",
    "tags\n",
    "\n",
    "\n",
    "# I will discuss more on POS tags later in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2cde3",
   "metadata": {
    "papermill": {
     "duration": 0.029078,
     "end_time": "2024-07-27T00:54:58.226575",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.197497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Named Entity Recognition (NER)\n",
    "Named Entity Recognition (NER) is a key task in Natural Language Processing (NLP) that involves the identification and classification of named entities in unstructured text, such as people, organizations, locations, dates, and other relevant information. NER is used in various NLP applications such as information extraction, sentiment analysis, question-answering, and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2de66",
   "metadata": {
    "papermill": {
     "duration": 0.029611,
     "end_time": "2024-07-27T00:54:58.286158",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.256547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Steps involved in NER**\n",
    "\n",
    "Now, let’s take a look at the various steps involved in the NER process:\n",
    "\n",
    "* **Tokenization**: The first step in NER involves breaking down the input text into individual words or tokens.\n",
    "* **POS Tagging**: Next, we need to label each word in the text with its corresponding part of speech.\n",
    "* **Chunking**: After POS tagging, we can group the words together into meaningful phrases using a process called chunking.\n",
    "* **Named Entity Recognition**: Once we have identified the chunks, we can apply NER techniques to identify and classify the named entities in the text.\n",
    "* **Evaluation**: Finally, we can evaluate the performance of our NER model on a set of testing data to determine its accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75babb34",
   "metadata": {
    "papermill": {
     "duration": 0.027461,
     "end_time": "2024-07-27T00:54:58.342849",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.315388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Use of NER in NLP**\n",
    "\n",
    "NER has numerous applications in NLP, including information extraction, sentiment analysis, question-answering, recommendation systems, and more. Here are some common use cases of NER:\n",
    "\n",
    "* **Information Extraction**: NER can be used to extract relevant information from large volumes of unstructured text, such as news articles, social media posts, and online reviews. This information can be used to generate insights and make informed decisions.\n",
    "* **Sentiment Analysis**: NER can be used to identify the sentiment expressed in a text towards a particular named entity, such as a product or service. This information can be used to improve customer satisfaction and identify areas for improvement.\n",
    "* **Question Answering**: NER can be used to identify the relevant entities in a text that can be used to answer a specific question. This is particularly useful for chatbots and virtual assistants.\n",
    "* **Recommendation Systems**: NER can be used to identify the interests and preferences of users based on the entities mentioned in their search queries or online interactions. This information can be used to provide personalized recommendations and improve user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7aae5e",
   "metadata": {
    "papermill": {
     "duration": 0.029526,
     "end_time": "2024-07-27T00:54:58.401128",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.371602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Necessary requirements:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc76867b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:58.458385Z",
     "iopub.status.busy": "2024-07-27T00:54:58.457301Z",
     "iopub.status.idle": "2024-07-27T00:54:58.468291Z",
     "shell.execute_reply": "2024-07-27T00:54:58.466247Z"
    },
    "papermill": {
     "duration": 0.042419,
     "end_time": "2024-07-27T00:54:58.471788",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.429369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c09964e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:58.539770Z",
     "iopub.status.busy": "2024-07-27T00:54:58.538655Z",
     "iopub.status.idle": "2024-07-27T00:54:58.699453Z",
     "shell.execute_reply": "2024-07-27T00:54:58.696747Z"
    },
    "papermill": {
     "duration": 0.197733,
     "end_time": "2024-07-27T00:54:58.702742",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.505009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Steve/NNP)\n",
      "(PERSON Jobs/NNP)\n",
      "('was', 'VBD')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION CEO/NNP)\n",
      "('of', 'IN')\n",
      "(PERSON Apple/NNP Inc/NNP)\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"Steve Jobs was the CEO of Apple Inc.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# Perform named entity recognition (NER)\n",
    "entities = ne_chunk(tagged)\n",
    "\n",
    "# Print named entities\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0148d",
   "metadata": {
    "papermill": {
     "duration": 0.033597,
     "end_time": "2024-07-27T00:54:58.769721",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.736124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That is a simple example. I will discuss more on NER on other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35682147",
   "metadata": {
    "papermill": {
     "duration": 0.030518,
     "end_time": "2024-07-27T00:54:58.830535",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.800017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. spaCy <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16bd2c",
   "metadata": {
    "papermill": {
     "duration": 0.02854,
     "end_time": "2024-07-27T00:54:58.891642",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.863102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "spaCy is a free, open-source library for NLP in Python written in Cython. spaCy is designed to make it easy to build systems for information extraction or general-purpose natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9bea8",
   "metadata": {
    "papermill": {
     "duration": 0.032537,
     "end_time": "2024-07-27T00:54:58.953186",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.920649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Installation of spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fb2381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:54:59.019509Z",
     "iopub.status.busy": "2024-07-27T00:54:59.019024Z",
     "iopub.status.idle": "2024-07-27T00:55:12.207111Z",
     "shell.execute_reply": "2024-07-27T00:55:12.205981Z"
    },
    "papermill": {
     "duration": 13.225188,
     "end_time": "2024-07-27T00:55:12.209577",
     "exception": false,
     "start_time": "2024-07-27T00:54:58.984389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install spacy    # install spacy library\n",
    "\n",
    "\n",
    "clear_output()     # clear the unnecessary output if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dd97e",
   "metadata": {
    "papermill": {
     "duration": 0.022777,
     "end_time": "2024-07-27T00:55:12.255520",
     "exception": false,
     "start_time": "2024-07-27T00:55:12.232743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are various spaCy models for different languages. The default model for the English language is designated as en_core_web_sm. Since the models are quite large, it’s best to install them separately—including all languages in one package would make the download too massive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9055a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:12.303876Z",
     "iopub.status.busy": "2024-07-27T00:55:12.303453Z",
     "iopub.status.idle": "2024-07-27T00:55:18.800638Z",
     "shell.execute_reply": "2024-07-27T00:55:18.799498Z"
    },
    "papermill": {
     "duration": 6.524521,
     "end_time": "2024-07-27T00:55:18.803311",
     "exception": false,
     "start_time": "2024-07-27T00:55:12.278790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae84342",
   "metadata": {
    "papermill": {
     "duration": 0.023009,
     "end_time": "2024-07-27T00:55:18.849730",
     "exception": false,
     "start_time": "2024-07-27T00:55:18.826721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To start processing your input, you construct a Doc object. A Doc object is a sequence of Token objects representing a lexical token. Each Token object has information about a particular piece—typically one word—of text. You can instantiate a Doc object by calling the Language object with the input string as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb440e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:18.898880Z",
     "iopub.status.busy": "2024-07-27T00:55:18.898293Z",
     "iopub.status.idle": "2024-07-27T00:55:18.927896Z",
     "shell.execute_reply": "2024-07-27T00:55:18.926652Z"
    },
    "papermill": {
     "duration": 0.056636,
     "end_time": "2024-07-27T00:55:18.930408",
     "exception": false,
     "start_time": "2024-07-27T00:55:18.873772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'spaCy', '.']\n"
     ]
    }
   ],
   "source": [
    "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
    "\n",
    "print(type(introduction_doc))\n",
    "\n",
    "\n",
    "print([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113c528",
   "metadata": {
    "papermill": {
     "duration": 0.022974,
     "end_time": "2024-07-27T00:55:18.976914",
     "exception": false,
     "start_time": "2024-07-27T00:55:18.953940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above example, the text is used to instantiate a Doc object. From there, you can access a whole bunch of information about the processed text.\n",
    "\n",
    "For instance, you iterated over the Doc object with a list comprehension that produces a series of Token objects. On each Token object, you called the .text attribute to get the text contained within that token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203624f2",
   "metadata": {
    "papermill": {
     "duration": 0.023894,
     "end_time": "2024-07-27T00:55:19.024396",
     "exception": false,
     "start_time": "2024-07-27T00:55:19.000502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sentence Detection\n",
    "Sentence detection is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part-of-speech (POS) tagging and named-entity recognition, which you’ll come to later in the tutorial.\n",
    "\n",
    "In spaCy, the .sents property is used to extract sentences from the Doc object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2268da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:19.073684Z",
     "iopub.status.busy": "2024-07-27T00:55:19.072427Z",
     "iopub.status.idle": "2024-07-27T00:55:19.091909Z",
     "shell.execute_reply": "2024-07-27T00:55:19.090716Z"
    },
    "papermill": {
     "duration": 0.047258,
     "end_time": "2024-07-27T00:55:19.095052",
     "exception": false,
     "start_time": "2024-07-27T00:55:19.047794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  2\n",
      "[Gus Proto is a Python developer currently working for a London-based Fintech company., He is interested in learning Natural Language Processing.]\n"
     ]
    }
   ],
   "source": [
    "about_text = (\"Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.\")\n",
    "\n",
    "doc = nlp(about_text)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print('Length: ',len(sentences))\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868ee58",
   "metadata": {
    "papermill": {
     "duration": 0.023409,
     "end_time": "2024-07-27T00:55:19.142166",
     "exception": false,
     "start_time": "2024-07-27T00:55:19.118757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the above example, spaCy is correctly able to identify the input’s sentences. With .sents, you get a list of Span objects representing individual sentences. You can also slice the Span objects to produce sections of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376aeaa",
   "metadata": {
    "papermill": {
     "duration": 0.023702,
     "end_time": "2024-07-27T00:55:19.189237",
     "exception": false,
     "start_time": "2024-07-27T00:55:19.165535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tokens in spaCy\n",
    "Building the Doc container involves tokenizing the text. The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects.\n",
    "\n",
    "As you’ve already seen, with spaCy, you can print the tokens by iterating over the Doc object. But Token objects also have other attributes available for exploration. For instance, the token’s original index position in the string is still available as an attribute on Token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a97bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:19.238363Z",
     "iopub.status.busy": "2024-07-27T00:55:19.237955Z",
     "iopub.status.idle": "2024-07-27T00:55:20.115411Z",
     "shell.execute_reply": "2024-07-27T00:55:20.113499Z"
    },
    "papermill": {
     "duration": 0.905569,
     "end_time": "2024-07-27T00:55:20.118314",
     "exception": false,
     "start_time": "2024-07-27T00:55:19.212745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_text = (\"Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.\")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf12f37",
   "metadata": {
    "papermill": {
     "duration": 0.02329,
     "end_time": "2024-07-27T00:55:20.165331",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.142041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this example, you iterate over Doc, printing both Token and the .idx attribute, which represents the starting position of the token in the original text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667164e",
   "metadata": {
    "papermill": {
     "duration": 0.02342,
     "end_time": "2024-07-27T00:55:20.212451",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.189031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Stop Words in spaCy\n",
    "Stop words are typically defined as the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\n",
    "\n",
    "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. spaCy stores a list of stop words for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d6dca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:20.262308Z",
     "iopub.status.busy": "2024-07-27T00:55:20.261466Z",
     "iopub.status.idle": "2024-07-27T00:55:20.268899Z",
     "shell.execute_reply": "2024-07-27T00:55:20.267800Z"
    },
    "papermill": {
     "duration": 0.034974,
     "end_time": "2024-07-27T00:55:20.271308",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.236334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67efdb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:20.321738Z",
     "iopub.status.busy": "2024-07-27T00:55:20.321331Z",
     "iopub.status.idle": "2024-07-27T00:55:20.327008Z",
     "shell.execute_reply": "2024-07-27T00:55:20.325865Z"
    },
    "papermill": {
     "duration": 0.034645,
     "end_time": "2024-07-27T00:55:20.329843",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.295198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifteen\n",
      "itself\n",
      "we\n",
      "either\n",
      "can\n",
      "below\n",
      "even\n",
      "whither\n",
      "eleven\n",
      "some\n"
     ]
    }
   ],
   "source": [
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9ae7e",
   "metadata": {
    "papermill": {
     "duration": 0.024744,
     "end_time": "2024-07-27T00:55:20.378138",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.353394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this example, you’ve examined the STOP_WORDS list from **spacy.lang.en.stop_words**. You don’t need to access this list directly, though. You can remove stop words from the input text by making use of the **.is_stop** attribute of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e31c131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:20.428367Z",
     "iopub.status.busy": "2024-07-27T00:55:20.427964Z",
     "iopub.status.idle": "2024-07-27T00:55:21.284012Z",
     "shell.execute_reply": "2024-07-27T00:55:21.282651Z"
    },
    "papermill": {
     "duration": 0.884476,
     "end_time": "2024-07-27T00:55:21.286444",
     "exception": false,
     "start_time": "2024-07-27T00:55:20.401968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    "custom_about_text = (\"Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_doc = nlp(custom_about_text)\n",
    "print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51671516",
   "metadata": {
    "papermill": {
     "duration": 0.024291,
     "end_time": "2024-07-27T00:55:21.334981",
     "exception": false,
     "start_time": "2024-07-27T00:55:21.310690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here you use a list comprehension with a conditional expression to produce a list of all the words that are not stop words in the text.\n",
    "\n",
    "While you can’t be sure exactly what the sentence is trying to say without stop words, you still have a lot of information about what it’s generally about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ee4b6",
   "metadata": {
    "papermill": {
     "duration": 0.023452,
     "end_time": "2024-07-27T00:55:21.382614",
     "exception": false,
     "start_time": "2024-07-27T00:55:21.359162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lemmatization in spaCy\n",
    "Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma.\n",
    "\n",
    "For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma.\n",
    "\n",
    "Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\n",
    "\n",
    "spaCy puts a **.lemma_** attribute on the Token class. This attribute has the lemmatized form of the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b739ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:21.432940Z",
     "iopub.status.busy": "2024-07-27T00:55:21.432531Z",
     "iopub.status.idle": "2024-07-27T00:55:22.464226Z",
     "shell.execute_reply": "2024-07-27T00:55:22.462780Z"
    },
    "papermill": {
     "duration": 1.060094,
     "end_time": "2024-07-27T00:55:22.466921",
     "exception": false,
     "start_time": "2024-07-27T00:55:21.406827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is  :  be\n",
      "He  :  he\n",
      "keeps  :  keep\n",
      "organizing  :  organize\n",
      "meetups  :  meetup\n",
      "talks  :  talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_text = (\"Gus is helping organize a developer conference on Applications of Natural Language Processing. He keeps organizing local Python meetups and several internal talks at his workplace.\")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(token,' : ',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df79397",
   "metadata": {
    "papermill": {
     "duration": 0.024388,
     "end_time": "2024-07-27T00:55:22.515654",
     "exception": false,
     "start_time": "2024-07-27T00:55:22.491266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this example, you check to see if the original word is different from the lemma, and if it is, you print both the original word and its lemma.\n",
    "\n",
    "You’ll note, for instance, that organizing reduces to its lemma form, organize. If you don’t lemmatize the text, then organize and organizing will be counted as different tokens, even though they both refer to the same concept. Lemmatization helps you avoid duplicate words that may overlap conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f53b94",
   "metadata": {
    "papermill": {
     "duration": 0.023537,
     "end_time": "2024-07-27T00:55:22.563068",
     "exception": false,
     "start_time": "2024-07-27T00:55:22.539531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Word Frequency in spaCy\n",
    "You can now convert a given text into tokens and perform statistical analysis on it. This analysis can give you various insights, such as common words or unique words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e63348f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:22.612781Z",
     "iopub.status.busy": "2024-07-27T00:55:22.612376Z",
     "iopub.status.idle": "2024-07-27T00:55:23.461247Z",
     "shell.execute_reply": "2024-07-27T00:55:23.459921Z"
    },
    "papermill": {
     "duration": 0.877,
     "end_time": "2024-07-27T00:55:23.463699",
     "exception": false,
     "start_time": "2024-07-27T00:55:22.586699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('words', 3), ('stop', 2), ('common', 1), ('tell', 1), ('summarized', 1)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"Four out of five of the most common words are stop words that don’t really tell you much about the summarized text. This is why stop words are often considered noise for many applications.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Words without stop_words and punctuation \n",
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d96fa",
   "metadata": {
    "papermill": {
     "duration": 0.023831,
     "end_time": "2024-07-27T00:55:23.512406",
     "exception": false,
     "start_time": "2024-07-27T00:55:23.488575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you can just look at the most common words, that may save you a lot of reading, because you can immediately tell if the text is about something that interests you or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38ed41",
   "metadata": {
    "papermill": {
     "duration": 0.02861,
     "end_time": "2024-07-27T00:55:23.566135",
     "exception": false,
     "start_time": "2024-07-27T00:55:23.537525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Part-of-Speech Tagging\n",
    "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are typically eight parts of speech:\n",
    "\n",
    "* Noun\n",
    "* Pronoun\n",
    "* Adjective\n",
    "* Verb\n",
    "* Adverb\n",
    "* Preposition\n",
    "* Conjunction\n",
    "* Interjection\n",
    "\n",
    "Part-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "143d4e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:23.616243Z",
     "iopub.status.busy": "2024-07-27T00:55:23.615340Z",
     "iopub.status.idle": "2024-07-27T00:55:24.479432Z",
     "shell.execute_reply": "2024-07-27T00:55:24.478035Z"
    },
    "papermill": {
     "duration": 0.892741,
     "end_time": "2024-07-27T00:55:24.482780",
     "exception": false,
     "start_time": "2024-07-27T00:55:23.590039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Token: He\n",
      "    ------------\n",
      "    Tag: PRP       Pos: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: is\n",
      "    ------------\n",
      "    Tag: VBZ       Pos: AUX\n",
      "    Explanation: verb, 3rd person singular present\n",
      "    \n",
      "\n",
      "    Token: interested\n",
      "    ------------\n",
      "    Tag: JJ       Pos: ADJ\n",
      "    Explanation: adjective (English), other noun-modifier (Chinese)\n",
      "    \n",
      "\n",
      "    Token: in\n",
      "    ------------\n",
      "    Tag: IN       Pos: ADP\n",
      "    Explanation: conjunction, subordinating or preposition\n",
      "    \n",
      "\n",
      "    Token: learning\n",
      "    ------------\n",
      "    Tag: VBG       Pos: VERB\n",
      "    Explanation: verb, gerund or present participle\n",
      "    \n",
      "\n",
      "    Token: Natural\n",
      "    ------------\n",
      "    Tag: NNP       Pos: PROPN\n",
      "    Explanation: noun, proper singular\n",
      "    \n",
      "\n",
      "    Token: Language\n",
      "    ------------\n",
      "    Tag: NNP       Pos: PROPN\n",
      "    Explanation: noun, proper singular\n",
      "    \n",
      "\n",
      "    Token: Processing\n",
      "    ------------\n",
      "    Tag: NNP       Pos: PROPN\n",
      "    Explanation: noun, proper singular\n",
      "    \n",
      "\n",
      "    Token: .\n",
      "    ------------\n",
      "    Tag: .       Pos: PUNCT\n",
      "    Explanation: punctuation mark, sentence closer\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"He is interested in learning Natural Language Processing.\")     # try with your own text and have fun.\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'''\n",
    "    Token: {str(token)}\n",
    "    ------------\n",
    "    Tag: {str(token.tag_)}       Pos: {str(token.pos_)}\n",
    "    Explanation: {spacy.explain(token.tag_)}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760d4d9",
   "metadata": {
    "papermill": {
     "duration": 0.026226,
     "end_time": "2024-07-27T00:55:24.533467",
     "exception": false,
     "start_time": "2024-07-27T00:55:24.507241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, two attributes of the Token class are accessed and printed using f-strings:\n",
    "\n",
    "1. **.tag_** displays a fine-grained tag.\n",
    "2. **.pos_** displays a coarse-grained tag, which is a reduced version of the fine-grained tags.\n",
    "\n",
    "You also use **spacy.explain()** to give descriptive details about a particular POS tag, which can be a valuable reference tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0696a",
   "metadata": {
    "papermill": {
     "duration": 0.025511,
     "end_time": "2024-07-27T00:55:24.584381",
     "exception": false,
     "start_time": "2024-07-27T00:55:24.558870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization: Using displaCy\n",
    "\n",
    "spaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.\n",
    "\n",
    "You can use displaCy to find POS tags for tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6d1d0b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:24.635123Z",
     "iopub.status.busy": "2024-07-27T00:55:24.634701Z",
     "iopub.status.idle": "2024-07-27T00:55:25.498132Z",
     "shell.execute_reply": "2024-07-27T00:55:25.496849Z"
    },
    "papermill": {
     "duration": 0.891509,
     "end_time": "2024-07-27T00:55:25.500749",
     "exception": false,
     "start_time": "2024-07-27T00:55:24.609240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1bc0ec9974d44488aa30900cd1f023d7-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1bc0ec9974d44488aa30900cd1f023d7-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1bc0ec9974d44488aa30900cd1f023d7-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"He is interested in learning Natural Language Processing.\")     # try with your own text and have fun.\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae7db2",
   "metadata": {
    "papermill": {
     "duration": 0.024439,
     "end_time": "2024-07-27T00:55:25.550255",
     "exception": false,
     "start_time": "2024-07-27T00:55:25.525816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the image above, each token is assigned a POS tag written just below the token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661af318",
   "metadata": {
    "papermill": {
     "duration": 0.023975,
     "end_time": "2024-07-27T00:55:25.598705",
     "exception": false,
     "start_time": "2024-07-27T00:55:25.574730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Named-Entity Recognition in spaCy\n",
    "Named-entity recognition (NER) is the process of locating named entities in unstructured text and then classifying them into predefined categories, such as person names, organizations, locations, monetary values, percentages, and time expressions.\n",
    "\n",
    "You can use NER to learn more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\n",
    "\n",
    "spaCy has the property **.ents** on Doc objects. You can use it to extract named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "135a9cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:25.649127Z",
     "iopub.status.busy": "2024-07-27T00:55:25.648737Z",
     "iopub.status.idle": "2024-07-27T00:55:26.523461Z",
     "shell.execute_reply": "2024-07-27T00:55:26.521973Z"
    },
    "papermill": {
     "duration": 0.902972,
     "end_time": "2024-07-27T00:55:26.525993",
     "exception": false,
     "start_time": "2024-07-27T00:55:25.623021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Great Piano Academy =  Companies, agencies, institutions, etc.\n",
      "    \n",
      "\n",
      "    Mayfair =  Countries, cities, states\n",
      "    \n",
      "\n",
      "    the City of London =  Countries, cities, states\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"Great Piano Academy is situated in Mayfair or the City of London and has world-class piano instructors.\")     \n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'''\n",
    "    {ent.text} =  {spacy.explain(ent.label_)}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25107761",
   "metadata": {
    "papermill": {
     "duration": 0.024091,
     "end_time": "2024-07-27T00:55:26.574508",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.550417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At first I extract Named entities by using **.ents** on Doc objects.\n",
    "\n",
    "In the above example, **ent** is a Span object with various attributes:\n",
    "\n",
    "* **.text** gives the Unicode text representation of the entity.\n",
    "* **.label_** gives the label of the entity.\n",
    "\n",
    "**spacy.explain** gives descriptive details about each entity label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ed1a1",
   "metadata": {
    "papermill": {
     "duration": 0.02433,
     "end_time": "2024-07-27T00:55:26.624290",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.599960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**You can also use displaCy to visualize these entities:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f26ad55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:26.695400Z",
     "iopub.status.busy": "2024-07-27T00:55:26.694065Z",
     "iopub.status.idle": "2024-07-27T00:55:26.705857Z",
     "shell.execute_reply": "2024-07-27T00:55:26.704603Z"
    },
    "papermill": {
     "duration": 0.050666,
     "end_time": "2024-07-27T00:55:26.709359",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.658693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Piano Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d0dc7",
   "metadata": {
    "papermill": {
     "duration": 0.025025,
     "end_time": "2024-07-27T00:55:26.762021",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.736996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**One use case** \n",
    "\n",
    "for NER is to redact people’s names from a text. For example, you might want to do this in order to hide personal information collected in a survey. Take a look at the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417363e",
   "metadata": {
    "papermill": {
     "duration": 0.026279,
     "end_time": "2024-07-27T00:55:26.814743",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.788464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some explanations I found for these:\n",
    "\n",
    "* PERSON:      People, including fictional.\n",
    "* NORP:        Nationalities or religious or political groups.\n",
    "* FAC:         Buildings, airports, highways, bridges, etc.\n",
    "* ORG:         Companies, agencies, institutions, etc.\n",
    "* GPE:         Countries, cities, states.\n",
    "* LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART: Titles of books, songs, etc.\n",
    "* LAW:         Named documents made into laws.\n",
    "* LANGUAGE:    Any named language.\n",
    "* DATE:        Absolute or relative dates or periods.\n",
    "* TIME:        Times smaller than a day.\n",
    "* PERCENT:     Percentage, including ”%“.\n",
    "* MONEY:       Monetary values, including unit.\n",
    "* QUANTITY:    Measurements, as of weight or distance.\n",
    "* ORDINAL:     “first”, “second”, etc.\n",
    "* CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b9208d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:26.868163Z",
     "iopub.status.busy": "2024-07-27T00:55:26.867751Z",
     "iopub.status.idle": "2024-07-27T00:55:27.740968Z",
     "shell.execute_reply": "2024-07-27T00:55:27.739956Z"
    },
    "papermill": {
     "duration": 0.902691,
     "end_time": "2024-07-27T00:55:27.743518",
     "exception": false,
     "start_time": "2024-07-27T00:55:26.840827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pepsi is a carbonated soft drink with a cola flavor, manufactured by PepsiCo. [REDACTED] loves pepsi a lot.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "survey_text = (\"Pepsi is a carbonated soft drink with a cola flavor, manufactured by PepsiCo. David loves pepsi a lot.\")\n",
    "\n",
    "doc = nlp(survey_text)\n",
    "\n",
    "def replace_person_names(token):\n",
    "    if token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    return token.text_with_ws\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    with nlp_doc.retokenize() as retokenizer:\n",
    "        for ent in nlp_doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(replace_person_names, nlp_doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "redact_names(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990cdf0",
   "metadata": {
    "papermill": {
     "duration": 0.026337,
     "end_time": "2024-07-27T00:55:27.795097",
     "exception": false,
     "start_time": "2024-07-27T00:55:27.768760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **redact_names()** function uses a retokenizer to adjust the tokenizing model. It gets all the tokens and passes the text through **map()** to replace any target tokens with [REDACTED].\n",
    "\n",
    "So just like that, you would be able to redact a huge amount of text in seconds, while doing it manually could take many hours. That said, you always need to be careful with redaction, because the models aren’t perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b785904",
   "metadata": {
    "papermill": {
     "duration": 0.024681,
     "end_time": "2024-07-27T00:55:27.844718",
     "exception": false,
     "start_time": "2024-07-27T00:55:27.820037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. TextBlob <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850930ba",
   "metadata": {
    "papermill": {
     "duration": 0.024619,
     "end_time": "2024-07-27T00:55:27.894686",
     "exception": false,
     "start_time": "2024-07-27T00:55:27.870067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TextBlob is a simple and intuitive NLP library built on NLTK and Pattern libraries. It provides a high-level interface for common NLP tasks like sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and classification. TextBlob’s easy-to-use API makes it suitable for beginners and rapid prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa9f34",
   "metadata": {
    "papermill": {
     "duration": 0.025302,
     "end_time": "2024-07-27T00:55:27.945159",
     "exception": false,
     "start_time": "2024-07-27T00:55:27.919857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Features**\n",
    "\n",
    "* Noun phrase extraction\n",
    "* Part-of-speech tagging\n",
    "* Sentiment analysis\n",
    "* Classification (Naive Bayes, Decision Tree)\n",
    "* Tokenization (splitting text into words and sentences)\n",
    "* Word and phrase frequencies\n",
    "* Parsing\n",
    "* n-grams\n",
    "* Word inflection (pluralization and singularization) and lemmatization\n",
    "* Spelling correction\n",
    "* Add new models or languages through extensions\n",
    "* WordNet integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c1615",
   "metadata": {
    "papermill": {
     "duration": 0.024353,
     "end_time": "2024-07-27T00:55:27.994204",
     "exception": false,
     "start_time": "2024-07-27T00:55:27.969851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Get it now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c792f626",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:28.048057Z",
     "iopub.status.busy": "2024-07-27T00:55:28.047621Z",
     "iopub.status.idle": "2024-07-27T00:55:45.199093Z",
     "shell.execute_reply": "2024-07-27T00:55:45.197713Z"
    },
    "papermill": {
     "duration": 17.181466,
     "end_time": "2024-07-27T00:55:45.201871",
     "exception": false,
     "start_time": "2024-07-27T00:55:28.020405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (0.18.0.post0)\r\n",
      "Collecting nltk>=3.8 (from textblob)\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2023.12.25)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.66.4)\r\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nltk\r\n",
      "  Attempting uninstall: nltk\r\n",
      "    Found existing installation: nltk 3.2.4\r\n",
      "    Uninstalling nltk-3.2.4:\r\n",
      "      Successfully uninstalled nltk-3.2.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nltk-3.8.1\r\n",
      "[nltk_data] Downloading package brown to /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package brown is already up-to-date!\r\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package punkt is already up-to-date!\r\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package wordnet is already up-to-date!\r\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\r\n",
      "[nltk_data]     /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\r\n",
      "[nltk_data]       date!\r\n",
      "[nltk_data] Downloading package conll2000 to /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\r\n",
      "[nltk_data] Downloading package movie_reviews to\r\n",
      "[nltk_data]     /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057ca6e",
   "metadata": {
    "papermill": {
     "duration": 0.025772,
     "end_time": "2024-07-27T00:55:45.253586",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.227814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tutorial: Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6c63914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:45.307442Z",
     "iopub.status.busy": "2024-07-27T00:55:45.306976Z",
     "iopub.status.idle": "2024-07-27T00:55:45.344232Z",
     "shell.execute_reply": "2024-07-27T00:55:45.343128Z"
    },
    "papermill": {
     "duration": 0.067101,
     "end_time": "2024-07-27T00:55:45.346869",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.279768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, the import\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Let’s create our first TextBlob.\n",
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b391a",
   "metadata": {
    "papermill": {
     "duration": 0.025122,
     "end_time": "2024-07-27T00:55:45.397611",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.372489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Part-of-speech Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8451dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:45.450840Z",
     "iopub.status.busy": "2024-07-27T00:55:45.450433Z",
     "iopub.status.idle": "2024-07-27T00:55:45.460316Z",
     "shell.execute_reply": "2024-07-27T00:55:45.459321Z"
    },
    "papermill": {
     "duration": 0.039311,
     "end_time": "2024-07-27T00:55:45.462418",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.423107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('high-level', 'JJ'),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part-of-speech tags can be accessed through the tags property.\n",
    "wiki.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f8884",
   "metadata": {
    "papermill": {
     "duration": 0.025122,
     "end_time": "2024-07-27T00:55:45.513214",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.488092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Noun Phrase Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb859c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:45.565956Z",
     "iopub.status.busy": "2024-07-27T00:55:45.565532Z",
     "iopub.status.idle": "2024-07-27T00:55:49.355706Z",
     "shell.execute_reply": "2024-07-27T00:55:49.354569Z"
    },
    "papermill": {
     "duration": 3.819677,
     "end_time": "2024-07-27T00:55:49.358350",
     "exception": false,
     "start_time": "2024-07-27T00:55:45.538673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly, noun phrases are accessed through the noun_phrases property.\n",
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db930e91",
   "metadata": {
    "papermill": {
     "duration": 0.026117,
     "end_time": "2024-07-27T00:55:49.411066",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.384949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba77e33",
   "metadata": {
    "papermill": {
     "duration": 0.026459,
     "end_time": "2024-07-27T00:55:49.463930",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.437471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c514f329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:49.517486Z",
     "iopub.status.busy": "2024-07-27T00:55:49.517081Z",
     "iopub.status.idle": "2024-07-27T00:55:49.570774Z",
     "shell.execute_reply": "2024-07-27T00:55:49.569463Z"
    },
    "papermill": {
     "duration": 0.08368,
     "end_time": "2024-07-27T00:55:49.573530",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.489850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9ca43",
   "metadata": {
    "papermill": {
     "duration": 0.025918,
     "end_time": "2024-07-27T00:55:49.625967",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.600049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e01d4e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:49.680137Z",
     "iopub.status.busy": "2024-07-27T00:55:49.679717Z",
     "iopub.status.idle": "2024-07-27T00:55:49.687362Z",
     "shell.execute_reply": "2024-07-27T00:55:49.686019Z"
    },
    "papermill": {
     "duration": 0.03807,
     "end_time": "2024-07-27T00:55:49.690190",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.652120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list:  ['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'] \n",
      "\n",
      "Sentence list:  [Sentence(\"Beautiful is better than ugly.\"), Sentence(\"Explicit is better than implicit.\"), Sentence(\"Simple is better than complex.\")]\n"
     ]
    }
   ],
   "source": [
    "zen = TextBlob(\n",
    "    \"Beautiful is better than ugly. \"\n",
    "    \"Explicit is better than implicit. \"\n",
    "    \"Simple is better than complex.\"\n",
    ")\n",
    "print('Word list: ',zen.words, '\\n')\n",
    "\n",
    "print('Sentence list: ',zen.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dc219",
   "metadata": {
    "papermill": {
     "duration": 0.026329,
     "end_time": "2024-07-27T00:55:49.743451",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.717122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**WordLists**\n",
    "\n",
    "A WordList is just a Python list with additional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b14bdd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:49.798610Z",
     "iopub.status.busy": "2024-07-27T00:55:49.797630Z",
     "iopub.status.idle": "2024-07-27T00:55:49.805408Z",
     "shell.execute_reply": "2024-07-27T00:55:49.804074Z"
    },
    "papermill": {
     "duration": 0.038044,
     "end_time": "2024-07-27T00:55:49.807792",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.769748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'octopus']\n",
      "['cats', 'dogs', 'octopodes']\n"
     ]
    }
   ],
   "source": [
    "animals = TextBlob(\"cat dog octopus\")\n",
    "print(animals.words)\n",
    "\n",
    "print(animals.words.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643f765",
   "metadata": {
    "papermill": {
     "duration": 0.026372,
     "end_time": "2024-07-27T00:55:49.861435",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.835063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Spelling Correction**\n",
    "\n",
    "Use the **correct()** method to attempt spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb98c3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:49.916256Z",
     "iopub.status.busy": "2024-07-27T00:55:49.915221Z",
     "iopub.status.idle": "2024-07-27T00:55:49.972881Z",
     "shell.execute_reply": "2024-07-27T00:55:49.971477Z"
    },
    "papermill": {
     "duration": 0.087671,
     "end_time": "2024-07-27T00:55:49.975404",
     "exception": false,
     "start_time": "2024-07-27T00:55:49.887733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "b.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997a286",
   "metadata": {
    "papermill": {
     "duration": 0.026677,
     "end_time": "2024-07-27T00:55:50.028719",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.002042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Word objects have a **spellcheck(), Word.spellcheck()** method that returns a list of (word, confidence) tuples with spelling suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10e8969b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.083128Z",
     "iopub.status.busy": "2024-07-27T00:55:50.082694Z",
     "iopub.status.idle": "2024-07-27T00:55:50.090050Z",
     "shell.execute_reply": "2024-07-27T00:55:50.089035Z"
    },
    "papermill": {
     "duration": 0.037283,
     "end_time": "2024-07-27T00:55:50.092309",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.055026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('correct', 1.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "w = Word('correct')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d55009",
   "metadata": {
    "papermill": {
     "duration": 0.02665,
     "end_time": "2024-07-27T00:55:50.145471",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.118821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Get Word and Noun Phrase Frequencies**\n",
    "\n",
    "There are two ways to get the frequency of a word or noun phrase in a TextBlob.\n",
    "\n",
    "The first is through the **word_counts** dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f146b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.201873Z",
     "iopub.status.busy": "2024-07-27T00:55:50.200962Z",
     "iopub.status.idle": "2024-07-27T00:55:50.208817Z",
     "shell.execute_reply": "2024-07-27T00:55:50.207638Z"
    },
    "papermill": {
     "duration": 0.03889,
     "end_time": "2024-07-27T00:55:50.211263",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.172373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = TextBlob(\"We are no longer the Knights who say Ni. \"\n",
    "                    \"We are now the Knights who say Ekki ekki ekki PTANG.\")\n",
    "monty.word_counts['ekki']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428f9b1",
   "metadata": {
    "papermill": {
     "duration": 0.028242,
     "end_time": "2024-07-27T00:55:50.266335",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.238093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The second way is to use the **count()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc83240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.321374Z",
     "iopub.status.busy": "2024-07-27T00:55:50.320975Z",
     "iopub.status.idle": "2024-07-27T00:55:50.327916Z",
     "shell.execute_reply": "2024-07-27T00:55:50.326808Z"
    },
    "papermill": {
     "duration": 0.036965,
     "end_time": "2024-07-27T00:55:50.330015",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.293050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.words.count('ekki')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63ec23",
   "metadata": {
    "papermill": {
     "duration": 0.026496,
     "end_time": "2024-07-27T00:55:50.383030",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.356534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Get Start and End Indices of Sentences**\n",
    "\n",
    "Use sentence.start and sentence.end to get the indices where a sentence starts and ends within a TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffaea124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.439330Z",
     "iopub.status.busy": "2024-07-27T00:55:50.438923Z",
     "iopub.status.idle": "2024-07-27T00:55:50.445264Z",
     "shell.execute_reply": "2024-07-27T00:55:50.444162Z"
    },
    "papermill": {
     "duration": 0.03725,
     "end_time": "2024-07-27T00:55:50.447509",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.410259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at index 0, Ends at index 39\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob('We are no longer the Knights who say Ni')\n",
    "for s in text.sentences:\n",
    "    print('Start at index {}, Ends at index {}'.format(s.start,s.end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e1677",
   "metadata": {
    "papermill": {
     "duration": 0.027253,
     "end_time": "2024-07-27T00:55:50.502362",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.475109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tutorial: Building a Text Classification System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779ee1e",
   "metadata": {
    "papermill": {
     "duration": 0.026489,
     "end_time": "2024-07-27T00:55:50.555764",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.529275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **textblob.classifiers** module makes it simple to create custom classifiers.\n",
    "\n",
    "As an example, let’s create a custom sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25044f",
   "metadata": {
    "papermill": {
     "duration": 0.026604,
     "end_time": "2024-07-27T00:55:50.609057",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.582453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Loading Data and Creating a Classifier**\n",
    "\n",
    "First we’ll create some training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48ea80d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.665407Z",
     "iopub.status.busy": "2024-07-27T00:55:50.664992Z",
     "iopub.status.idle": "2024-07-27T00:55:50.671611Z",
     "shell.execute_reply": "2024-07-27T00:55:50.670571Z"
    },
    "papermill": {
     "duration": 0.03793,
     "end_time": "2024-07-27T00:55:50.673971",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.636041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    (\"I love this sandwich.\", \"pos\"),\n",
    "    (\"this is an amazing place!\", \"pos\"),\n",
    "    (\"I feel very good about these beers.\", \"pos\"),\n",
    "    (\"this is my best work.\", \"pos\"),\n",
    "    (\"what an awesome view\", \"pos\"),\n",
    "    (\"I do not like this restaurant\", \"neg\"),\n",
    "    (\"I am tired of this stuff.\", \"neg\"),\n",
    "    (\"I can't deal with this\", \"neg\"),\n",
    "    (\"he is my sworn enemy!\", \"neg\"),\n",
    "    (\"my boss is horrible.\", \"neg\"),\n",
    "]\n",
    "test = [\n",
    "    (\"the beer was good.\", \"pos\"),\n",
    "    (\"I do not enjoy my job\", \"neg\"),\n",
    "    (\"I ain't feeling dandy today.\", \"neg\"),\n",
    "    (\"I feel amazing!\", \"pos\"),\n",
    "    (\"Gary is a friend of mine.\", \"pos\"),\n",
    "    (\"I can't believe I'm doing this.\", \"neg\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67eaebf",
   "metadata": {
    "papermill": {
     "duration": 0.026418,
     "end_time": "2024-07-27T00:55:50.727117",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.700699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we’ll create a Naive Bayes classifier, passing the training data into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb89a943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.782390Z",
     "iopub.status.busy": "2024-07-27T00:55:50.781966Z",
     "iopub.status.idle": "2024-07-27T00:55:50.793100Z",
     "shell.execute_reply": "2024-07-27T00:55:50.791644Z"
    },
    "papermill": {
     "duration": 0.041879,
     "end_time": "2024-07-27T00:55:50.795677",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.753798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfde56",
   "metadata": {
    "papermill": {
     "duration": 0.02626,
     "end_time": "2024-07-27T00:55:50.850233",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.823973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Classifying Text**\n",
    "\n",
    "Call the classify(text) method to use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed28013d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:50.905632Z",
     "iopub.status.busy": "2024-07-27T00:55:50.905165Z",
     "iopub.status.idle": "2024-07-27T00:55:50.913840Z",
     "shell.execute_reply": "2024-07-27T00:55:50.912733Z"
    },
    "papermill": {
     "duration": 0.039273,
     "end_time": "2024-07-27T00:55:50.916130",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.876857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify('I do not like this restaurant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca1385",
   "metadata": {
    "papermill": {
     "duration": 0.026533,
     "end_time": "2024-07-27T00:55:50.969543",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.943010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Classifying TextBlobs**\n",
    "\n",
    "Another way to classify text is to pass a classifier into the constructor of **TextBlob** and call its **classify()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55673860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:51.024734Z",
     "iopub.status.busy": "2024-07-27T00:55:51.024321Z",
     "iopub.status.idle": "2024-07-27T00:55:51.032154Z",
     "shell.execute_reply": "2024-07-27T00:55:51.031004Z"
    },
    "papermill": {
     "duration": 0.038434,
     "end_time": "2024-07-27T00:55:51.034764",
     "exception": false,
     "start_time": "2024-07-27T00:55:50.996330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful is better than ugly. --> pos\n",
      "Explicit is better than implicit. --> pos\n",
      "Simple is not better than complex. --> neg\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sents = TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is not better than complex.\", classifier = cl)\n",
    "\n",
    "for s in sents.sentences:\n",
    "    print(s,'-->',s.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80273ff8",
   "metadata": {
    "papermill": {
     "duration": 0.026581,
     "end_time": "2024-07-27T00:55:51.088986",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.062405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Evaluating Classifiers**\n",
    "\n",
    "To compute the accuracy on our test set, use the **accuracy(test_data)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c69677e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:51.145172Z",
     "iopub.status.busy": "2024-07-27T00:55:51.144420Z",
     "iopub.status.idle": "2024-07-27T00:55:51.153663Z",
     "shell.execute_reply": "2024-07-27T00:55:51.152467Z"
    },
    "papermill": {
     "duration": 0.040114,
     "end_time": "2024-07-27T00:55:51.156020",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.115906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befac5bf",
   "metadata": {
    "papermill": {
     "duration": 0.026927,
     "end_time": "2024-07-27T00:55:51.211210",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.184283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Advanced Usage: Overriding Models and the Blobber Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4979470",
   "metadata": {
    "papermill": {
     "duration": 0.026708,
     "end_time": "2024-07-27T00:55:51.265248",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.238540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TextBlob allows you to specify which algorithms you want to use under the hood of its simple API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f28fe3",
   "metadata": {
    "papermill": {
     "duration": 0.026792,
     "end_time": "2024-07-27T00:55:51.319113",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.292321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sentiment Analyzers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b77f2",
   "metadata": {
    "papermill": {
     "duration": 0.026948,
     "end_time": "2024-07-27T00:55:51.372989",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.346041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **textblob.sentiments** module contains two sentiment analysis implementations, **PatternAnalyzer** (based on the pattern library) and **NaiveBayesAnalyzer** (an NLTK classifier trained on a movie reviews corpus).\n",
    "\n",
    "The default implementation is **PatternAnalyzer**, but you can override the analyzer by passing another implementation into a TextBlob’s constructor.\n",
    "\n",
    "For instance, the **NaiveBayesAnalyzer** returns its result as a namedtuple of the form: **Sentiment(classification, p_pos, p_neg)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f28e7919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:51.429643Z",
     "iopub.status.busy": "2024-07-27T00:55:51.429239Z",
     "iopub.status.idle": "2024-07-27T00:55:57.476994Z",
     "shell.execute_reply": "2024-07-27T00:55:57.475745Z"
    },
    "papermill": {
     "duration": 6.078943,
     "end_time": "2024-07-27T00:55:57.479484",
     "exception": false,
     "start_time": "2024-07-27T00:55:51.400541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.7996209910191279, p_neg=0.2003790089808724)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "blob = TextBlob('I love this library', analyzer = NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403892dd",
   "metadata": {
    "papermill": {
     "duration": 0.027012,
     "end_time": "2024-07-27T00:55:57.534292",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.507280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2841e",
   "metadata": {
    "papermill": {
     "duration": 0.027141,
     "end_time": "2024-07-27T00:55:57.588488",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.561347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **words** and **sentences** properties are helpers that use the **textblob.tokenizers.WordTokenizer** and **textblob.tokenizers.SentenceTokenizer** classes, respectively.\n",
    "\n",
    "You can use other tokenizers, such as those provided by NLTK, by passing them into the **TextBlob** constructor then accessing the **tokens** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe0b2f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:57.644695Z",
     "iopub.status.busy": "2024-07-27T00:55:57.644268Z",
     "iopub.status.idle": "2024-07-27T00:55:57.701168Z",
     "shell.execute_reply": "2024-07-27T00:55:57.700128Z"
    },
    "papermill": {
     "duration": 0.087478,
     "end_time": "2024-07-27T00:55:57.703408",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.615930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['This is', 'a rather tabby', 'blob.'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TabTokenizer\n",
    "tokenizer = TabTokenizer()\n",
    "blob = TextBlob(\"This is\\ta rather tabby\\tblob.\", tokenizer=tokenizer)\n",
    "blob.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dad033d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:57.759811Z",
     "iopub.status.busy": "2024-07-27T00:55:57.758967Z",
     "iopub.status.idle": "2024-07-27T00:55:57.765206Z",
     "shell.execute_reply": "2024-07-27T00:55:57.763984Z"
    },
    "papermill": {
     "duration": 0.037089,
     "end_time": "2024-07-27T00:55:57.767702",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.730613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13f88e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:57.825822Z",
     "iopub.status.busy": "2024-07-27T00:55:57.825085Z",
     "iopub.status.idle": "2024-07-27T00:55:57.830873Z",
     "shell.execute_reply": "2024-07-27T00:55:57.829810Z"
    },
    "papermill": {
     "duration": 0.037563,
     "end_time": "2024-07-27T00:55:57.833630",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.796067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'And another sentence!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is a sentence. And another sentence!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c6c31",
   "metadata": {
    "papermill": {
     "duration": 0.028552,
     "end_time": "2024-07-27T00:55:57.894229",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.865677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Noun Phrase Chunkers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4367ef",
   "metadata": {
    "papermill": {
     "duration": 0.028447,
     "end_time": "2024-07-27T00:55:57.950966",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.922519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TextBlob currently has two noun phrases chunker implementations, **textblob.np_extractors.FastNPExtractor** (default, based on Shlomi Babluki’s implementation from this blog post) and **textblob.np_extractors.ConllExtractor**, which uses the CoNLL 2000 corpus to train a tagger.\n",
    "\n",
    "You can change the chunker implementation (or even use your own) by explicitly passing an instance of a noun phrase extractor to a TextBlob’s constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd3ef066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:55:58.007544Z",
     "iopub.status.busy": "2024-07-27T00:55:58.007174Z",
     "iopub.status.idle": "2024-07-27T00:56:00.535228Z",
     "shell.execute_reply": "2024-07-27T00:56:00.534100Z"
    },
    "papermill": {
     "duration": 2.559204,
     "end_time": "2024-07-27T00:56:00.537645",
     "exception": false,
     "start_time": "2024-07-27T00:55:57.978441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python', 'high-level programming language'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "extractor = ConllExtractor()\n",
    "blob = TextBlob(\"Python is a high-level programming language.\", np_extractor=extractor)\n",
    "\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544fdfb",
   "metadata": {
    "papermill": {
     "duration": 0.027084,
     "end_time": "2024-07-27T00:56:00.592533",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.565449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### POS Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097dbee",
   "metadata": {
    "papermill": {
     "duration": 0.027746,
     "end_time": "2024-07-27T00:56:00.647660",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.619914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TextBlob currently has two POS tagger implementations, located in **textblob.taggers**. The default is the **PatternTagger** which uses the same implementation as the pattern library.\n",
    "\n",
    "The second implementation is **NLTKTagger** which uses NLTK’s TreeBank tagger. Numpy is required to use the **NLTKTagger**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b9732",
   "metadata": {
    "papermill": {
     "duration": 0.027371,
     "end_time": "2024-07-27T00:56:00.703264",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.675893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similar to the tokenizers and noun phrase chunkers, you can explicitly specify which POS tagger to use by passing a tagger instance to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca716404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:56:00.759985Z",
     "iopub.status.busy": "2024-07-27T00:56:00.759548Z",
     "iopub.status.idle": "2024-07-27T00:56:00.768723Z",
     "shell.execute_reply": "2024-07-27T00:56:00.767601Z"
    },
    "papermill": {
     "duration": 0.040252,
     "end_time": "2024-07-27T00:56:00.771086",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.730834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tag', 'NN'), ('You', 'PRP'), (\"'re\", 'VBP'), ('It', 'PRP')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger\n",
    "nltk_tagger = NLTKTagger()\n",
    "blob = TextBlob(\"Tag! You're It!\", pos_tagger=nltk_tagger)\n",
    "blob.pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46869a",
   "metadata": {
    "papermill": {
     "duration": 0.02765,
     "end_time": "2024-07-27T00:56:00.826497",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.798847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6d02b",
   "metadata": {
    "papermill": {
     "duration": 0.02734,
     "end_time": "2024-07-27T00:56:00.882955",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.855615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Parser implementations can also be passed to the TextBlob constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00d9c262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T00:56:00.940712Z",
     "iopub.status.busy": "2024-07-27T00:56:00.939807Z",
     "iopub.status.idle": "2024-07-27T00:56:00.947548Z",
     "shell.execute_reply": "2024-07-27T00:56:00.946402Z"
    },
    "papermill": {
     "duration": 0.03922,
     "end_time": "2024-07-27T00:56:00.949861",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.910641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parsing/VBG/B-VP/O is/VBZ/I-VP/O fun/NN/B-NP/O ././O/O'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.parsers import PatternParser\n",
    "blob = TextBlob(\"Parsing is fun.\", parser=PatternParser())\n",
    "blob.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97edbec6",
   "metadata": {
    "papermill": {
     "duration": 0.028195,
     "end_time": "2024-07-27T00:56:01.005822",
     "exception": false,
     "start_time": "2024-07-27T00:56:00.977627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Hugging Face Transformer <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8dfde2",
   "metadata": {
    "papermill": {
     "duration": 0.027934,
     "end_time": "2024-07-27T00:56:01.062311",
     "exception": false,
     "start_time": "2024-07-27T00:56:01.034377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read the basics from this artical\n",
    "\n",
    "https://www.geeksforgeeks.org/hugging-face-transformers/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 103.63375,
   "end_time": "2024-07-27T00:56:03.473471",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-27T00:54:19.839721",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
